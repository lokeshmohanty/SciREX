{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SciREX: Scientific Research and Engineering eXcellence","text":"<p>SciREX is an open-source scientific AI and machine learning framework designed for researchers and engineers. Jointly developed by Zenteiq Aitech Innovations Private Limited and the AiREX (AI for Research and Engineering eXcellence) Lab at Indian Institute of Science, Bangalore, SciREX bridges the gap between theoretical research and practical implementation while maintaining mathematical rigor and computational efficiency.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Research-First Design: Built specifically for scientific computing and research workflows</li> <li>Mathematical Foundations: Strong emphasis on mathematical correctness and theoretical foundations</li> <li>Hardware Optimization: Efficient implementation with GPU acceleration support</li> <li>Reproducible Research: Built-in experiment tracking and result reproduction capabilities</li> <li>Scientific Visualization: Publication-ready plotting and visualization tools</li> <li>Industrial Integration: Enterprise-ready solutions backed by Zenteiq's industrial expertise</li> </ul>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Scientific Computing</li> <li>Physics-Informed Neural Networks (PINNs)</li> <li>Differential Equation Solvers</li> <li> <p>Scientific Data Analysis</p> </li> <li> <p>Machine Learning</p> </li> <li>Classical ML Algorithms</li> <li>Deep Learning Models</li> <li>Custom Loss Functions</li> <li> <p>Advanced Optimizers</p> </li> <li> <p>Research Tools</p> </li> <li>Experiment Management</li> <li>Result Visualization</li> <li>Benchmark Datasets</li> <li>Performance Metrics</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Clone the SciREX repo <pre><code>mkdir scirex\ncd scirex\ngit clone https://github.com/zenoxml/SciREX.git\n</code></pre></li> <li>Create a virtual environment <pre><code>python3 -m venv .\nsource bin/activate\n</code></pre></li> <li>Install dependencies using: <pre><code>pip install -e .\n</code></pre></li> <li>This completes the installation. Check out the examples for usage</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Visit our documentation  for:</p> <ul> <li> <p>Getting Started Guide</p> </li> <li> <p>API Reference</p> </li> <li> <p>Mathematical Background</p> </li> <li> <p>Tutorials and Examples</p> </li> <li> <p>Contribution Guidelines</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>Copyright (c) 2024 Zenteiq Aitech Innovations Private Limited and AiREX Lab, Indian Institute of Science, Bangalore. All rights reserved.</p>"},{"location":"#software-license","title":"Software License","text":"<p>SciREX is licensed under the Apache License, Version 2.0 (the \"License\"). You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"#intellectual-property","title":"Intellectual Property","text":""},{"location":"#copyright-holders","title":"Copyright Holders","text":"<ul> <li>Zenteiq Aitech Innovations Private Limited</li> <li>The AiREX Lab at IISc Bangalore</li> </ul>"},{"location":"#components-and-libraries","title":"Components and Libraries","text":"<ul> <li>The core SciREX framework and its original components are copyright of the above holders</li> <li>Third-party libraries and dependencies are subject to their respective licenses</li> <li>Mathematical algorithms and scientific methods implemented may be subject to their own patents or licenses</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from both the research and industrial communities! Please see our Contributing Guidelinesfor details.</p>"},{"location":"#community","title":"Community","text":"<ul> <li>Discord</li> </ul>"},{"location":"#official-partners","title":"Official Partners","text":"<ul> <li>ARTPARK (AI &amp; Robotics Technology Park) at IISc</li> <li>In discussion with NVIDIA and other technology companies</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>SciREX is developed and maintained through the collaborative efforts of Zenteiq Aitech Innovations and the AiREX Lab at IISc Bangalore. We thank all contributors from both industry and academia for their valuable input and support in advancing scientific computing.</p> Powered by  Zenteiq Aitech Innovations Private Limited and Airex Lab (IISc) - Aritifical Intelligence Research Engineering Exellence Lab"},{"location":"api/experimental/Contribution/contribution/","title":"Contributing to SciREX","text":"<p>First off, thank you for considering contributing to SciREX! We value contributions from both the academic and industrial communities.</p> <p>All types of contributions are encouraged and valued. This document outlines various ways to help and details about how this project handles them. Please read the relevant sections before making your contribution.</p> <p>If you like the project but don't have time to contribute, there are other ways to support it: - Star the project on GitHub - Share it in academic and professional networks - Cite it in your research papers - Mention it at conferences and workshops - Use it in your research or industrial projects</p>"},{"location":"api/experimental/Contribution/contribution/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Questions and Support</li> <li>Ways to Contribute</li> <li>Reporting Bugs</li> <li>Suggesting Enhancements</li> <li>Code Contributions</li> <li>Documentation</li> <li>Research Contributions</li> <li>Development Guidelines</li> <li>Setting Up Development Environment</li> <li>Code Style</li> <li>Testing</li> <li>Documentation Standards</li> <li>Legal Notices</li> </ul>"},{"location":"api/experimental/Contribution/contribution/#questions-and-support","title":"Questions and Support","text":"<p>Before asking a question: - Read the Documentation - Search existing Issues - Check our Discord community</p> <p>For new questions: 1. Open an Issue 2. Provide context about your use case 3. Include relevant details (Python version, OS, etc.) 4. Specify if it's a research or industrial application</p>"},{"location":"api/experimental/Contribution/contribution/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"api/experimental/Contribution/contribution/#reporting-bugs","title":"Reporting Bugs","text":""},{"location":"api/experimental/Contribution/contribution/#before-submitting-a-bug-report","title":"Before Submitting a Bug Report","text":"<ul> <li>Verify you're using the latest version</li> <li>Check if it's a known issue in our bug tracker</li> <li>Search academic literature if it's a known scientific computing issue</li> <li>Collect information:</li> <li>Full error traceback</li> <li>OS and hardware details</li> <li>Minimal reproducible example</li> <li>Scientific context if applicable</li> </ul>"},{"location":"api/experimental/Contribution/contribution/#submitting-a-bug-report","title":"Submitting a Bug Report","text":"<p>Security Issues: Never report security vulnerabilities through public GitHub issues. Email security@zenteiq.ai instead.</p> <p>Submit bugs through GitHub issues: 1. Use the bug report template 2. Describe expected vs actual behavior 3. Provide reproduction steps 4. Include relevant scientific context 5. Attach minimal test data if needed</p>"},{"location":"api/experimental/Contribution/contribution/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>For enhancement suggestions: 1. Verify the functionality doesn't exist 2. Explain the scientific/technical motivation 3. Provide mathematical foundations if applicable 4. Describe implementation approach 5. Reference relevant research papers</p>"},{"location":"api/experimental/Contribution/contribution/#code-contributions","title":"Code Contributions","text":""},{"location":"api/experimental/Contribution/contribution/#first-time-contributors","title":"First-time Contributors","text":"<ol> <li>Fork the repository</li> <li>Clone your fork: <pre><code>git clone https://github.com/&lt;YOUR_USERNAME&gt;/SciREX.git\ncd SciREX\n</code></pre></li> <li>Create a branch: <pre><code>git checkout -b feature/&lt;descriptive-name&gt;\n</code></pre></li> <li>Install dependencies: <pre><code>pip install -e \".[dev,test]\"\n</code></pre></li> </ol>"},{"location":"api/experimental/Contribution/contribution/#development-workflow","title":"Development Workflow","text":"<ol> <li>Write tests first</li> <li>Implement your changes</li> <li>Add documentation</li> <li>Run test suite: <pre><code>pytest tests/\n</code></pre></li> <li>Submit a pull request</li> </ol>"},{"location":"api/experimental/Contribution/contribution/#documentation","title":"Documentation","text":"<p>Documentation contributions should: - Follow scientific writing standards - Include mathematical notation where appropriate - Provide practical examples - Reference relevant literature - Follow our documentation style guide</p>"},{"location":"api/experimental/Contribution/contribution/#research-contributions","title":"Research Contributions","text":"<p>For research-related contributions: - Provide theoretical foundations - Include mathematical proofs if applicable - Add benchmark results - Reference related work - Follow scientific reproducibility guidelines</p>"},{"location":"api/experimental/Contribution/contribution/#development-guidelines","title":"Development Guidelines","text":""},{"location":"api/experimental/Contribution/contribution/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use meaningful scientific variable names</li> <li>Document mathematical equations</li> <li>Include algorithm complexity analysis</li> <li>Add references to papers/methods</li> </ul>"},{"location":"api/experimental/Contribution/contribution/#testing","title":"Testing","text":"<ul> <li>Write unit tests for new features</li> <li>Include numerical accuracy tests</li> <li>Add performance benchmarks</li> <li>Test edge cases</li> <li>Verify mathematical correctness</li> </ul>"},{"location":"api/experimental/Contribution/contribution/#styleguides","title":"Styleguides","text":""},{"location":"api/experimental/Contribution/contribution/#code-style-and-documentation","title":"Code Style and Documentation","text":""},{"location":"api/experimental/Contribution/contribution/#google-style-guide-adaptation-for-scientific-computing","title":"Google Style Guide Adaptation for Scientific Computing","text":"<p>We follow the Google Python Style Guide with specific adaptations for scientific computing:</p>"},{"location":"api/experimental/Contribution/contribution/#function-and-variable-names","title":"Function and Variable Names","text":"<pre><code># Good\ndef solve_navier_stokes(reynolds_number, initial_conditions):\n    velocity_field = initialize_flow_field()\n    pressure_gradient = compute_pressure_gradient()\n\n# Bad\ndef solveNS(re, ic):\n    v = init_flow()\n    dp = comp_press()\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#docstring-format","title":"Docstring Format","text":"<pre><code>def compute_finite_difference(\n    function_values: np.ndarray,\n    dx: float,\n    order: int = 2\n) -&gt; np.ndarray:\n    \"\"\"Computes finite difference approximation of derivatives.\n\n    Implements centered finite difference scheme of specified order for\n    approximating derivatives of discrete data.\n\n    Args:\n        function_values: Array of function values at discrete points.\n            Shape: (n_points,) or (n_points, n_dimensions)\n        dx: Grid spacing (assumed uniform)\n        order: Order of accuracy for finite difference scheme.\n            Must be 2 or 4. Defaults to 2.\n\n    Returns:\n        np.ndarray: Approximated derivatives at each point.\n            Same shape as input function_values.\n\n    Raises:\n        ValueError: If order is not 2 or 4\n        ValueError: If function_values has less than order + 1 points\n\n    Notes:\n        Uses the following stencils:\n        2nd order: f'(x) \u2248 (f(x+h) - f(x-h))/(2h)\n        4th order: f'(x) \u2248 (-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h))/(12h)\n\n    References:\n        [1] LeVeque, R. J. (2007). Finite Difference Methods for Ordinary\n            and Partial Differential Equations.\n    \"\"\"\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#module-level-docstrings","title":"Module-Level Docstrings","text":"<pre><code>\"\"\"Solvers for partial differential equations using finite element methods.\n\nThis module implements various finite element solvers for elliptic, parabolic,\nand hyperbolic PDEs. The implementation follows the formulation in Hughes (2000)\nwith extensions for stabilized methods.\n\nKey classes:\n    - FEMSolver: Base solver class\n    - EllipticSolver: For elliptic PDEs\n    - ParabolicSolver: For parabolic PDEs\n\nKey functions:\n    - assemble_stiffness_matrix: Assembles global stiffness matrix\n    - solve_linear_system: Solves resulting linear system\n\nNote that all solvers assume a well-posed problem with appropriate boundary\nconditions.\n\nReferences:\n    [1] Hughes, T. J. R. (2000). The Finite Element Method: Linear Static and\n        Dynamic Finite Element Analysis.\n\"\"\"\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#class-docstrings","title":"Class Docstrings","text":"<pre><code>class AdaptiveMesh:\n    \"\"\"A mesh that adapts based on solution features.\n\n    This class implements adaptive mesh refinement/coarsening based on\n    error estimates. The refinement strategy follows the procedure outlined\n    in Bank et al. (1983).\n\n    Attributes:\n        nodes: Array of node coordinates, shape (n_nodes, dimension)\n        elements: Array of element connectivity, shape (n_elements, nodes_per_element)\n        error_estimator: Callable that computes error estimates\n        refinement_threshold: Float, threshold for mesh refinement\n\n    Example:\n        &gt;&gt;&gt; mesh = AdaptiveMesh(domain=[0, 1], initial_elements=10)\n        &gt;&gt;&gt; mesh.refine(error_threshold=1e-3)\n        &gt;&gt;&gt; solution = solve_pde(mesh)\n\n    References:\n        [1] Bank, R. E., Sherman, A. H., &amp; Weiser, A. (1983). \"Refinement\n            Algorithms and Data Structures for Regular Local Mesh Refinement\"\n    \"\"\"\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#exception-handling","title":"Exception Handling","text":"<pre><code>def integrate_numerically(function, bounds, method='gauss'):\n    \"\"\"Performs numerical integration using specified method.\n\n    Args:\n        function: Callable to integrate\n        bounds: Tuple of (lower, upper) bounds\n        method: Integration method ('gauss' or 'simpson')\n\n    Raises:\n        ValueError: If bounds are invalid or method is unsupported\n        IntegrationError: If integration fails to converge\n\n    Note:\n        For singular integrands, consider using adaptive methods\n    \"\"\"\n    if bounds[0] &gt;= bounds[1]:\n        raise ValueError(\n            f\"Lower bound {bounds[0]} must be less than upper bound {bounds[1]}\"\n        )\n\n    try:\n        result = _perform_integration(function, bounds, method)\n    except ConvergenceError as e:\n        raise IntegrationError(f\"Integration failed to converge: {e}\")\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#type-hints-and-comments","title":"Type Hints and Comments","text":"<pre><code>from typing import Union, Callable, Optional\nimport numpy as np\nimport scipy.sparse as sp\n\ndef solve_sparse_system(\n    matrix: Union[np.ndarray, sp.spmatrix],  # System matrix\n    rhs: np.ndarray,  # Right-hand side vector\n    solver: Optional[str] = 'gmres',  # Iterative solver type\n    preconditioner: Optional[Callable] = None,  # Custom preconditioner\n    tolerance: float = 1e-6\n) -&gt; np.ndarray:\n    \"\"\"Solves sparse linear system Ax = b.\"\"\"\n\nExample docstring:\n```python\ndef solve_pde(mesh, boundary_conditions, tolerance=1e-6):\n    \"\"\"Solves partial differential equations using the finite element method.\n\n    Implements the algorithm described in [Smith et al., 2023] using an adaptive\n    mesh refinement strategy.\n\n    The method solves the equation:\n\n    \u2202u/\u2202t = \u03b1\u2207\u00b2u + f(x,t)\n\n    Args:\n        mesh (FEMesh): Finite element mesh object\n        boundary_conditions (dict): Dictionary of boundary conditions\n        tolerance (float, optional): Convergence tolerance. Defaults to 1e-6\n\n    Returns:\n        np.ndarray: Solution vector on mesh nodes\n\n    References:\n        [1] Smith et al. (2023). Advanced PDE Solvers. J. Comp. Physics\n    \"\"\"\n</code></pre>"},{"location":"api/experimental/Contribution/contribution/#commit-messages","title":"Commit Messages","text":"<ul> <li>Use clear, descriptive commit messages</li> <li>Follow the format: <code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;</code></li> <li>Types: feat, fix, docs, style, refactor, perf, test</li> <li>Include issue numbers if applicable</li> </ul> <p>Example: <pre><code>feat(pinns): implement physics-informed loss function\n\n- Add custom loss terms for PDE constraints\n- Implement automatic differentiation for physics terms\n- Add tests for conservation laws\n- Update documentation with mathematical formulation\n\nFixes #123\n</code></pre></p>"},{"location":"api/experimental/Contribution/contribution/#improving-the-documentation","title":"Improving The Documentation","text":"<p>Documentation improvements are highly valued. Please follow these guidelines:</p>"},{"location":"api/experimental/Contribution/contribution/#types-of-documentation","title":"Types of Documentation","text":"<ol> <li>API Documentation</li> <li>Clear function/class descriptions</li> <li>Mathematical foundations</li> <li>Parameter explanations</li> <li>Example usage</li> <li> <p>References to papers</p> </li> <li> <p>Tutorials</p> </li> <li>Step-by-step guides</li> <li>Practical examples</li> <li>Clear explanations</li> <li> <p>Reproducible code</p> </li> <li> <p>Theoretical Documentation</p> </li> <li>Mathematical background</li> <li>Algorithm descriptions</li> <li>Derivations</li> <li> <p>Limitations and assumptions</p> </li> <li> <p>Benchmark Documentation</p> </li> <li>Performance metrics</li> <li>Comparison with other methods</li> <li>Hardware specifications</li> <li>Test conditions</li> </ol>"},{"location":"api/experimental/Contribution/contribution/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, scientific language</li> <li>Include mathematical notation where appropriate</li> <li>Provide practical examples</li> <li>Reference relevant literature</li> <li>Follow consistent formatting</li> </ul>"},{"location":"api/experimental/Contribution/contribution/#contributing-workflow","title":"Contributing Workflow","text":"<ol> <li> <p>Fork and Clone <pre><code>git clone https://github.com/&lt;YOUR_USERNAME&gt;/SciREX.git\ncd SciREX\n</code></pre></p> </li> <li> <p>Set Up Environment <pre><code>python -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\npip install -e \".[dev,test,docs]\"\npre-commit install\n</code></pre></p> </li> <li> <p>Create Feature Branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Development Process</p> </li> <li>Write tests first</li> <li>Implement changes</li> <li>Add documentation</li> <li>Run test suite</li> <li>Check code style</li> <li> <p>Update benchmarks if needed</p> </li> <li> <p>Review Checklist</p> </li> <li>[ ] Tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] Code style guidelines followed</li> <li>[ ] Benchmarks updated (if applicable)</li> <li>[ ] Mathematical correctness verified</li> <li> <p>[ ] Examples added</p> </li> <li> <p>Submit Pull Request</p> </li> <li>Use PR template</li> <li>Link related issues</li> <li>Describe changes</li> <li>Add benchmark results</li> <li> <p>Request review</p> </li> <li> <p>Review Process</p> </li> <li>Address reviewer comments</li> <li>Update documentation</li> <li>Maintain mathematical rigor</li> <li>Ensure reproducibility</li> </ol>"},{"location":"api/experimental/Contribution/contribution/#legal-notices","title":"Legal Notices","text":"<ul> <li>All contributions must be licensed under the Apache License 2.0</li> <li>Contributors must sign our Contributor License Agreement</li> <li>Maintain all copyright notices</li> <li>Include appropriate citations</li> </ul> <p>For detailed documentation and guidelines, visit https://scirex.org/docs/contributing</p> <p>This document is maintained by the SciREX team at Zenteiq Aitech Innovations and AiREX Lab.</p>"},{"location":"api/experimental/examples/dl/cnn_mnist/","title":"cnn_mnist","text":"<p>Example Script: cnn-mnist.py</p> <p>This script demonstrates how to use the neural network implementation from the SciREX library to perform classification on the MNIST dataset.</p> This example includes <ul> <li>Loading the MNIST dataset using tensorflow.keras.datasets</li> <li>Training Convolutional Neural Networks</li> <li>Evaluating and visualizing the results</li> </ul> Key Features <ul> <li>Uses cross-entropy loss for training</li> <li>Implements accuracy metric for evaluation</li> <li>Includes model checkpointing</li> <li>Provides training history visualization</li> </ul> Authors <ul> <li>Lokesh Mohanty (lokeshm@iisc.ac.in)</li> </ul> Version Info <ul> <li>04/01/2024: Initial version</li> <li>06/01/2024: Update imports</li> </ul>"},{"location":"api/experimental/examples/dl/cnn_mnist/#examples.experimental.dl.cnn_mnist.CNN","title":"<code>CNN</code>","text":"<p>               Bases: <code>Network</code></p> <p>Convolutional Neural Network for MNIST digit classification.</p> <p>Architecture: - Conv2D: 1-&gt;4 channels, 4x4 kernel - MaxPool2D: 2x2 pooling - ReLU activation - Conv2D: 4-&gt;8 channels, 4x4 kernel - MaxPool2D: 2x2 pooling - ReLU activation - Flatten - Dense: 844-&gt;10 units - LogSoftmax activation</p> Source code in <code>examples/experimental/dl/cnn_mnist.py</code> <pre><code>class CNN(Network):\n    \"\"\"\n    Convolutional Neural Network for MNIST digit classification.\n\n    Architecture:\n    - Conv2D: 1-&gt;4 channels, 4x4 kernel\n    - MaxPool2D: 2x2 pooling\n    - ReLU activation\n    - Conv2D: 4-&gt;8 channels, 4x4 kernel\n    - MaxPool2D: 2x2 pooling\n    - ReLU activation\n    - Flatten\n    - Dense: 8*4*4-&gt;10 units\n    - LogSoftmax activation\n    \"\"\"\n\n    layers: list\n\n    def __init__(self):\n        \"\"\"Initialize the CNN architecture with predefined layers.\"\"\"\n        self.layers = [\n            layers.Conv2d(\n                1, 4, kernel_size=4, key=key1\n            ),  # First conv layer: 1-&gt;4 channels\n            layers.MaxPool2d(2, 2),  # Reduce spatial dimensions\n            activations.relu,  # Activation function\n            layers.Conv2d(\n                4, 8, kernel_size=4, key=key1\n            ),  # Second conv layer: 4-&gt;8 channels\n            layers.MaxPool2d(2, 2),  # Further reduce dimensions\n            activations.relu,  # Activation function\n            jnp.ravel,  # Flatten for dense layer\n            layers.Linear(8 * 4 * 4, 10, key=key2),  # Output layer: 10 classes\n            utils.log_softmax,  # For numerical stability\n        ]\n\n    def __call__(self, x):\n        \"\"\"\n        Forward pass through the network.\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"\n        Generate class predictions from model outputs.\n        \"\"\"\n        return jnp.argmax(self(x), axis=-1)\n</code></pre>"},{"location":"api/experimental/examples/dl/cnn_mnist/#examples.experimental.dl.cnn_mnist.CNN.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward pass through the network.</p> Source code in <code>examples/experimental/dl/cnn_mnist.py</code> <pre><code>def __call__(self, x):\n    \"\"\"\n    Forward pass through the network.\n    \"\"\"\n    for layer in self.layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/experimental/examples/dl/cnn_mnist/#examples.experimental.dl.cnn_mnist.CNN.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the CNN architecture with predefined layers.</p> Source code in <code>examples/experimental/dl/cnn_mnist.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the CNN architecture with predefined layers.\"\"\"\n    self.layers = [\n        layers.Conv2d(\n            1, 4, kernel_size=4, key=key1\n        ),  # First conv layer: 1-&gt;4 channels\n        layers.MaxPool2d(2, 2),  # Reduce spatial dimensions\n        activations.relu,  # Activation function\n        layers.Conv2d(\n            4, 8, kernel_size=4, key=key1\n        ),  # Second conv layer: 4-&gt;8 channels\n        layers.MaxPool2d(2, 2),  # Further reduce dimensions\n        activations.relu,  # Activation function\n        jnp.ravel,  # Flatten for dense layer\n        layers.Linear(8 * 4 * 4, 10, key=key2),  # Output layer: 10 classes\n        utils.log_softmax,  # For numerical stability\n    ]\n</code></pre>"},{"location":"api/experimental/examples/dl/cnn_mnist/#examples.experimental.dl.cnn_mnist.CNN.predict","title":"<code>predict(x)</code>","text":"<p>Generate class predictions from model outputs.</p> Source code in <code>examples/experimental/dl/cnn_mnist.py</code> <pre><code>def predict(self, x):\n    \"\"\"\n    Generate class predictions from model outputs.\n    \"\"\"\n    return jnp.argmax(self(x), axis=-1)\n</code></pre>"},{"location":"api/experimental/examples/dl/gcn_cora/","title":"gcn_cora","text":"<p>Example Script: gcn_cora.py</p> <p>This script demonstrates how to use the graph convolution network implementation from the SciREX library to perform classification on the CORA dataset from pytorch geometirc.</p> This example includes <ul> <li>Loading the CORA dataset using pytorch.geometric</li> <li>Training Graph Convolutional Neural Networks</li> <li>Evaluating the results</li> </ul> Key Features <ul> <li>Uses cross-entropy loss for training</li> <li>Implements accuracy metric for evaluation</li> </ul> Authors <ul> <li>Rajarshi Dasgupta (rajarshid@iisc.ac.in)</li> </ul> Version Info <ul> <li>10/01/2025: Initial version</li> </ul>"},{"location":"api/experimental/examples/dl/gcn_cora/#examples.experimental.dl.gcn_cora.data","title":"<code>data = Planetoid(root='/tmp/Cora', name='Cora')[0]</code>  <code>module-attribute</code>","text":"<p>From the CORA dataset from pytorch geometric we obtain     - node feature vectors         There are 2,708 nodes.         Each node represents a scientific publication.         The size of the node feature vector is 1,433         and has values 0 or 1.         A dictionary of 1,433 unique words         is used to assign the node feature vector.         The i-th value of the node feature vector is 1,         if the i-th word in the dictionary is present         in the research paper represented by the node and 0 otherwise.     - node labels         There are 7 classes indicating the topics of the research paper         and the nodes have labels 0 to 6.     - edges         Research publication contain citations         to other scientific works.         The dataset provides edges         which are a set of pairs of nodes connected by citation.         The citation network is represented by a directed graph.</p>"},{"location":"api/experimental/examples/dl/vae_mnist/","title":"vae_mnist","text":"<p>Example Script: vae-mnist.py</p> <p>This script demonstrates how to use the neural network implementation from the SciREX library to variational auto-encoder on the MNIST dataset.</p> This example includes <ul> <li>Loading the MNIST dataset using tensorflow.keras.datasets</li> <li>Training Variational Auto-Encoder</li> <li>Evaluating and visualizing the results</li> </ul> Key features <ul> <li>Encoder-decoder architecture</li> <li>Latent space sampling with reparameterization trick</li> <li>KL divergence loss for regularization</li> <li>Model checkpointing and visualization</li> </ul> Authors <ul> <li>Lokesh Mohanty (lokeshm@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/dl/vae_mnist/#examples.experimental.dl.vae_mnist.VAE","title":"<code>VAE</code>","text":"<p>               Bases: <code>Network</code></p> <p>Variational Autoencoder implementation.</p> <p>The VAE consists of: 1. An encoder that maps inputs to latent space parameters 2. A sampling layer that uses the reparameterization trick 3. A decoder that reconstructs inputs from latent samples</p> <p>Attributes:</p> Name Type Description <code>encoder</code> <code>FCNN</code> <p>Neural network for encoding</p> <code>decoder</code> <code>FCNN</code> <p>Neural network for decoding</p> Source code in <code>examples/experimental/dl/vae_mnist.py</code> <pre><code>class VAE(Network):\n    \"\"\"\n    Variational Autoencoder implementation.\n\n    The VAE consists of:\n    1. An encoder that maps inputs to latent space parameters\n    2. A sampling layer that uses the reparameterization trick\n    3. A decoder that reconstructs inputs from latent samples\n\n    Attributes:\n        encoder (FCNN): Neural network for encoding\n        decoder (FCNN): Neural network for decoding\n    \"\"\"\n\n    encoder: FCNN\n    decoder: FCNN\n\n    def __init__(self, encoderLayers, decoderLayers):\n        \"\"\"\n        Initialize VAE with encoder and decoder architectures.\n\n        Args:\n            encoderLayers (list): Layer definitions for encoder\n            decoderLayers (list): Layer definitions for decoder\n        \"\"\"\n        self.encoder = FCNN(encoderLayers)\n        self.decoder = FCNN(decoderLayers)\n\n    def __call__(self, x):\n        \"\"\"\n        Forward pass through the VAE.\n\n        Args:\n            x (jax.Array): Input image tensor\n\n        Returns:\n            jax.Array: Reconstructed image tensor\n        \"\"\"\n        # Encode input to get latent parameters\n        x = self.encoder(x)\n        # Split into mean and log standard deviation\n        mean, stddev = x[:-1], jnp.exp(x[-1])\n        # Sample from latent space using reparameterization trick\n        z = mean + stddev * jax.random.normal(jax.random.PRNGKey(0), mean.shape)\n        # Decode latent sample\n        return self.decoder(z)\n</code></pre>"},{"location":"api/experimental/examples/dl/vae_mnist/#examples.experimental.dl.vae_mnist.VAE.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward pass through the VAE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input image tensor</p> required <p>Returns:</p> Type Description <p>jax.Array: Reconstructed image tensor</p> Source code in <code>examples/experimental/dl/vae_mnist.py</code> <pre><code>def __call__(self, x):\n    \"\"\"\n    Forward pass through the VAE.\n\n    Args:\n        x (jax.Array): Input image tensor\n\n    Returns:\n        jax.Array: Reconstructed image tensor\n    \"\"\"\n    # Encode input to get latent parameters\n    x = self.encoder(x)\n    # Split into mean and log standard deviation\n    mean, stddev = x[:-1], jnp.exp(x[-1])\n    # Sample from latent space using reparameterization trick\n    z = mean + stddev * jax.random.normal(jax.random.PRNGKey(0), mean.shape)\n    # Decode latent sample\n    return self.decoder(z)\n</code></pre>"},{"location":"api/experimental/examples/dl/vae_mnist/#examples.experimental.dl.vae_mnist.VAE.__init__","title":"<code>__init__(encoderLayers, decoderLayers)</code>","text":"<p>Initialize VAE with encoder and decoder architectures.</p> <p>Parameters:</p> Name Type Description Default <code>encoderLayers</code> <code>list</code> <p>Layer definitions for encoder</p> required <code>decoderLayers</code> <code>list</code> <p>Layer definitions for decoder</p> required Source code in <code>examples/experimental/dl/vae_mnist.py</code> <pre><code>def __init__(self, encoderLayers, decoderLayers):\n    \"\"\"\n    Initialize VAE with encoder and decoder architectures.\n\n    Args:\n        encoderLayers (list): Layer definitions for encoder\n        decoderLayers (list): Layer definitions for decoder\n    \"\"\"\n    self.encoder = FCNN(encoderLayers)\n    self.decoder = FCNN(decoderLayers)\n</code></pre>"},{"location":"api/experimental/examples/dl/vae_mnist/#examples.experimental.dl.vae_mnist.loss_fn","title":"<code>loss_fn(output, y)</code>","text":"<p>Compute KL divergence loss between output and target.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Array</code> <p>Model output</p> required <code>y</code> <code>Array</code> <p>Target values</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Loss value</p> Source code in <code>examples/experimental/dl/vae_mnist.py</code> <pre><code>def loss_fn(output, y):\n    \"\"\"\n    Compute KL divergence loss between output and target.\n\n    Args:\n        output (jax.Array): Model output\n        y (jax.Array): Target values\n\n    Returns:\n        float: Loss value\n    \"\"\"\n    return jnp.abs(losses.kl_divergence(output.reshape(-1), y.reshape(-1)))\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/classification/example_logistic_regression/","title":"example logistic regresssion","text":"<p>Example Script: logistic_regression_example.py</p> <p>This script demonstrates how to use the LogisticRegression class from the SciREX library to perform classification on the Iris dataset.</p> The example includes <ul> <li>Loading the Iris dataset using sklearn.datasets</li> <li>Preprocessing the data</li> <li>Splitting the data into train and test sets</li> <li>Running the Logistic Regression classifier</li> <li>Evaluating and visualizing the results</li> </ul> Dependencies <ul> <li>numpy</li> <li>pandas</li> <li>matplotlib</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.classification.logistic_regression</li> </ul> Authors <ul> <li>Protyush P. Chowdhury (protyushc@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/classification/example_naive_bayes/","title":"example naive bayes","text":"<p>Module: example_naive_bayes.py</p> <p>This module demonstrates the usage of the NaiveBayes class for Gaussian class-conditional densities using the Iris dataset.</p> Dependencies <ul> <li>scikit-learn</li> <li>pandas</li> <li>numpy</li> <li>scirex.experimental.ml.unsupervised.clustering.kmeans.NaiveBayes</li> </ul> Example Usage <p>Run this script to load the Iris dataset, fit the NaiveBayes classifier, and evaluate its performance metrics.</p> Authors <ul> <li>Protyush P. Chowdhury (protyushc@iisc.ac.in)</li> </ul> Version Info <ul> <li>30/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/","title":"example lasso regression","text":"<p>Example Script: example_lasso_regress.py</p> <p>This script demonstrates how to use the LassoRegressionModel class from the SciREX library to perform regression on synthetic data.</p> The example includes <ul> <li>Generating synthetic regression data using sklearn</li> <li>Fitting the Lasso Regression model</li> <li>Evaluating the model's performance using regression metrics</li> <li>Visualizing the results</li> </ul> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>scirex.experimental.ml.supervised.regression.lasso_regression</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#lasso-regression-example-documentation","title":"Lasso Regression Example Documentation","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#introduction","title":"Introduction","text":"<p>This example demonstrates the usage of the <code>LassoRegressionModel</code> class from the SciREX library to perform regression on synthetic data. Lasso Regression is a regularization technique that helps mitigate overfitting by adding a penalty proportional to the absolute value of the coefficients, leading to sparse models.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#workflow-overview","title":"Workflow Overview","text":"<p>The example covers the following steps:</p> <ol> <li>Generating synthetic regression data using <code>sklearn</code>.</li> <li>Fitting the Lasso Regression model.</li> <li>Evaluating the model's performance using regression metrics.</li> <li>Visualizing the results.</li> </ol>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#requirements","title":"Requirements","text":"<p>Ensure the following dependencies are installed before running the script:</p> <ul> <li><code>numpy</code></li> <li><code>scikit-learn</code></li> <li><code>matplotlib</code></li> <li><code>SciREX</code> library</li> </ul> <p>Install missing dependencies using pip: <pre><code>pip install numpy scikit-learn matplotlib\n</code></pre></p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#example-script","title":"Example Script","text":"<p>Here is the complete example script:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_regression\nfrom scirex.experimental.ml.supervised.regression.lasso_regression import LassoRegressionModel\n\n# Generate synthetic regression data\nX, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n\n# Initialize the Lasso Regression model\nlasso_model = LassoRegressionModel(alpha=1.0, random_state=42)\n\n# Fit the model\nlasso_model.fit(X, y)\n\n# Make predictions\ny_pred_lasso = lasso_model.predict(X)\n\n# Get the model parameters\nparams_lasso = lasso_model.get_model_params()\n\n# Print model parameters\nprint(\"Lasso Regression Model Parameters:\")\nprint(f\"Coefficients: {params_lasso['coefficients']}\")\nprint(f\"Intercept: {params_lasso['intercept']}\")\n\n# Evaluate the model's performance\nmetrics_lasso = lasso_model.evaluation_metrics(y, y_pred_lasso)\nprint(\"\\nLasso Regression Evaluation Metrics:\")\nprint(f\"MSE: {metrics_lasso['mse']:.2f}\")\nprint(f\"MAE: {metrics_lasso['mae']:.2f}\")\nprint(f\"R2 Score: {metrics_lasso['r2']:.2f}\")\n\n# Visualize the regression results\nlasso_model.plot_regression_results(y, y_pred_lasso)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#key-steps-explained","title":"Key Steps Explained","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#1-generating-synthetic-data","title":"1. Generating Synthetic Data","text":"<p>We use <code>sklearn.datasets.make_regression</code> to create a synthetic dataset with one feature and added noise for demonstration purposes:</p> <pre><code>X, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#2-initializing-the-model","title":"2. Initializing the Model","text":"<p>We initialize the <code>LassoRegressionModel</code> with an alpha value for regularization and a fixed random state for reproducibility:</p> <pre><code>lasso_model = LassoRegressionModel(alpha=1.0, random_state=42)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#3-fitting-the-model","title":"3. Fitting the Model","text":"<p>Train the model using the <code>fit</code> method:</p> <pre><code>lasso_model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#4-predictions","title":"4. Predictions","text":"<p>Make predictions using the trained model:</p> <pre><code>y_pred_lasso = lasso_model.predict(X)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#5-retrieving-model-parameters","title":"5. Retrieving Model Parameters","text":"<p>Retrieve the coefficients and intercept:</p> <pre><code>params_lasso = lasso_model.get_model_params()\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#6-evaluating-performance","title":"6. Evaluating Performance","text":"<p>Evaluate the model using common regression metrics such as MSE, MAE, and R2 Score:</p> <pre><code>metrics_lasso = lasso_model.evaluation_metrics(y, y_pred_lasso)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#7-visualization","title":"7. Visualization","text":"<p>Visualize the regression results using the built-in plotting function:</p> <pre><code>lasso_model.plot_regression_results(y, y_pred_lasso)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#sample-output","title":"Sample Output","text":"<p>Here\u2019s an example of the output you can expect:</p> <pre><code>Lasso Regression Model Parameters:\nCoefficients:\nIntercept:\n\nLasso Regression Evaluation Metrics:\nMSE:\nMAE:\nR2 Score:\n</code></pre> <p>The visualization will show the actual vs. predicted values on a scatter plot with the regression line.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#conclusion","title":"Conclusion","text":"<p>This example showcases how to use the <code>LassoRegressionModel</code> class from SciREX for regression tasks. Lasso Regression is particularly useful when feature selection and sparsity are important in models.</p> <p>For more advanced use cases and detailed documentation, visit the SciREX Documentation.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#author","title":"Author","text":"<p>This example was authored by Paranidharan (paranidharan@iisc.ac.in).</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_lasso_regression/#license","title":"License","text":"<p>This script is part of the SciREX library and is licensed under the Apache License 2.0. For more information, visit Apache License 2.0.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/","title":"example linear regression","text":"<p>Example Script: linear_regression_example.py</p> <p>This script demonstrates how to use the LinearRegressionModel class from the SciREX library to perform regression on a synthetic dataset.</p> The example includes <ul> <li>Generating synthetic regression data</li> <li>Training a Linear Regression model</li> <li>Evaluating model performance using regression metrics</li> <li>Visualizing the regression results</li> </ul> Dependencies <ul> <li>numpy</li> <li>matplotlib</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.linear_regression</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#linear-regression-example-documentation","title":"Linear Regression Example Documentation","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#introduction","title":"Introduction","text":"<p>This document demonstrates the usage of the <code>LinearRegressionModel</code> class from the SciREX library to perform regression on synthetic data. Linear Regression provides a baseline regression model without regularization, making it an interpretable and widely used technique for regression tasks.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#workflow-overview","title":"Workflow Overview","text":"<p>The example covers the following steps:</p> <ol> <li>Generating synthetic regression data using <code>sklearn</code>.</li> <li>Fitting the Linear Regression model.</li> <li>Evaluating the model's performance using regression metrics.</li> <li>Visualizing the results.</li> </ol>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#requirements","title":"Requirements","text":"<p>Ensure the following dependencies are installed before running the script:</p> <ul> <li><code>numpy</code></li> <li><code>scikit-learn</code></li> <li><code>matplotlib</code></li> <li><code>SciREX</code> library</li> </ul> <p>Install missing dependencies using pip: <pre><code>pip install numpy scikit-learn matplotlib\n</code></pre></p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#linear-regression-example-script","title":"Linear Regression Example Script","text":"<p>Here is the complete example script for Linear Regression:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_regression\nfrom scirex.experimental.ml.supervised.regression.linear_regression import LinearRegressionModel\n\n# Generate synthetic regression data\nX, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n\n# Initialize the Linear Regression model\nmodel = LinearRegressionModel(random_state=42)\n\n# Fit the model on the training data\nmodel.fit(X, y)\n\n# Make predictions on the training data\ny_pred = model.predict(X)\n\n# Get model parameters (coefficients and intercept)\nparams = model.get_model_params()\nprint(\"Model Parameters:\")\nprint(f\"Coefficients: {params['coefficients']}\")\nprint(f\"Intercept: {params['intercept']}\")\n\n# Evaluate the model using regression metrics\nmetrics = model.evaluation_metrics(y, y_pred)\nprint(\"\\nEvaluation Metrics:\")\nprint(f\"MSE: {metrics['mse']:.2f}\")\nprint(f\"MAE: {metrics['mae']:.2f}\")\nprint(f\"R2 Score: {metrics['r2']:.2f}\")\n\n# Visualize the regression results\nmodel.plot_regression_results(y, y_pred)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#key-steps-explained","title":"Key Steps Explained","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#1-generating-synthetic-data","title":"1. Generating Synthetic Data","text":"<p>We use <code>sklearn.datasets.make_regression</code> to create a synthetic dataset with one feature and added noise for demonstration purposes:</p> <pre><code>X, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#2-initializing-the-model","title":"2. Initializing the Model","text":"<p>Initialize the <code>LinearRegressionModel</code> with an optional random seed for reproducibility:</p> <pre><code>model = LinearRegressionModel(random_state=42)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#3-fitting-the-model","title":"3. Fitting the Model","text":"<p>Train the model using the <code>fit</code> method:</p> <pre><code>model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#4-predictions","title":"4. Predictions","text":"<p>Make predictions using the trained model:</p> <pre><code>y_pred = model.predict(X)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#5-retrieving-model-parameters","title":"5. Retrieving Model Parameters","text":"<p>Retrieve the coefficients and intercept:</p> <pre><code>params = model.get_model_params()\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#6-evaluating-performance","title":"6. Evaluating Performance","text":"<p>Evaluate the model using common regression metrics such as MSE, MAE, and R2 Score:</p> <pre><code>metrics = model.evaluation_metrics(y, y_pred)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#7-visualization","title":"7. Visualization","text":"<p>Visualize the regression results using the built-in plotting function:</p> <pre><code>model.plot_regression_results(y, y_pred)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#sample-output","title":"Sample Output","text":"<pre><code>Model Parameters:\nCoefficients:\nIntercept:\n\nEvaluation Metrics:\nMSE:\nMAE:\nR2 Score:\n</code></pre> <p>The visualization will show the actual vs. predicted values on a scatter plot with the regression line.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#conclusion","title":"Conclusion","text":"<p>This example showcases how to use the <code>LinearRegressionModel</code> class from SciREX for regression tasks. Linear Regression provides a simple and interpretable baseline model, making it suitable for many applications.</p> <p>For more advanced use cases and detailed documentation, visit the SciREX Documentation.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#author","title":"Author","text":"<p>This example was authored by Paranidharan (paranidharan@iisc.ac.in).</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_linear_regression/#license","title":"License","text":"<p>This document is part of the SciREX library and is licensed under the Apache License 2.0. For more information, visit Apache License 2.0.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_polynomial_regression/","title":"example polynomial regression","text":"<p>Example Usage: Polynomial Regression on a Sample Dataset</p> <p>This example demonstrates how to use the PolynomialRegressionModel class from the SciREX library to perform regression on a synthetic dataset with non-linear relationships.</p> Steps <ol> <li>Prepare the data (using a CSV file or a synthetic dataset).</li> <li>Initialize the Polynomial Regression model.</li> <li>Fit the model to the training data.</li> <li>Make predictions and evaluate the model.</li> <li>Visualize the regression results.</li> </ol> <p>Authors: - Paranidharan (paranidharan@iisc.ac.in)</p> Version Info <ul> <li>01/Feb/2025: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/","title":"example ridge regression","text":"<p>Example Script: example_ridge_regress.py</p> <p>This script demonstrates how to use the RidgeRegressionModel class from the SciREX library to perform regression on synthetic data.</p> The example includes <ul> <li>Generating synthetic regression data using sklearn</li> <li>Fitting the Ridge Regression model</li> <li>Evaluating the model's performance using regression metrics</li> <li>Visualizing the results</li> </ul> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>scirex.experimental.ml.supervised.regression.ridge_regression</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#ridge-regression-example-documentation","title":"Ridge Regression Example Documentation","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#introduction","title":"Introduction","text":"<p>This example demonstrates the usage of the <code>RidgeRegressionModel</code> class from the SciREX library to perform regression on synthetic data. Ridge Regression is a regularization technique that helps mitigate overfitting by adding a penalty proportional to the magnitude of the coefficients.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#workflow-overview","title":"Workflow Overview","text":"<p>The example covers the following steps:</p> <ol> <li>Generating synthetic regression data using <code>sklearn</code>.</li> <li>Fitting the Ridge Regression model.</li> <li>Evaluating the model's performance using regression metrics.</li> <li>Visualizing the results.</li> </ol>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#requirements","title":"Requirements","text":"<p>Ensure the following dependencies are installed before running the script:</p> <ul> <li><code>numpy</code></li> <li><code>scikit-learn</code></li> <li><code>matplotlib</code></li> <li><code>SciREX</code> library</li> </ul> <p>Install missing dependencies using pip: <pre><code>pip install numpy scikit-learn matplotlib\n</code></pre></p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#example-script","title":"Example Script","text":"<p>Here is the complete example script:</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_regression\nfrom scirex.experimental.ml.supervised.regression.ridge_regression import RidgeRegressionModel\n\n# Generate synthetic regression data\nX, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n\n# Initialize the Ridge Regression model\nridge_model = RidgeRegressionModel(random_state=42)\n\n# Fit the model\nridge_model.fit(X, y)\n\n# Make predictions\ny_pred_ridge = ridge_model.predict(X)\n\n# Get the model parameters\nparams_ridge = ridge_model.get_model_params()\n\n# Print model parameters\nprint(\"Ridge Regression Model Parameters:\")\nprint(f\"Coefficients: {params_ridge['coefficients']}\")\nprint(f\"Intercept: {params_ridge['intercept']}\")\n\n# Evaluate the model's performance\nmetrics_ridge = ridge_model.evaluation_metrics(y, y_pred_ridge)\nprint(\"\\nRidge Regression Evaluation Metrics:\")\nprint(f\"MSE: {metrics_ridge['mse']:.2f}\")\nprint(f\"MAE: {metrics_ridge['mae']:.2f}\")\nprint(f\"R2 Score: {metrics_ridge['r2']:.2f}\")\n\n# Visualize the regression results\nridge_model.plot_regression_results(y, y_pred_ridge)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#key-steps-explained","title":"Key Steps Explained","text":""},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#1-generating-synthetic-data","title":"1. Generating Synthetic Data","text":"<p>We use <code>sklearn.datasets.make_regression</code> to create a synthetic dataset with one feature and added noise for demonstration purposes:</p> <pre><code>X, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#2-initializing-the-model","title":"2. Initializing the Model","text":"<p>We initialize the <code>RidgeRegressionModel</code> with a fixed random state for reproducibility:</p> <pre><code>ridge_model = RidgeRegressionModel(random_state=42)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#3-fitting-the-model","title":"3. Fitting the Model","text":"<p>Train the model using the <code>fit</code> method:</p> <pre><code>ridge_model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#4-predictions","title":"4. Predictions","text":"<p>Make predictions using the trained model:</p> <pre><code>y_pred_ridge = ridge_model.predict(X)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#5-retrieving-model-parameters","title":"5. Retrieving Model Parameters","text":"<p>Retrieve the coefficients and intercept:</p> <pre><code>params_ridge = ridge_model.get_model_params()\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#6-evaluating-performance","title":"6. Evaluating Performance","text":"<p>Evaluate the model using common regression metrics such as MSE, MAE, and R2 Score:</p> <pre><code>metrics_ridge = ridge_model.evaluation_metrics(y, y_pred_ridge)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#7-visualization","title":"7. Visualization","text":"<p>Visualize the regression results using the built-in plotting function:</p> <pre><code>ridge_model.plot_regression_results(y, y_pred_ridge)\n</code></pre>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#sample-output","title":"Sample Output","text":"<p>Here\u2019s an example of the output you can expect:</p> <pre><code>Ridge Regression Model Parameters:\nCoefficients:\nIntercept:\n\nRidge Regression Evaluation Metrics:\nMSE:\nMAE:\nR2 Score:\n</code></pre> <p>The visualization will show the actual vs. predicted values on a scatter plot with the regression line.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#conclusion","title":"Conclusion","text":"<p>This example showcases how to use the <code>RidgeRegressionModel</code> class from SciREX for regression tasks. Ridge Regression is particularly useful when dealing with multicollinearity or overfitting in datasets.</p> <p>For more advanced use cases and detailed documentation, visit the SciREX Documentation.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#author","title":"Author","text":"<p>This example was authored by Paranidharan (paranidharan@iisc.ac.in).</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_ridge_regression/#license","title":"License","text":"<p>This script is part of the SciREX library and is licensed under the Apache License 2.0. For more information, visit Apache License 2.0.</p>"},{"location":"api/experimental/examples/ml/supervised/regression/example_svr/","title":"example support vector regression","text":"<p>Example Usage: Support Vector Regression (SVR) on a Sample Dataset</p> <pre><code>This example demonstrates how to use the SVRModel class from the SciREX library\nto perform regression on a sample dataset.\n\nSteps:\n    1. Prepare the data (using a CSV file or a synthetic dataset).\n    2. Initialize the SVR model.\n    3. Fit the model to the training data.\n    4. Make predictions and evaluate the model.\n    5. Visualize the regression results.\n\nAuthors:\n    - Paranidharan (paranidharan@iisc.ac.in)\n\nVersion Info:\n    - 01/Feb/2025: Initial version\n</code></pre>"},{"location":"api/experimental/examples/ml/unsupervised/clustering/example_kmeans/","title":"example Kmeans","text":"<p>Example Script: example_kmeans.py</p> <p>This script demonstrates how to use the KMeans clustering class from SciREX library to perform clustering on synthetic datasets. The example shows parameter selection, metrics calculation and visualization capabilities.</p> The example includes <ul> <li>Generating synthetic datasets (blobs and moons) using sklearn</li> <li>Data preprocessing using StandardScaler</li> <li>Running KMeans clustering with automatic parameter selection</li> <li>Evaluating results using multiple metrics</li> <li>Visualizing the clustering results</li> </ul> Dependencies <ul> <li>numpy</li> <li>matplotlib</li> <li>scikit-learn</li> <li>scirex.experimental.ml.unsupervised.clustering.kmeans</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>30/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/ml/unsupervised/clustering/example_kmeans/#examples.experimental.ml.unsupervised.clustering.example_kmeans.main","title":"<code>main()</code>","text":"<p>Main function to demonstrate KMeans clustering on synthetic datasets. Generates two datasets (blobs and moons) and applies KMeans clustering.</p> Source code in <code>examples/experimental/ml/unsupervised/clustering/example_kmeans.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to demonstrate KMeans clustering on synthetic datasets.\n    Generates two datasets (blobs and moons) and applies KMeans clustering.\n    \"\"\"\n    # Generate blob dataset\n    X_blobs, _ = make_blobs(n_samples=1000, centers=4, random_state=42)\n    X_blobs = StandardScaler().fit_transform(X_blobs)\n\n    # Generate moons dataset\n    X_moons, _ = make_moons(n_samples=1000, noise=0.05, random_state=42)\n    X_moons = StandardScaler().fit_transform(X_moons)\n\n    # Initialize KMeans models\n    kmeans_blobs = Kmeans(max_k=10)  # auto-select best k\n    kmeans_moons = Kmeans(n_clusters=4, max_k=10)  # user-defined 4 clusters\n\n    # Run clustering on both datasets\n    run_clustering(kmeans_blobs, X_blobs, \"blobs\")\n    run_clustering(kmeans_moons, X_moons, \"moons\")\n</code></pre>"},{"location":"api/experimental/examples/ml/unsupervised/clustering/example_kmeans/#examples.experimental.ml.unsupervised.clustering.example_kmeans.run_clustering","title":"<code>run_clustering(model, X, dataset_name)</code>","text":"<p>Run clustering model and display results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Initialized clustering model instance</p> required <code>X</code> <code>ndarray</code> <p>Input data matrix</p> required <code>dataset_name</code> <code>str</code> <p>Name of dataset for display purposes</p> required Prints <ul> <li>Clustering metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)</li> <li>Number of clusters (if applicable)</li> </ul> Displays <ul> <li>Clustering visualization plot</li> </ul> Source code in <code>examples/experimental/ml/unsupervised/clustering/example_kmeans.py</code> <pre><code>def run_clustering(model, X, dataset_name: str):\n    \"\"\"\n    Run clustering model and display results.\n\n    Args:\n        model: Initialized clustering model instance\n        X (np.ndarray): Input data matrix\n        dataset_name (str): Name of dataset for display purposes\n\n    Prints:\n        - Clustering metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)\n        - Number of clusters (if applicable)\n\n    Displays:\n        - Clustering visualization plot\n    \"\"\"\n    # Run model and get results\n    results = model.run(data=X)\n\n    # Print metrics\n    print(f\"\\n--- {model.model_type.upper()} on {dataset_name} dataset ---\")\n    print(f\"Silhouette Score:      {results['silhouette_score']:.3f}\")\n    print(f\"Calinski-Harabasz:     {results['calinski_harabasz_score']:.3f}\")\n    print(f\"Davies-Bouldin:        {results['davies_bouldin_score']:.3f}\")\n\n    if hasattr(model, \"n_clusters\"):\n        print(f\"Number of clusters:    {model.n_clusters}\")\n\n    # Generate and display plot\n    fig, plot_path = model.plots(X, model.labels)\n\n    if not os.path.exists(plot_path):\n        print(f\"[WARNING] Plot file not found: {plot_path}\")\n    else:\n        img = mpimg.imread(plot_path)\n        plt.figure(figsize=(7, 5))\n        plt.imshow(img)\n        plt.title(\n            f\"{model.model_type.upper()} Clustering Plot for {dataset_name}\",\n            fontsize=14,\n        )\n        plt.axis(\"off\")\n        plt.show()\n</code></pre>"},{"location":"api/experimental/examples/model%20compression/pruned_base_mnist/","title":"pruned base mnist","text":"<p>Example Script: mnist_pruning_baseline.py This script establishes a baseline model for pruning experiments using the MNIST dataset through TensorFlow Model Optimization Toolkit.</p> This example includes <ul> <li>Training baseline model on MNIST dataset</li> <li>Performance evaluation metrics</li> </ul> Authors <ul> <li>Nithyashree R (nithyashreer@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/model%20compression/pruned_mnist/","title":"pruned mnist","text":"<p>Example Script: pruned_mnist.py</p> <p>This script demonstrates the application of model pruning on the MNIST dataset using TensorFlow Model Optimization Toolkit. The pruning process reduces model size while maintaining accuracy.</p> This example includes <ul> <li>Loading and preprocessing MNIST dataset</li> <li>Implementing model pruning for compression</li> <li>Training and evaluating pruned models</li> </ul> Authors <ul> <li>Nithyashree R (nithyashreer@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/model%20compression/quantized_base_mnist/","title":"quantized base mnist","text":"<p>Example Script: quantization_base_mnist.py</p> <p>This script demonstrates quantization-aware training on the MNIST dataset. It gives the performance of the baseline model.</p> This example includes <ul> <li>Training baseline model on MNIST dataset</li> <li>Evaluating model accuracy before QAT</li> </ul> Authors <ul> <li>Nithyashree R (nithyashreer@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/model%20compression/quantized_mnist/","title":"quantized mnist","text":"<p>Example Script: quantized_mnist.py</p> <p>This script demonstrates model quantization techniques on the MNIST dataset. Compares accuracy of different quantization approaches.</p> This example includes <ul> <li>Applying quantization-aware training (QAT)</li> <li>Implementing post-training quantization</li> <li>Comparing model sizes and accuracies</li> </ul> Authors <ul> <li>Nithyashree R (nithyashreer@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/","title":"FastVPINNs - Training Tutorial - Helmholtz Problem","text":"<p>In this notebook, we will explore how to use FastVPINNs to solve the Helmholtz equation using custom loss functions and neural networks.</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#hp-variational-physics-informed-neural-networks-hp-vpinns","title":"hp-Variational Physics-Informed Neural Networks (hp-VPINNs)","text":"<p>Variational Physics-Informed Neural Networks (VPINNs) are a specialized class of Physics-Informed Neural Networks (PINNs) that are trained using the variational formulation of governing equations. The hp-VPINNs extend this concept by incorporating hp-FEM principles such as h- and p-refinement to enhance solution accuracy.</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The 2D Helmholtz equation is given by:</p> \\[ -\\Delta u + ku = f \\] <p>where: - \\(u\\) is the solution - \\(k\\) is the Helmholtz coefficient - \\(f\\) is the source term - \\(\\Delta\\) is the Laplacian operator</p> <p>To obtain the weak form, we multiply by a test function \\(v\\) and integrate over the domain \\(\\Omega\\):</p> \\[ \\int_{\\Omega} (-\\Delta u + ku)v \\, dx = \\int_{\\Omega} f v \\, dx \\] <p>Applying integration by parts to the Laplacian term:</p> \\[ \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx + \\int_{\\Omega} kuv \\, dx - \\int_{\\partial \\Omega} \\nabla u \\cdot \\mathbf{n} v \\, ds = \\int_{\\Omega} f v \\, dx \\] <p>With \\(v = 0\\) on \\(\\partial \\Omega\\), the boundary integral vanishes, giving us the weak form:</p> \\[ \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx + \\int_{\\Omega} kuv \\, dx = \\int_{\\Omega} f v \\, dx \\]"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#problem-setup","title":"Problem Setup","text":"<p>In this example, we solve the Helmholtz equation with the following exact solution:</p> \\[ u(x,y) = (x + y)\\sin(\\pi x)\\sin(\\pi y) \\]"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#key-parameters","title":"Key Parameters","text":""},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#geometry-parameters","title":"Geometry Parameters:","text":"<ul> <li>Mesh type: Quadrilateral</li> <li>Domain: \\([0,1] \\times [0,1]\\)</li> <li>Cells: \\(2\\times2\\) grid</li> <li>Boundary points: 400</li> <li>Test points: \\(100\\times100\\) grid</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#finite-element-parameters","title":"Finite Element Parameters:","text":"<ul> <li>Order: 6 (Legendre)</li> <li>Quadrature: Gauss-Jacobi (order 10)</li> <li>Transformation: Bilinear</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#neural-network-parameters","title":"Neural Network Parameters:","text":"<ul> <li>Architecture: \\([2, 30, 30, 30, 1]\\)</li> <li>Activation: \\(\\tanh\\)</li> <li>Learning rate: 0.002</li> <li>Boundary loss penalty (\\(\\beta\\)): 10</li> <li>Training epochs: 20,000</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#boundary-conditions","title":"Boundary Conditions","text":"<p>The boundary conditions are implemented through four functions defining values on each boundary:</p> <pre><code>def left_boundary(x, y):\n    return (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n\ndef right_boundary(x, y):\n    return (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n\ndef top_boundary(x, y):\n    return (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n\ndef bottom_boundary(x, y):\n    return (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n\n### Source Term\n\nThe source term f(x,y) is derived through the method of manufactured solutions:\n\n```python\ndef rhs(x, y):\n    term1 = 2 * np.pi * np.cos(np.pi * y) * np.sin(np.pi * x)\n    term2 = 2 * np.pi * np.cos(np.pi * x) * np.sin(np.pi * y)\n    term3 = (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n    term4 = -2 * (np.pi**2) * (x + y) * np.sin(np.pi * x) * np.sin(np.pi * y)\n    return term1 + term2 + term3 + term4\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Mesh Generation:</li> <li>Create a 2\u00d72 quadrilateral mesh</li> <li>Generate boundary points</li> <li> <p>Set up test points for solution evaluation</p> </li> <li> <p>Finite Element Space:</p> </li> <li>Define Legendre basis functions</li> <li>Set up quadrature rules</li> <li> <p>Configure boundary conditions</p> </li> <li> <p>Neural Network Model:</p> </li> <li>Create a dense neural network with 3 hidden layers</li> <li>Configure the Helmholtz loss function</li> <li> <p>Set up the training parameters</p> </li> <li> <p>Training Process:</p> </li> <li>Train for 20,000 epochs</li> <li>Track loss and timing metrics</li> <li> <p>Evaluate solution accuracy</p> </li> <li> <p>Visualization and Analysis:</p> </li> <li>Plot training loss</li> <li>Compare exact and predicted solutions</li> <li>Generate error plots</li> <li>Calculate error metrics (L2, L1, L\u221e norms)</li> </ol>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#key-differences-from-poisson-problem","title":"Key Differences from Poisson Problem","text":"<ol> <li>PDE Structure:</li> <li>Addition of the ku term</li> <li>Modified variational form</li> <li> <p>Different source term computation</p> </li> <li> <p>Loss Function:</p> </li> <li>Uses <code>pde_loss_helmholtz</code> instead of <code>pde_loss_poisson</code></li> <li> <p>Includes Helmholtz coefficient in bilinear parameters</p> </li> <li> <p>Boundary Conditions:</p> </li> <li>Non-zero Dirichlet conditions</li> <li>More complex boundary value functions</li> </ol>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_helmholtz2d/#results-analysis","title":"Results Analysis","text":"<p>The implementation generates four key visualizations: 1. Training loss convergence 2. Exact solution contour 3. Predicted solution contour 4. Absolute error distribution</p> <p>Error metrics are computed in various norms: - L2 norm (global accuracy) - L1 norm (average absolute error) - L\u221e norm (maximum error) - Relative versions of each norm</p> <p>These metrics help assess the solution quality and convergence characteristics of the method.</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/","title":"example poisson2d","text":""},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#fastvpinns-training-tutorial-poisson-problem","title":"FastVPINNs - Training Tutorial - Poisson Problem","text":"<p>In this notebook, we will try to understand, how to write FastVPINNs using Custom loss functions and custom neural networks.</p> <p>Author : Thivin Anandh Linkedin GitHub Portfolio</p> <p>Paper: FastVPINNs: Tensor-driven acceleration of VPINNs for complex geometries</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-hp-variational-physics-informed-neural-networks-hp-vpinns","title":"hp-Variational Physics-Informed Neural Networks (hp-VPINNs)","text":"<p>Variational Physics-Informed Neural Networks (VPINNs) are a special class of Physics-Informed Neural Networks (PINNs) that are trained using the variational formulation of the governing equations. Variational formulation us used in conventional numerical methods like Finite Element Method (FEM) to solve Partial Differential Equations (PDEs). hp-VPINNs are a special class of VPINNs that uses the concepts of hp-FEM such as h- and p-refinement to improve the accuracy of the solution. h-refinement is the process of refining the mesh size, whereas p-refinement is the process of increasing the order of the basis functions. For more information, please refer the paper by Kharazmi et al. (2021) here.</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-mathematical-formulation","title":"Mathematical Formulation","text":"<p>Lets consider the 2D Poisson equation as an example. The Poisson equation is given by</p> \\[ \\nabla^2 u = f \\] <p>where \\(u\\) is the solution, \\(f\\) is the source term and \\(\\nabla^2\\) is the Laplacian operator. Now, in order to get the weak form of the Poisson equation, we multiply the equation by a test function \\(v\\) and integrate over the domain \\(\\Omega\\).</p> \\[ \\int_{\\Omega} \\nabla^2 u v \\, dx = \\int_{\\Omega} f v \\, dx \\] <p>Now, lets apply integration by parts to the left hand side of the equation to get the weak form of the Poisson equation.</p> \\[ \\int_{\\Omega} \\nabla^2 u v \\, dx = -\\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx + \\int_{\\partial \\Omega} \\nabla u \\cdot n v \\, ds \\] <p>where \\(n\\) is the normal vector to the boundary \\(\\partial \\Omega\\). The function \\(v\\) is choosen such that \\(v = 0\\) on the boundary \\(\\partial \\Omega\\). Therefore, the boundary integral term becomes zero. Hence, the weak form of the Poisson equation is given by</p> <p>$$</p> <p>\\int_{\\Omega} \\nabla u \\cdot \\nabla v , dx = \\int_{\\Omega} f v , dx $$</p> <p>Now, we can write the above equation in the variational form as</p> \\[ \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\Omega} f v \\, dx = 0 \\] <p>with the dirichlet boundary condition \\(u = g\\) on the boundary \\(\\partial \\Omega\\).</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-imports","title":"## Imports","text":"<p>Lets import the necessary functions for this tutorial from the FastVPINNs library.</p> <pre><code># Common library imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport tensorflow as tf\nimport time\nfrom tqdm import tqdm\n\n# Fastvpinns Modules\nfrom fastvpinns.geometry.geometry_2d import Geometry_2D\nfrom fastvpinns.fe.fespace2d import Fespace2D\nfrom fastvpinns.data.datahandler2d import DataHandler2D\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-setting-up-problem-parameters","title":"## Setting up Problem Parameters","text":"<p>In this section, we will setup all the parameters related to the Poisson problem such as Geometries, Test Function spaces, Neural Network Architectures, Learning Rates Etc</p> <p>For Additional information on all Parameters , please refer the documentation here</p>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#geometry-parameters","title":"Geometry Parameters","text":"<ul> <li>i_mesh_type : Type of mesh elements</li> <li>i_mesh_generation_method: Internal mesh generation or external mesh</li> <li>i_n_test_points_x: Number of test points in x direction</li> <li>i_n_test_points_y: Number of test points in y direction</li> <li>i_output_path: Output path for the results</li> <li>i_x_min: Minimum value of x</li> <li>i_x_max: Maximum value of x</li> <li>i_y_min: Minimum value of y</li> <li>i_y_max: Maximum value of y</li> <li>i_n_cells_x: Number of cells in x direction</li> <li>i_n_cells_y: Number of cells in y direction</li> <li>i_n_boundary_points: Number of boundary points</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#finite-element-parameters","title":"Finite Element Parameters","text":"<ul> <li>i_fe_order: Order of the finite element basis functions</li> <li>i_fe_type: Type of the finite element basis functions</li> <li>i_quad_order: Order of the quadrature rule</li> <li>i_quad_type: Name of the quadrature rule</li> <li>i_fe_transform: Bilinear or Affine transformation</li> </ul>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#neural-network-parameters","title":"Neural Network Parameters","text":"<ul> <li>i_learning_rate_dict: Dictionary containing the learning rates for     the neural networks</li> <li>i_data_type: Data type float32 or float64</li> <li>i_activation: Activation function for the neural networks</li> </ul> <pre><code>i_mesh_type = \"quadrilateral\" # \"quadrilateral\"\ni_mesh_generation_method = \"internal\" # \"internal\" or \"external\"\ni_x_min = 0 # minimum x value\ni_x_max = 1 # maximum x value\ni_y_min = 0 # minimum y value\ni_y_max = 1 # maximum y value\ni_n_cells_x = 2 # Number of cells in the x direction\ni_n_cells_y = 2 # Number of cells in the y direction\ni_n_boundary_points = 400 # Number of points on the boundary\ni_output_path = \"output/poisson_2d\" # Output path\n\ni_n_test_points_x = 100 # Number of test points in the x direction\ni_n_test_points_y = 100 # Number of test points in the y direction\n\n# fe Variables\ni_fe_order = 6 # Order of the finite element space\ni_fe_type = \"legendre\"\ni_quad_order = 10 # 10 points in 1D, so 100 points in 2D for one cell\ni_quad_type = \"gauss-jacobi\"\n\n# Neural Network Variables\ni_learning_rate_dict = {\n    \"initial_learning_rate\" : 0.001, # Initial learning rate\n    \"use_lr_scheduler\" : False, # Use learning rate scheduler\n    \"decay_steps\": 1000, # Decay steps\n    \"decay_rate\": 0.96, # Decay rate\n    \"staircase\": True, # Staircase Decay\n}\n\ni_dtype = tf.float32\ni_activation = \"tanh\"\ni_beta = 10 # Boundary Loss Penalty ( Adds more weight to the boundary loss)\n\n# Epochs\ni_num_epochs = 20000\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-setting-up-boundary-conditions-and-boundary-values","title":"## Setting up Boundary Conditions and Boundary Values","text":"<p>In this section, we will setup the boundary conditions and boundary values for the Poisson problem. lets take an example, where the exact solution of the problem is given by</p> \\[ u(x,y) =-1.0 \\sin(2.0  \\pi x) \\sin(2.0 \\pi y) \\] <p>The values of \\(u\\) will be \\(0\\) on the boundary \\(\\partial \\Omega\\). The source term \\(f\\) ( from method of manufactured solutions) is given by</p> \\[ f(x,y) = -2(2.0\\pi)^2 \\sin(2.0\\pi x) \\sin(2.0\\pi y) \\] <p>As per internal mesh generation, the boundary points are generated automatically.Further, the bottom boundary is assigned a tag of 1000, right boundary is assigned a tag of 1001, top boundary is assigned a tag of 1002 and left boundary is assigned a tag of 1003. The boundary conditions are given as follows</p> <p>Currently, this library supports only Dirichlet Boundary Conditions. the Development branches of these versions have Neumann Boundary Conditions.</p> <pre><code>def left_boundary(x, y):\n    \"\"\"\n    This function will return the boundary value for given component of a boundary\n    \"\"\"\n    val = 0.0\n    return np.ones_like(x) * val\n\n\ndef right_boundary(x, y):\n    \"\"\"\n    This function will return the boundary value for given component of a boundary\n    \"\"\"\n    val = 0.0\n    return np.ones_like(x) * val\n\n\ndef top_boundary(x, y):\n    \"\"\"\n    This function will return the boundary value for given component of a boundary\n    \"\"\"\n    val = 0.0\n    return np.ones_like(x) * val\n\n\ndef bottom_boundary(x, y):\n    \"\"\"\n    This function will return the boundary value for given component of a boundary\n    \"\"\"\n    val = 0.0\n    return np.ones_like(x) * val\n\n\ndef rhs(x, y):\n    \"\"\"\n    This function will return the value of the rhs at a given point\n    \"\"\"\n    omegaX = 2.0 * np.pi\n    omegaY = 2.0 * np.pi\n    f_temp = -2.0 * (omegaX**2) * (np.sin(omegaX * x) * np.sin(omegaY * y))\n\n    return f_temp\n\n\ndef exact_solution(x, y):\n    \"\"\"\n    This function will return the exact solution at a given point\n    \"\"\"\n    # If the exact Solution does not have an analytical expression, leave the value as 0(zero)\n    # it can be set using `np.ones_like(x) * 0.0` and then ignore the errors and the error plots generated.\n\n    omegaX = 2.0 * np.pi\n    omegaY = 2.0 * np.pi\n    val = -1.0 * np.sin(omegaX * x) * np.sin(omegaY * y)\n\n    return val\n\n\ndef get_boundary_function_dict():\n    \"\"\"\n    This function will return a dictionary of boundary functions\n    \"\"\"\n    return {1000: bottom_boundary, 1001: right_boundary, 1002: top_boundary, 1003: left_boundary}\n\n\ndef get_bound_cond_dict():\n    \"\"\"\n    This function will return a dictionary of boundary conditions\n    \"\"\"\n    return {1000: \"dirichlet\", 1001: \"dirichlet\", 1002: \"dirichlet\", 1003: \"dirichlet\"}\n\n\ndef get_bilinear_params_dict():\n    \"\"\"\n    This function will return a dictionary of bilinear parameters\n    \"\"\"\n    eps = 1.0\n\n    return {\"eps\": eps}\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-create-an-output-folder","title":"Create an Output Folder","text":"<pre><code>## CREATE OUTPUT FOLDER\n# use pathlib to create the folder,if it does not exist\nfolder = Path(i_output_path)\n# create the folder if it does not exist\nif not folder.exists():\n    folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#obtain-the-boundary-conditions-and-boundary-values","title":"Obtain the Boundary conditions and Boundary values","text":"<pre><code># get the boundary function dictionary from example file\nbound_function_dict, bound_condition_dict = get_boundary_function_dict(), get_bound_cond_dict()\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#generate-internal-mesh","title":"Generate internal mesh","text":"<p>This calls the geometry module to generate the internal mesh and set up all cell and boundary information.</p> <pre><code># Initiate a Geometry_2D object\ndomain = Geometry_2D(\n    i_mesh_type, i_mesh_generation_method, i_n_test_points_x, i_n_test_points_y, i_output_path\n)\n\n# load the mesh\ncells, boundary_points = domain.generate_quad_mesh_internal(\n    x_limits=[i_x_min, i_x_max],\n    y_limits=[i_y_min, i_y_max],\n    n_cells_x=i_n_cells_x,\n    n_cells_y=i_n_cells_y,\n    num_boundary_points=i_n_boundary_points,\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#generate-fe-space","title":"Generate fe Space","text":"<p>This module is the core module of the library. It generates the finite element space and sets up the basis functions and quadrature rules. It preassembles the test function matrices for each cell and assigns boundary values to the boundary points.</p> <pre><code>fespace = Fespace2D(\n        mesh=domain.mesh,\n        cells=cells,\n        boundary_points=boundary_points,\n        cell_type=domain.mesh_type,\n        fe_order=i_fe_order,\n        fe_type=i_fe_type,\n        quad_order=i_quad_order,\n        quad_type=i_quad_type,\n        fe_transformation_type=\"bilinear\",\n        bound_function_dict=bound_function_dict,\n        bound_condition_dict=bound_condition_dict,\n        forcing_function=rhs,\n        output_path=i_output_path,\n        generate_mesh_plot=True,\n    )\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#datahandler","title":"Datahandler","text":"<p>This module is used to convert the data into tensorflow datasets. It also contains functions which assemble the test function and other values into tensor format, which will be used for training</p> <pre><code># instantiate data handler\ndatahandler = DataHandler2D(fespace, domain, dtype=i_dtype)\n\nparams_dict = {}\nparams_dict['n_cells'] = fespace.n_cells\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-model","title":"Model","text":"<p>In this section, we will use the neural Network module available within the FastVPINNs library to create and train the neural networks. They can be imported from the \\\"model\\\" module of fastvpinns.</p> <pre><code>fastvpinns.model.model import DenseModel\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-loss-function","title":"Loss Function","text":"<p>The tensor based variational loss formulation for most of scalar problems can be imported from \\\"physics\\\" modules.</p> <pre><code>from fastvpinns.physics.poisson2d import pde_loss_poisson\n</code></pre> <pre><code>from fastvpinns.model.model import DenseModel\nfrom fastvpinns.physics.poisson2d import pde_loss_poisson\n\nparams_dict = {}\nparams_dict['n_cells'] = fespace.n_cells\n\n# get the input data for the PDE\ntrain_dirichlet_input, train_dirichlet_output = datahandler.get_dirichlet_input()\n\n# get bilinear parameters\n# this function will obtain the values of the bilinear parameters from the model\n# and convert them into tensors of desired dtype\nbilinear_params_dict = datahandler.get_bilinear_params_dict_as_tensors(get_bilinear_params_dict)\n\nmodel = DenseModel(\n    layer_dims=[2, 30, 30, 30, 1],\n    learning_rate_dict=i_learning_rate_dict,\n    params_dict=params_dict,\n    loss_function=pde_loss_poisson,\n    input_tensors_list=[datahandler.x_pde_list, train_dirichlet_input, train_dirichlet_output],\n    orig_factor_matrices=[\n        datahandler.shape_val_mat_list,\n        datahandler.grad_x_mat_list,\n        datahandler.grad_y_mat_list,\n    ],\n    force_function_list=datahandler.forcing_function_list,\n    tensor_dtype=i_dtype,\n    activation=i_activation,\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-train-the-model","title":"Train the model","text":"<p>Now, we will train the model to solve the Poisson problem using the custom loss function and custom neural network model.</p> <pre><code>loss_array = []  # total loss\ntime_array = []  # time taken for each epoch\n\n\nfor epoch in tqdm(range(i_num_epochs)):\n        # Train the model\n        batch_start_time = time.time()\n        loss = model.train_step(beta=i_beta, bilinear_params_dict=bilinear_params_dict)\n        elapsed = time.time() - batch_start_time\n\n        # print(elapsed)\n        time_array.append(elapsed)\n\n        loss_array.append(loss['loss'])\n</code></pre>"},{"location":"api/experimental/examples/sciml/fastvpinns/example_poisson2d/#-visualise-loss-and-results","title":"Visualise loss and Results","text":"<pre><code># predict the values for the test points\ntest_points = domain.get_test_points()\nprint(f\"[bold]Number of Test Points = [/bold] {test_points.shape[0]}\")\ny_exact = exact_solution(test_points[:, 0], test_points[:, 1])\n\n# Get predicted values from the model\ny_pred = model(test_points).numpy()\ny_pred = y_pred.reshape(-1)\n\n# compute the error\nerror = np.abs(y_exact - y_pred)\n\n# plot a 2x2 Grid, loss plot, exact solution, predicted solution and error\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# loss plot\naxs[0, 0].plot(loss_array)\naxs[0, 0].set_title(\"Loss Plot\")\naxs[0, 0].set_xlabel(\"Epochs\")\naxs[0, 0].set_ylabel(\"Loss\")\naxs[0, 0].set_yscale(\"log\")\n\n# exact solution\n# contour plot of the exact solution\naxs[0, 1].tricontourf(test_points[:, 0], test_points[:, 1], y_exact, 100)\naxs[0, 1].set_title(\"Exact Solution\")\naxs[0, 1].set_xlabel(\"x\")\naxs[0, 1].set_ylabel(\"y\")\n# add colorbar\ncbar = plt.colorbar(axs[0, 1].collections[0], ax=axs[0, 1])\n\n\n# predicted solution\n# contour plot of the predicted solution\naxs[1, 0].tricontourf(test_points[:, 0], test_points[:, 1], y_pred, 100)\naxs[1, 0].set_title(\"Predicted Solution\")\naxs[1, 0].set_xlabel(\"x\")\naxs[1, 0].set_ylabel(\"y\")\n# add colorbar\ncbar = plt.colorbar(axs[1, 0].collections[0], ax=axs[1, 0])\n\n# error plot\n# contour plot of the error\naxs[1, 1].tricontourf(test_points[:, 0], test_points[:, 1], error, 100)\naxs[1, 1].set_title(\"Error\")\naxs[1, 1].set_xlabel(\"x\")\naxs[1, 1].set_ylabel(\"y\")\n# add colorbar\ncbar = plt.colorbar(axs[1, 1].collections[0], ax=axs[1, 1])\n\n\nplt.tight_layout()\n\n\n# print error statistics\nl2_error = np.sqrt(np.mean(error ** 2))\nl1_error = np.mean(np.abs(error))\nl_inf_error = np.max(np.abs(error))\nrel_l2_error = l2_error / np.sqrt(np.mean(y_exact ** 2))\nrel_l1_error = l1_error / np.mean(np.abs(y_exact))\nrel_l_inf_error = l_inf_error / np.max(np.abs(y_exact))\n\n# print the error statistics in a formatted table\nerror_df = pd.DataFrame(\n    {\n        \"L2 Error\": [l2_error],\n        \"L1 Error\": [l1_error],\n        \"L_inf Error\": [l_inf_error],\n        \"Relative L2 Error\": [rel_l2_error],\n        \"Relative L1 Error\": [rel_l1_error],\n        \"Relative L_inf Error\": [rel_l_inf_error],\n    }\n)\nprint(error_df)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/","title":"Fourier Neural Operator (FNO) - Training Tutorial - 1D Advection Problem","text":""},{"location":"api/experimental/examples/sciml/fno/example_advection/#overview","title":"Overview","text":"<p>This tutorial demonstrates implementing a Fourier Neural Operator (FNO) to solve the 1D advection equation using JAX and Equinox.</p>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The 1D advection equation:</p> \\[ \\frac{\\partial u}{\\partial t} + v\\frac{\\partial u}{\\partial x} = 0 \\] <p>where: - \\(u(x,t)\\): solution (wave amplitude) - \\(v\\): wave velocity - \\(x\\): spatial coordinate - \\(t\\): time</p>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#initial-and-boundary-conditions","title":"Initial and Boundary Conditions","text":"<ul> <li>Initial condition: \\(u(x,0) = u_0(x)\\)</li> <li>Periodic boundary conditions: \\(u(0,t) = u(2\\pi,t)\\)</li> </ul>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#implementation-parameters","title":"Implementation Parameters","text":""},{"location":"api/experimental/examples/sciml/fno/example_advection/#domain-parameters","title":"Domain Parameters","text":"<pre><code>nx = 64  # Spatial resolution\nnt = 100  # Temporal resolution\nL = 2 * \u03c0  # Domain length\nT = 2.0  # Final time\nv = 1.0  # Wave velocity\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#dataset-parameters","title":"Dataset Parameters","text":"<pre><code>n_samples = 1200  # Total samples\ntrain_samples = 1000\ntest_samples = 200\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#fno-architecture","title":"FNO Architecture","text":"<pre><code>model = FNO1d(\n    in_channels=2,\n    out_channels=1,\n    modes=16,\n    width=64,\n    activation=jax.nn.gelu,\n    n_blocks=4\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#code-components","title":"Code Components","text":""},{"location":"api/experimental/examples/sciml/fno/example_advection/#data-generation","title":"Data Generation","text":"<pre><code>def generate_advection_data(n_samples=1200, nx=64, nt=100, v=1.0):\n    \"\"\"Generate data for the 1D advection equation\"\"\"\n    # Domain setup\n    L = 2 * jnp.pi\n    dx = L / nx\n    x = jnp.linspace(0, L, nx)\n\n    # Time domain\n    T = 2.0\n    dt = T / nt\n    t = jnp.linspace(0, T, nt)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#initial-condition-generator","title":"Initial Condition Generator","text":"<pre><code>def generate_initial_condition(key):\n    \"\"\"Generate random sinusoidal initial conditions\"\"\"\n    k1, k2 = jax.random.split(key)\n    n_waves = 3\n    amplitudes = jax.random.uniform(k1, (n_waves,), minval=0.1, maxval=1.0)\n    frequencies = jax.random.randint(k2, (n_waves,), minval=1, maxval=4)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#advection-solver","title":"Advection Solver","text":"<pre><code>def solve_advection(u0):\n    \"\"\"Solve using upwind scheme\"\"\"\n    u = jnp.zeros((nt, nx))\n    u = u.at[0].set(u0)\n\n    for n in range(1, nt):\n        if v &gt; 0:\n            u = u.at[n].set(\n                u[n-1] - v * dt/dx * (u[n-1] - jnp.roll(u[n-1], 1))\n            )\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#training-loop","title":"Training Loop","text":"<pre><code>@eqx.filter_jit\ndef make_step(model, opt_state, batch):\n    \"\"\"Single training step\"\"\"\n    def loss_fn(model):\n        pred = jax.vmap(model)(batch[0])\n        return jnp.mean((pred - batch[1])**2)\n\n    loss, grads = eqx.filter_value_and_grad(loss_fn)(model)\n    updates, opt_state = optimizer.update(grads, opt_state, model)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#results-visualization","title":"Results Visualization","text":"<pre><code># Example prediction\nplt.figure()\nplt.plot(spatial_grid, test_x[0, 0], label='Initial condition')\nplt.plot(spatial_grid, test_y[0, 0], label='True solution')\nplt.plot(spatial_grid, test_pred[0, 0], '--', label='FNO prediction')\n\n# Training loss\nplt.figure()\nplt.semilogy(losses)\n\n# Error analysis\nplt.figure()\nplt.plot(spatial_grid, jnp.abs(test_pred[0, 0] - test_y[0, 0]))\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_advection/#output-structure","title":"Output Structure","text":"<pre><code>outputs/fno/advection/\n\u251c\u2500\u2500 advection_example.png\n\u251c\u2500\u2500 advection_loss.png\n\u2514\u2500\u2500 advection_error.png\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/","title":"Fourier Neural Operator (FNO) - Training Tutorial - Burgers Equation","text":""},{"location":"api/experimental/examples/sciml/fno/example_burgers/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The 1D Burgers equation:</p> \\[ \\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = \\nu\\frac{\\partial^2 u}{\\partial x^2} \\] <p>where: - \\(u(x,t)\\): velocity field - \\(\\nu\\): viscosity coefficient - \\(x\\): spatial coordinate - \\(t\\): time</p>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#implementation-parameters","title":"Implementation Parameters","text":""},{"location":"api/experimental/examples/sciml/fno/example_burgers/#domain-parameters","title":"Domain Parameters","text":"<pre><code>x_domain = [0, 2\u03c0]\nspatial_resolution = 8192  # Full resolution\ntraining_resolution = 256  # Subsampled for training\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#dataset-parameters","title":"Dataset Parameters","text":"<pre><code>train_samples = 1000\ntest_samples = 200\nbatch_size = 100\nepochs = 200\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#fno-architecture","title":"FNO Architecture","text":"<pre><code>fno = FNO1d(\n    in_channels=2,    # Initial condition + spatial coordinate\n    out_channels=1,   # Solution at t=1\n    modes=16,         # Number of Fourier modes\n    width=64,         # Channel width\n    activation=jax.nn.relu,\n    n_blocks=4\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#implementation-steps","title":"Implementation Steps","text":""},{"location":"api/experimental/examples/sciml/fno/example_burgers/#1-data-loading","title":"1. Data Loading","text":"<pre><code>data = scipy.io.loadmat(\"burgers_data_R10.mat\")\na, u = data[\"a\"], data[\"u\"]  # Initial conditions and solutions\n\n# Add channel dimension and mesh\na = a[:, jnp.newaxis, :]\nu = u[:, jnp.newaxis, :]\nmesh = jnp.linspace(0, 2 * jnp.pi, u.shape[-1])\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#2-data-preprocessing","title":"2. Data Preprocessing","text":"<pre><code># Combine initial condition with mesh information\nmesh_shape_corrected = jnp.repeat(mesh[jnp.newaxis, jnp.newaxis, :], u.shape[0], axis=0)\na_with_mesh = jnp.concatenate((a, mesh_shape_corrected), axis=1)\n\n# Train-test split\ntrain_x, test_x = a_with_mesh[:1000], a_with_mesh[1000:1200]\ntrain_y, test_y = u[:1000], u[1000:1200]\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#3-training-loop","title":"3. Training Loop","text":"<pre><code>@eqx.filter_jit\ndef make_step(model, state, x, y):\n    loss, grad = eqx.filter_value_and_grad(loss_fn)(model, x, y)\n    val_loss = loss_fn(model, test_x[..., ::32], test_y[..., ::32])\n    updates, new_state = optimizer.update(grad, state, model)\n    new_model = eqx.apply_updates(model, updates)\n    return new_model, new_state, loss, val_loss\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#4-evaluation-metrics","title":"4. Evaluation Metrics","text":"<pre><code>def relative_l2_norm(pred, ref):\n    diff_norm = jnp.linalg.norm(pred - ref)\n    ref_norm = jnp.linalg.norm(ref)\n    return diff_norm / ref_norm\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#results-analysis","title":"Results Analysis","text":"<p>The implementation generates five visualizations:</p> <ol> <li><code>initial_vs_after.png</code>: Initial condition vs solution at t=1</li> <li><code>loss.png</code>: Training and validation loss curves</li> <li><code>prediction.png</code>: Model prediction vs ground truth</li> <li><code>difference.png</code>: Error analysis</li> <li><code>superresolution.png</code>: Zero-shot superresolution capability</li> </ol>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#key-performance-metrics","title":"Key Performance Metrics","text":"<ul> <li>Relative L2 error: ~1e-2</li> <li>Training time: 200 epochs</li> <li>Resolution invariance demonstrated through superresolution</li> </ul>"},{"location":"api/experimental/examples/sciml/fno/example_burgers/#output-directory-structure","title":"Output Directory Structure","text":"<pre><code>outputs/fno/burgers/\n\u251c\u2500\u2500 initial_vs_after.png\n\u251c\u2500\u2500 loss.png\n\u251c\u2500\u2500 prediction.png\n\u251c\u2500\u2500 difference.png\n\u2514\u2500\u2500 superresolution.png\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/","title":"Fourier Neural Operator (FNO) - Training Tutorial - Heat Equation","text":""},{"location":"api/experimental/examples/sciml/fno/example_heat/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The heat equation:</p> \\[ \\frac{\\partial u}{\\partial t} = D\\frac{\\partial^2 u}{\\partial x^2} \\] <p>where: - \\(u(x,t)\\): temperature field - \\(D\\): thermal diffusivity - \\(x\\): spatial coordinate - \\(t\\): time</p>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#implementation-parameters","title":"Implementation Parameters","text":""},{"location":"api/experimental/examples/sciml/fno/example_heat/#domain-parameters","title":"Domain Parameters","text":"<pre><code>nx = 64  # Spatial points\nnt = 100  # Time steps\nL = 2\u03c0  # Domain length\nT = 1.0  # Final time\nD = 0.1  # Diffusion coefficient\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#dataset-parameters","title":"Dataset Parameters","text":"<pre><code>n_samples = 1200\ntrain_samples = 1000\ntest_samples = 200\nbatch_size = 50\nepochs = 50\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#fno-architecture","title":"FNO Architecture","text":"<pre><code>model = FNO1d(\n    in_channels=2,  # Initial temperature + spatial coordinate\n    out_channels=1,  # Final temperature\n    modes=16,\n    width=64,\n    activation=jax.nn.gelu,\n    n_blocks=4\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#code-components","title":"Code Components","text":""},{"location":"api/experimental/examples/sciml/fno/example_heat/#initial-condition-generator","title":"Initial Condition Generator","text":"<pre><code>def generate_initial_condition(key):\n    \"\"\"Generate random Gaussian pulses\"\"\"\n    max_pulses = 3\n    positions = jax.random.uniform(k1, (max_pulses,)) * L\n    widths = jax.random.uniform(k2, (max_pulses,)) * 0.2 + 0.1\n    amplitudes = jax.random.uniform(k3, (max_pulses,)) * 0.8 + 0.2\n\n    u0 = jnp.zeros(nx)\n    for pos, width, amp in zip(positions, widths, amplitudes):\n        u0 += amp * jnp.exp(-(x - pos)**2 / (2 * width**2))\n    return u0\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#heat-equation-solver","title":"Heat Equation Solver","text":"<pre><code>def solve_heat_equation(u0):\n    \"\"\"Solve using explicit finite differences\"\"\"\n    u = jnp.zeros((nt, nx))\n    u = u.at[0].set(u0)\n\n    for n in range(1, nt):\n        u = u.at[n].set(\n            u[n-1] + D * dt/dx**2 * (\n                jnp.roll(u[n-1], 1) - 2*u[n-1] + jnp.roll(u[n-1], -1)\n            )\n        )\n    return u\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#training-loop","title":"Training Loop","text":"<pre><code>@eqx.filter_jit\ndef make_step(model, opt_state, batch):\n    def loss_fn(model):\n        pred = jax.vmap(model)(batch[0])\n        return jnp.mean((pred - batch[1])**2)\n\n    loss, grads = eqx.filter_value_and_grad(loss_fn)(model)\n    updates, opt_state = optimizer.update(grads, opt_state, model)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#results-analysis","title":"Results Analysis","text":"<p>The implementation generates three visualizations:</p> <ol> <li><code>example_prediction.png</code>:</li> <li>Initial temperature profile</li> <li>True solution at final time</li> <li> <p>FNO prediction</p> </li> <li> <p><code>training_loss.png</code>:</p> </li> <li>MSE loss over training steps</li> <li> <p>Log-scale visualization</p> </li> <li> <p><code>absolute_error.png</code>:</p> </li> <li>Point-wise absolute error</li> <li>Error distribution analysis</li> </ol>"},{"location":"api/experimental/examples/sciml/fno/example_heat/#output-structure","title":"Output Structure","text":"<pre><code>outputs/fno/heat/\n\u251c\u2500\u2500 example_prediction.png\n\u251c\u2500\u2500 training_loss.png\n\u2514\u2500\u2500 absolute_error.png\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/","title":"Fourier Neural Operator (FNO) - Wave Equation Tutorial","text":""},{"location":"api/experimental/examples/sciml/fno/example_wave/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The wave equation:</p> \\[ \\frac{\\partial^2 u}{\\partial t^2} = c^2\\frac{\\partial^2 u}{\\partial x^2} \\] <p>where: - \\(u(x,t)\\): displacement field - \\(c\\): wave speed - \\(x\\): spatial coordinate - \\(t\\): time</p>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#parameters","title":"Parameters","text":""},{"location":"api/experimental/examples/sciml/fno/example_wave/#domain-parameters","title":"Domain Parameters","text":"<pre><code>nx = 64  # Spatial points\nnt = 100  # Time steps\nL = 2\u03c0  # Domain length\nT = 2.0  # Final time\nc = 1.0  # Wave speed\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#architecture-parameters","title":"Architecture Parameters","text":"<pre><code>model = FNO1d(\n    in_channels=3,  # Initial displacement + velocity + coordinate\n    out_channels=1,  # Final displacement\n    modes=16,\n    width=64,\n    activation=jax.nn.gelu,\n    n_blocks=4\n)\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#implementation","title":"Implementation","text":""},{"location":"api/experimental/examples/sciml/fno/example_wave/#initial-conditions-generator","title":"Initial Conditions Generator","text":"<pre><code>def generate_initial_conditions(key):\n    # Initial displacement (Gaussian pulses)\n    max_pulses = 2\n    positions = jax.random.uniform(k1, (max_pulses,)) * L\n    widths = jax.random.uniform(k2, (max_pulses,)) * 0.2 + 0.1\n    amplitudes = jax.random.uniform(k3, (max_pulses,)) * 0.8 + 0.2\n\n    # Initial velocity (smoothed random)\n    v0 = jax.random.normal(k4, (nx,)) * 0.1\n    v0 = jnp.convolve(v0, jnp.ones(10)/10, mode='same')\n\n    return u0, v0\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#wave-equation-solver","title":"Wave Equation Solver","text":"<pre><code>def solve_wave_equation(init_conditions):\n    \"\"\"Central differences scheme\"\"\"\n    u0, v0 = init_conditions\n    u = jnp.zeros((nt, nx))\n    u = u.at[0].set(u0)\n    u = u.at[1].set(u0 + dt * v0)\n\n    for n in range(2, nt):\n        u = u.at[n].set(\n            2 * u[n-1] - u[n-2] +\n            (c * dt/dx)**2 * (\n                jnp.roll(u[n-1], 1) - 2*u[n-1] + jnp.roll(u[n-1], -1)\n            )\n        )\n    return u\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#training-loop","title":"Training Loop","text":"<pre><code>@eqx.filter_jit\ndef make_step(model, opt_state, batch):\n    def loss_fn(model):\n        pred = jax.vmap(model)(batch[0])\n        return jnp.mean((pred - batch[1])**2)\n\n    loss, grads = eqx.filter_value_and_grad(loss_fn)(model)\n    updates, opt_state = optimizer.update(grads, opt_state, model)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#output-visualization","title":"Output Visualization","text":"<pre><code>outputs/fno/wave/\n\u251c\u2500\u2500 example_prediction.png  # Initial conditions, true solution, prediction\n\u251c\u2500\u2500 training_loss.png      # MSE loss evolution\n\u2514\u2500\u2500 absolute_error.png     # Point-wise absolute error\n</code></pre>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#key-metrics","title":"Key Metrics","text":"<ul> <li>MSE Loss tracking</li> <li>Test set error evaluation</li> <li>Point-wise error analysis</li> </ul>"},{"location":"api/experimental/examples/sciml/fno/example_wave/#features","title":"Features","text":"<ul> <li>Handles coupled initial conditions (displacement and velocity)</li> <li>Multi-component input processing</li> <li>Periodic boundary conditions</li> <li>Second-order accuracy in time and space</li> </ul>"},{"location":"api/experimental/ml/supervised/classification/base/","title":"base","text":"<p>Module: base.py</p> <p>This module provides the abstract base class for all classification algorithms in SciREX. It defines shared functionality for:     - Data preparation (loading from CSV and standard scaling)     - Classification performance metric computation (accuracy, precision, recall, f1-score)</p> <p>Classes:</p> Name Description <code>Classification</code> <p>Abstract base class that outlines common behavior for classification algorithms.</p> Dependencies <ul> <li>numpy, pandas, sklearn</li> <li>abc, pathlib, time, typing (for structural and type support)</li> </ul> Key Features <ul> <li>Consistent interface for loading and preparing data</li> <li>Standard approach to computing and returning classification metrics</li> <li>Enforces subclasses to implement <code>fit</code>, <code>predict</code>, and <code>get_model_params</code></li> </ul> Authors <ul> <li>Protyush P. Chowdhury (protyushc@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification","title":"<code>Classification</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for classification algorithms in the SciREX library.</p> This class provides <ul> <li>A consistent interface for loading and preparing data</li> <li>A standard approach to computing and returning classification metrics (accuracy, precision, recall, F1-score)</li> <li>A method for plotting confusion matrix for classification results</li> </ul> Subclasses must <ol> <li>Implement the <code>fit(X: np.ndarray, y: np.ndarray) -&gt; None</code> method, which should populate <code>self.model</code>.</li> <li>Implement the <code>get_model_params() -&gt; Dict[str, Any]</code> method, which returns a dict of model parameters for logging/debugging.</li> </ol> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>str</code> <p>The name or identifier of the classification model (e.g., \"logistic_regression\", \"decision_tree\").</p> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>model</code> <code>Optional</code> <p>The trained classification model.</p> <code>plots_dir</code> <code>Path</code> <p>Directory where confusion matrix plots will be saved.</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>class Classification(ABC):\n    \"\"\"\n    Abstract base class for classification algorithms in the SciREX library.\n\n    This class provides:\n      - A consistent interface for loading and preparing data\n      - A standard approach to computing and returning classification metrics (accuracy, precision, recall, F1-score)\n      - A method for plotting confusion matrix for classification results\n\n    Subclasses must:\n      1. Implement the `fit(X: np.ndarray, y: np.ndarray) -&gt; None` method, which should populate `self.model`.\n      2. Implement the `get_model_params() -&gt; Dict[str, Any]` method, which returns a dict of model parameters for logging/debugging.\n\n    Attributes:\n        model_type (str): The name or identifier of the classification model (e.g., \"logistic_regression\", \"decision_tree\").\n        random_state (int): Random seed for reproducibility.\n        model (Optional): The trained classification model.\n        plots_dir (Path): Directory where confusion matrix plots will be saved.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the base classification class.\n\n        Args:\n            model_type (str): A string identifier for the classification algorithm\n                              (e.g. \"logistic_regression\", \"decision_tree\", etc.).\n            random_state (int, optional): Seed for reproducibility where applicable.\n                                          Defaults to 42.\n        \"\"\"\n        self.model_type = model_type\n        self.random_state = random_state\n\n        # Directory for saving plots\n        self.plots_dir = Path.cwd() / \"plots\"\n        self.plots_dir.mkdir(parents=True, exist_ok=True)\n\n        # Subclasses must set self.model after fitting\n        self.model: Optional[Any] = None\n\n    def prepare_data(self, path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Load and preprocess data from a CSV file, returning features and labels.\n\n        This method:\n          1. Reads the CSV file into a pandas DataFrame.\n          2. Drops rows containing NaN values.\n          3. Selects only numeric columns from the DataFrame.\n          4. Scales the features using scikit-learn's StandardScaler.\n          5. Assumes the last column is the target label.\n\n        Args:\n            path (str): Filepath to the CSV data file.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]:\n                - Features dataset (X) of shape (n_samples, n_features).\n                - Labels (y) of shape (n_samples,).\n        \"\"\"\n        df = pd.read_csv(Path(path))\n        df = df.dropna()\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n        if numeric_columns.empty:\n            raise ValueError(\"No numeric columns found in the data.\")\n        X = df[numeric_columns].values\n        y = df[df.columns[-1]].values  # Assuming last column is the label\n        return StandardScaler().fit_transform(X), y\n\n    @abstractmethod\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the classification model on the training dataset.\n\n        Args:\n            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the features.\n            y (np.ndarray): A 1D array of shape (n_samples,) containing the labels.\n\n        Subclasses must implement this method. After fitting the model,\n        `self.model` should be populated with the trained model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return model parameters for logging or debugging.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing key model parameters and\n                            potentially any learned attributes (e.g., coefficients, intercept).\n        \"\"\"\n        pass\n\n    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n        \"\"\"\n        Plot the confusion matrix using the true and predicted labels.\n\n        Args:\n            y_true (np.ndarray): True labels for the test data.\n            y_pred (np.ndarray): Predicted labels for the test data.\n\n        Returns:\n            Figure: A matplotlib Figure object containing the confusion matrix plot.\n        \"\"\"\n        cm = confusion_matrix(y_true, y_pred)\n        fig, ax = plt.subplots(figsize=(8, 6))\n        cax = ax.matshow(cm, cmap=plt.cm.Blues)\n        fig.colorbar(cax)\n\n        ax.set_xticklabels([\"\"] + [str(i) for i in np.unique(y_true)])\n        ax.set_yticklabels([\"\"] + [str(i) for i in np.unique(y_true)])\n\n        ax.set_xlabel(\"Predicted\", fontsize=12)\n        ax.set_ylabel(\"True\", fontsize=12)\n        ax.set_title(\"Confusion Matrix\", fontsize=14)\n\n        plt.tight_layout()\n\n        plot_path = self.plots_dir / f\"confusion_matrix_{self.model_type}.png\"\n        fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n        plt.close(fig)\n\n        return fig\n\n    def run(\n        self,\n        data: Optional[np.ndarray] = None,\n        path: Optional[str] = None,\n        test_size: float = 0.2,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run the complete classification pipeline: data loading/preprocessing,\n        fitting the model, and computing classification metrics on the test set.\n\n        Args:\n            data (Optional[np.ndarray]): Preprocessed data array of shape (n_samples, n_features).\n            path (Optional[str]): Path to a CSV file from which to read data.\n                                  If `data` is not provided, this must be specified.\n            test_size (float): The proportion of the dataset to include in the test split (default 0.2).\n\n        Returns:\n            Dict[str, Any]: A dictionary with the following keys:\n                - \"params\" (Dict[str, Any]): Model parameters from `self.get_model_params()`\n                - \"accuracy\" (float): Accuracy score of the classification model.\n                - \"precision\" (float): Precision score.\n                - \"recall\" (float): Recall score.\n                - \"f1_score\" (float): F1-score.\n        \"\"\"\n        if data is None and path is None:\n            raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n        # Load/prepare data if needed\n        X, y = data if data is not None else self.prepare_data(path)\n\n        # Split the data into train and test sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=self.random_state\n        )\n\n        # Fit the model on the training data\n        self.fit(X_train, y_train)\n\n        # Check model and make predictions\n        if self.model is None:\n            raise ValueError(\"Model is not trained. Did you implement fit()?\")\n\n        y_pred = self.model.predict(X_test)\n\n        # Calculate classification metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average=\"weighted\")\n        recall = recall_score(y_test, y_pred, average=\"weighted\")\n        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n\n        # Plot confusion matrix\n        self.plot_confusion_matrix(y_test, y_pred)\n\n        # Return results\n        return {\n            \"params\": self.get_model_params(),\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.__init__","title":"<code>__init__(model_type, random_state=42)</code>","text":"<p>Initialize the base classification class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>A string identifier for the classification algorithm               (e.g. \"logistic_regression\", \"decision_tree\", etc.).</p> required <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.                           Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"\n    Initialize the base classification class.\n\n    Args:\n        model_type (str): A string identifier for the classification algorithm\n                          (e.g. \"logistic_regression\", \"decision_tree\", etc.).\n        random_state (int, optional): Seed for reproducibility where applicable.\n                                      Defaults to 42.\n    \"\"\"\n    self.model_type = model_type\n    self.random_state = random_state\n\n    # Directory for saving plots\n    self.plots_dir = Path.cwd() / \"plots\"\n    self.plots_dir.mkdir(parents=True, exist_ok=True)\n\n    # Subclasses must set self.model after fitting\n    self.model: Optional[Any] = None\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.fit","title":"<code>fit(X, y)</code>  <code>abstractmethod</code>","text":"<p>Fit the classification model on the training dataset.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array of shape (n_samples, n_features) containing the features.</p> required <code>y</code> <code>ndarray</code> <p>A 1D array of shape (n_samples,) containing the labels.</p> required <p>Subclasses must implement this method. After fitting the model, <code>self.model</code> should be populated with the trained model.</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>@abstractmethod\ndef fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the classification model on the training dataset.\n\n    Args:\n        X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the features.\n        y (np.ndarray): A 1D array of shape (n_samples,) containing the labels.\n\n    Subclasses must implement this method. After fitting the model,\n    `self.model` should be populated with the trained model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.get_model_params","title":"<code>get_model_params()</code>  <code>abstractmethod</code>","text":"<p>Return model parameters for logging or debugging.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing key model parameters and             potentially any learned attributes (e.g., coefficients, intercept).</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>@abstractmethod\ndef get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return model parameters for logging or debugging.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing key model parameters and\n                        potentially any learned attributes (e.g., coefficients, intercept).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.plot_confusion_matrix","title":"<code>plot_confusion_matrix(y_true, y_pred)</code>","text":"<p>Plot the confusion matrix using the true and predicted labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True labels for the test data.</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted labels for the test data.</p> required <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>A matplotlib Figure object containing the confusion matrix plot.</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n    \"\"\"\n    Plot the confusion matrix using the true and predicted labels.\n\n    Args:\n        y_true (np.ndarray): True labels for the test data.\n        y_pred (np.ndarray): Predicted labels for the test data.\n\n    Returns:\n        Figure: A matplotlib Figure object containing the confusion matrix plot.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n    fig.colorbar(cax)\n\n    ax.set_xticklabels([\"\"] + [str(i) for i in np.unique(y_true)])\n    ax.set_yticklabels([\"\"] + [str(i) for i in np.unique(y_true)])\n\n    ax.set_xlabel(\"Predicted\", fontsize=12)\n    ax.set_ylabel(\"True\", fontsize=12)\n    ax.set_title(\"Confusion Matrix\", fontsize=14)\n\n    plt.tight_layout()\n\n    plot_path = self.plots_dir / f\"confusion_matrix_{self.model_type}.png\"\n    fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    return fig\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.prepare_data","title":"<code>prepare_data(path)</code>","text":"<p>Load and preprocess data from a CSV file, returning features and labels.</p> This method <ol> <li>Reads the CSV file into a pandas DataFrame.</li> <li>Drops rows containing NaN values.</li> <li>Selects only numeric columns from the DataFrame.</li> <li>Scales the features using scikit-learn's StandardScaler.</li> <li>Assumes the last column is the target label.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filepath to the CSV data file.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: - Features dataset (X) of shape (n_samples, n_features). - Labels (y) of shape (n_samples,).</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>def prepare_data(self, path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load and preprocess data from a CSV file, returning features and labels.\n\n    This method:\n      1. Reads the CSV file into a pandas DataFrame.\n      2. Drops rows containing NaN values.\n      3. Selects only numeric columns from the DataFrame.\n      4. Scales the features using scikit-learn's StandardScaler.\n      5. Assumes the last column is the target label.\n\n    Args:\n        path (str): Filepath to the CSV data file.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]:\n            - Features dataset (X) of shape (n_samples, n_features).\n            - Labels (y) of shape (n_samples,).\n    \"\"\"\n    df = pd.read_csv(Path(path))\n    df = df.dropna()\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    if numeric_columns.empty:\n        raise ValueError(\"No numeric columns found in the data.\")\n    X = df[numeric_columns].values\n    y = df[df.columns[-1]].values  # Assuming last column is the label\n    return StandardScaler().fit_transform(X), y\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/base/#scirex.experimental.ml.supervised.classification.base.Classification.run","title":"<code>run(data=None, path=None, test_size=0.2)</code>","text":"<p>Run the complete classification pipeline: data loading/preprocessing, fitting the model, and computing classification metrics on the test set.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[ndarray]</code> <p>Preprocessed data array of shape (n_samples, n_features).</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Path to a CSV file from which to read data.                   If <code>data</code> is not provided, this must be specified.</p> <code>None</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split (default 0.2).</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the following keys: - \"params\" (Dict[str, Any]): Model parameters from <code>self.get_model_params()</code> - \"accuracy\" (float): Accuracy score of the classification model. - \"precision\" (float): Precision score. - \"recall\" (float): Recall score. - \"f1_score\" (float): F1-score.</p> Source code in <code>scirex/experimental/ml/supervised/classification/base.py</code> <pre><code>def run(\n    self,\n    data: Optional[np.ndarray] = None,\n    path: Optional[str] = None,\n    test_size: float = 0.2,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run the complete classification pipeline: data loading/preprocessing,\n    fitting the model, and computing classification metrics on the test set.\n\n    Args:\n        data (Optional[np.ndarray]): Preprocessed data array of shape (n_samples, n_features).\n        path (Optional[str]): Path to a CSV file from which to read data.\n                              If `data` is not provided, this must be specified.\n        test_size (float): The proportion of the dataset to include in the test split (default 0.2).\n\n    Returns:\n        Dict[str, Any]: A dictionary with the following keys:\n            - \"params\" (Dict[str, Any]): Model parameters from `self.get_model_params()`\n            - \"accuracy\" (float): Accuracy score of the classification model.\n            - \"precision\" (float): Precision score.\n            - \"recall\" (float): Recall score.\n            - \"f1_score\" (float): F1-score.\n    \"\"\"\n    if data is None and path is None:\n        raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n    # Load/prepare data if needed\n    X, y = data if data is not None else self.prepare_data(path)\n\n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=self.random_state\n    )\n\n    # Fit the model on the training data\n    self.fit(X_train, y_train)\n\n    # Check model and make predictions\n    if self.model is None:\n        raise ValueError(\"Model is not trained. Did you implement fit()?\")\n\n    y_pred = self.model.predict(X_test)\n\n    # Calculate classification metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average=\"weighted\")\n    recall = recall_score(y_test, y_pred, average=\"weighted\")\n    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n\n    # Plot confusion matrix\n    self.plot_confusion_matrix(y_test, y_pred)\n\n    # Return results\n    return {\n        \"params\": self.get_model_params(),\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/","title":"Decision tree","text":"<p>Decision Tree classification implementation for SciREX.</p> <p>This module provides a Decision Tree implementation using scikit-learn with automatic parameter tuning using grid search. The implementation focuses on both accuracy and interpretability.</p> Mathematical Background <p>Decision Trees recursively partition the feature space using:</p> <ol> <li>Splitting Criteria:</li> <li>Gini Impurity: 1 - \u2211\u1d62p\u1d62\u00b2</li> <li> <p>Entropy: -\u2211\u1d62p\u1d62log(p\u1d62)    where p\u1d62 is the proportion of class i in the node</p> </li> <li> <p>Information Gain:    IG(parent, children) = I(parent) - \u2211(n\u2c7c/n)I(child\u2c7c)    where I is impurity measure (Gini or Entropy)</p> </li> <li> <p>Tree Pruning:    Cost-Complexity: R\u03b1(T) = R(T) + \u03b1|T|    where R(T) is tree error, |T| is tree size, \u03b1 is complexity parameter</p> </li> </ol> Key Features <ul> <li>Automatic parameter optimization</li> <li>Multiple splitting criteria</li> <li>Built-in tree visualization</li> <li>Pruning capabilities</li> <li>Feature importance estimation</li> </ul> References <p>[1] Breiman, L., et al. (1984). Classification and Regression Trees [2] Quinlan, J. R. (1986). Induction of Decision Trees [3] Hastie, T., et al. (2009). Elements of Statistical Learning, Ch. 9</p>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier","title":"<code>DecisionTreeClassifier</code>","text":"<p>               Bases: <code>Classification</code></p> <p>Decision Tree with automatic parameter tuning.</p> <p>This implementation includes automatic selection of optimal parameters using grid search with cross-validation. It balances model complexity with performance through pruning and parameter optimization.</p> <p>Attributes:</p> Name Type Description <code>cv</code> <p>Number of cross-validation folds</p> <code>best_params</code> <code>Optional[Dict[str, Any]]</code> <p>Best parameters found by grid search</p> <code>model</code> <code>Optional[Any]</code> <p>Fitted DecisionTreeClassifier instance</p> Example <p>classifier = DecisionTreeClassifier(cv=5) X_train = np.array([[1, 2], [2, 3], [3, 4]]) y_train = np.array([0, 0, 1]) classifier.fit(X_train, y_train) print(classifier.best_params)</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>class DecisionTreeClassifier(Classification):\n    \"\"\"Decision Tree with automatic parameter tuning.\n\n    This implementation includes automatic selection of optimal parameters\n    using grid search with cross-validation. It balances model complexity\n    with performance through pruning and parameter optimization.\n\n    Attributes:\n        cv: Number of cross-validation folds\n        best_params: Best parameters found by grid search\n        model: Fitted DecisionTreeClassifier instance\n\n    Example:\n        &gt;&gt;&gt; classifier = DecisionTreeClassifier(cv=5)\n        &gt;&gt;&gt; X_train = np.array([[1, 2], [2, 3], [3, 4]])\n        &gt;&gt;&gt; y_train = np.array([0, 0, 1])\n        &gt;&gt;&gt; classifier.fit(X_train, y_train)\n        &gt;&gt;&gt; print(classifier.best_params)\n    \"\"\"\n\n    def __init__(self, cv: int = 5, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize Decision Tree classifier.\n\n        Args:\n            cv: Number of cross-validation folds. Defaults to 5.\n            **kwargs: Additional keyword arguments passed to parent class.\n\n        Notes:\n            The classifier uses GridSearchCV for parameter optimization,\n            searching over different tree depths, splitting criteria,\n            and minimum sample thresholds.\n        \"\"\"\n        super().__init__(\"decision_tree\", **kwargs)\n        self.cv = cv\n        self.best_params: Optional[Dict[str, Any]] = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"Fit Decision Tree model with parameter tuning.\n\n        Performs grid search over tree parameters to find optimal\n        model configuration using cross-validation.\n\n        Args:\n            X: Training feature matrix of shape (n_samples, n_features)\n            y: Training labels of shape (n_samples,)\n\n        Notes:\n            The grid search optimizes over:\n            - Splitting criterion (gini vs entropy)\n            - Maximum tree depth\n            - Minimum samples for splitting\n            - Minimum samples per leaf\n            - Maximum features considered per split\n        \"\"\"\n        param_grid = {\n            \"criterion\": [\"gini\", \"entropy\"],\n            \"max_depth\": [3, 5, 7, 9, None],\n            \"min_samples_split\": [2, 5, 10],\n            \"min_samples_leaf\": [1, 2, 4],\n            \"max_features\": [\"sqrt\", \"log2\", None],\n        }\n\n        base_model = DTC(random_state=self.random_state)\n\n        grid_search = GridSearchCV(\n            base_model, param_grid, cv=self.cv, scoring=\"accuracy\", n_jobs=-1\n        )\n\n        grid_search.fit(X, y)\n\n        self.best_params = grid_search.best_params_\n        self.model = grid_search.best_estimator_\n\n        print(f\"Best parameters found: {self.best_params}\")\n        print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters of the fitted model.\n\n        Returns:\n            Dictionary containing:\n                - model_type: Type of classifier\n                - best_params: Best parameters found by grid search\n                - cv: Number of cross-validation folds used\n        \"\"\"\n        return {\n            \"model_type\": self.model_type,\n            \"best_params\": self.best_params,\n            \"cv\": self.cv,\n        }\n\n    def get_feature_importance(self) -&gt; Dict[str, float]:\n        \"\"\"Get feature importance scores.\n\n        Returns:\n            Dictionary mapping feature indices to importance scores\n\n        Raises:\n            ValueError: If model hasn't been fitted yet\n\n        Notes:\n            Feature importance is computed based on the decrease in\n            impurity (Gini or entropy) brought by each feature across\n            all tree splits.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before getting feature importance\")\n\n        importance_dict = {}\n        for idx, importance in enumerate(self.model.feature_importances_):\n            importance_dict[f\"feature_{idx}\"] = importance\n\n        return importance_dict\n\n    # Add these methods to decision_tree.py\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict class labels for samples in X.\n\n        Args:\n            X: Test samples of shape (n_samples, n_features)\n\n        Returns:\n            Array of predicted class labels\n\n        Raises:\n            ValueError: If model hasn't been fitted yet\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before prediction\")\n        return self.model.predict(X)\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict class probabilities for samples in X.\n\n        Args:\n            X: Test samples of shape (n_samples, n_features)\n\n        Returns:\n            Array of shape (n_samples, n_classes) with class probabilities\n\n        Raises:\n            ValueError: If model hasn't been fitted yet\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before prediction\")\n        return self.model.predict_proba(X)\n\n    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, float]:\n        \"\"\"Evaluate model performance on test data.\n\n        Args:\n            X_test: Test features of shape (n_samples, n_features)\n            y_test: True labels of shape (n_samples,)\n\n        Returns:\n            Dictionary containing evaluation metrics:\n                - accuracy: Overall classification accuracy\n                - precision: Precision score (micro-averaged)\n                - recall: Recall score (micro-averaged)\n                - f1_score: F1 score (micro-averaged)\n        \"\"\"\n        from sklearn.metrics import (\n            accuracy_score,\n            precision_score,\n            recall_score,\n            f1_score,\n        )\n\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before evaluation\")\n\n        y_pred = self.predict(X_test)\n\n        return {\n            \"accuracy\": accuracy_score(y_test, y_pred),\n            \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n            \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n            \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.__init__","title":"<code>__init__(cv=5, **kwargs)</code>","text":"<p>Initialize Decision Tree classifier.</p> <p>Parameters:</p> Name Type Description Default <code>cv</code> <code>int</code> <p>Number of cross-validation folds. Defaults to 5.</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to parent class.</p> <code>{}</code> Notes <p>The classifier uses GridSearchCV for parameter optimization, searching over different tree depths, splitting criteria, and minimum sample thresholds.</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def __init__(self, cv: int = 5, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize Decision Tree classifier.\n\n    Args:\n        cv: Number of cross-validation folds. Defaults to 5.\n        **kwargs: Additional keyword arguments passed to parent class.\n\n    Notes:\n        The classifier uses GridSearchCV for parameter optimization,\n        searching over different tree depths, splitting criteria,\n        and minimum sample thresholds.\n    \"\"\"\n    super().__init__(\"decision_tree\", **kwargs)\n    self.cv = cv\n    self.best_params: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.evaluate","title":"<code>evaluate(X_test, y_test)</code>","text":"<p>Evaluate model performance on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test features of shape (n_samples, n_features)</p> required <code>y_test</code> <code>ndarray</code> <p>True labels of shape (n_samples,)</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing evaluation metrics: - accuracy: Overall classification accuracy - precision: Precision score (micro-averaged) - recall: Recall score (micro-averaged) - f1_score: F1 score (micro-averaged)</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model performance on test data.\n\n    Args:\n        X_test: Test features of shape (n_samples, n_features)\n        y_test: True labels of shape (n_samples,)\n\n    Returns:\n        Dictionary containing evaluation metrics:\n            - accuracy: Overall classification accuracy\n            - precision: Precision score (micro-averaged)\n            - recall: Recall score (micro-averaged)\n            - f1_score: F1 score (micro-averaged)\n    \"\"\"\n    from sklearn.metrics import (\n        accuracy_score,\n        precision_score,\n        recall_score,\n        f1_score,\n    )\n\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before evaluation\")\n\n    y_pred = self.predict(X_test)\n\n    return {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit Decision Tree model with parameter tuning.</p> <p>Performs grid search over tree parameters to find optimal model configuration using cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training feature matrix of shape (n_samples, n_features)</p> required <code>y</code> <code>ndarray</code> <p>Training labels of shape (n_samples,)</p> required Notes <p>The grid search optimizes over: - Splitting criterion (gini vs entropy) - Maximum tree depth - Minimum samples for splitting - Minimum samples per leaf - Maximum features considered per split</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"Fit Decision Tree model with parameter tuning.\n\n    Performs grid search over tree parameters to find optimal\n    model configuration using cross-validation.\n\n    Args:\n        X: Training feature matrix of shape (n_samples, n_features)\n        y: Training labels of shape (n_samples,)\n\n    Notes:\n        The grid search optimizes over:\n        - Splitting criterion (gini vs entropy)\n        - Maximum tree depth\n        - Minimum samples for splitting\n        - Minimum samples per leaf\n        - Maximum features considered per split\n    \"\"\"\n    param_grid = {\n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_depth\": [3, 5, 7, 9, None],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 4],\n        \"max_features\": [\"sqrt\", \"log2\", None],\n    }\n\n    base_model = DTC(random_state=self.random_state)\n\n    grid_search = GridSearchCV(\n        base_model, param_grid, cv=self.cv, scoring=\"accuracy\", n_jobs=-1\n    )\n\n    grid_search.fit(X, y)\n\n    self.best_params = grid_search.best_params_\n    self.model = grid_search.best_estimator_\n\n    print(f\"Best parameters found: {self.best_params}\")\n    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>Get feature importance scores.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary mapping feature indices to importance scores</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted yet</p> Notes <p>Feature importance is computed based on the decrease in impurity (Gini or entropy) brought by each feature across all tree splits.</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def get_feature_importance(self) -&gt; Dict[str, float]:\n    \"\"\"Get feature importance scores.\n\n    Returns:\n        Dictionary mapping feature indices to importance scores\n\n    Raises:\n        ValueError: If model hasn't been fitted yet\n\n    Notes:\n        Feature importance is computed based on the decrease in\n        impurity (Gini or entropy) brought by each feature across\n        all tree splits.\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before getting feature importance\")\n\n    importance_dict = {}\n    for idx, importance in enumerate(self.model.feature_importances_):\n        importance_dict[f\"feature_{idx}\"] = importance\n\n    return importance_dict\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get parameters of the fitted model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - model_type: Type of classifier - best_params: Best parameters found by grid search - cv: Number of cross-validation folds used</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"Get parameters of the fitted model.\n\n    Returns:\n        Dictionary containing:\n            - model_type: Type of classifier\n            - best_params: Best parameters found by grid search\n            - cv: Number of cross-validation folds used\n    \"\"\"\n    return {\n        \"model_type\": self.model_type,\n        \"best_params\": self.best_params,\n        \"cv\": self.cv,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Test samples of shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of predicted class labels</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted yet</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict class labels for samples in X.\n\n    Args:\n        X: Test samples of shape (n_samples, n_features)\n\n    Returns:\n        Array of predicted class labels\n\n    Raises:\n        ValueError: If model hasn't been fitted yet\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before prediction\")\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/decision_tree/#scirex.experimental.ml.supervised.classification.decision_tree.DecisionTreeClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Test samples of shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_samples, n_classes) with class probabilities</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted yet</p> Source code in <code>scirex/experimental/ml/supervised/classification/decision_tree.py</code> <pre><code>def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict class probabilities for samples in X.\n\n    Args:\n        X: Test samples of shape (n_samples, n_features)\n\n    Returns:\n        Array of shape (n_samples, n_classes) with class probabilities\n\n    Raises:\n        ValueError: If model hasn't been fitted yet\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before prediction\")\n    return self.model.predict_proba(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/","title":"Logistic regression","text":"<p>Module: logistic_regression.py</p> <p>This module implements Logistic Regression for binary and multi-class classification tasks.</p> It provides functionality to <ul> <li>Train Logistic Regression classifiers on a dataset</li> <li>Evaluate model performance using classification metrics</li> <li>Visualize results with a confusion matrix</li> <li>Optimize hyperparameters using grid search</li> </ul> <p>Classes:</p> Name Description <code>LogisticRegressionClassifier</code> <p>Implements Logistic Regression using scikit-learn.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.linear_model.LogisticRegression</li> <li>sklearn.metrics (classification metrics)</li> <li>sklearn.model_selection.GridSearchCV</li> <li>matplotlib, seaborn</li> <li>base.py (Classification)</li> </ul> Key Features <ul> <li>Support for binary and multi-class classification</li> <li>Regularization options (L1, L2, ElasticNet)</li> <li>Grid search for hyperparameter tuning</li> <li>Automatic data preparation and evaluation</li> </ul> Authors <ul> <li>Protyush P. Chowdhury (protyushc@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier","title":"<code>LogisticRegressionClassifier</code>","text":"<p>               Bases: <code>Classification</code></p> <p>Implements Logistic Regression for classification tasks.</p> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>class LogisticRegressionClassifier(Classification):\n    \"\"\"\n    Implements Logistic Regression for classification tasks.\n    \"\"\"\n\n    def __init__(self, random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the Logistic Regression classifier.\n\n        Args:\n            random_state (int): Seed for reproducibility.\n        \"\"\"\n        super().__init__(model_type=\"logistic_regression\", random_state=random_state)\n        self.model = LogisticRegression(random_state=random_state, solver=\"lbfgs\")\n\n    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -&gt; None:\n        \"\"\"\n        Train the Logistic Regression model.\n\n        Args:\n            X_train (np.ndarray): Training data features.\n            y_train (np.ndarray): Training data labels.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the model on test data.\n\n        Args:\n            X_test (np.ndarray): Test data features.\n            y_test (np.ndarray): Test data labels.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).\n        \"\"\"\n        y_pred = self.model.predict(X_test)\n        report = classification_report(y_test, y_pred, output_dict=True)\n        return {\n            \"accuracy\": report[\"accuracy\"],\n            \"precision\": report[\"weighted avg\"][\"precision\"],\n            \"recall\": report[\"weighted avg\"][\"recall\"],\n            \"f1_score\": report[\"weighted avg\"][\"f1-score\"],\n        }\n\n    def predict(self, X_test: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict the labels for the test data.\n\n        Args:\n            X_test (np.ndarray): Test data features.\n\n        Returns:\n            np.ndarray: Array of predicted labels.\n        \"\"\"\n        # Use the model to predict the labels for the given test data\n        return self.model.predict(X_test)\n\n    def plot(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; None:\n        \"\"\"\n        Plot the confusion matrix for the test data.\n\n        Args:\n            X_test (np.ndarray): Test data features.\n            y_test (np.ndarray): Test data labels.\n        \"\"\"\n        y_pred = self.model.predict(X_test)\n        cm = confusion_matrix(y_test, y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(\n            cm,\n            annot=True,\n            fmt=\"d\",\n            cmap=\"Blues\",\n            xticklabels=np.unique(y_test),\n            yticklabels=np.unique(y_test),\n        )\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix - Logistic Regression\")\n        plt.show()\n\n    def grid_search(\n        self, X_train: np.ndarray, y_train: np.ndarray, param_grid: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Perform hyperparameter tuning using grid search.\n\n        Args:\n            X_train (np.ndarray): Training data features.\n            y_train (np.ndarray): Training data labels.\n            param_grid (Dict[str, Any]): Dictionary of hyperparameters to search.\n        \"\"\"\n        grid = GridSearchCV(\n            estimator=self.model, param_grid=param_grid, scoring=\"accuracy\", cv=5\n        )\n        grid.fit(X_train, y_train)\n        self.model = grid.best_estimator_\n        print(f\"Best Parameters: {grid.best_params_}\")\n        print(f\"Best Cross-Validated Accuracy: {grid.best_score_}\")\n\n    def run(\n        self,\n        data: np.ndarray,\n        labels: np.ndarray,\n        split_ratio: float = 0.2,\n        param_grid: Dict[str, Any] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute the full classification pipeline with optional grid search.\n\n        Args:\n            data (np.ndarray): Input features.\n            labels (np.ndarray): Input labels.\n            split_ratio (float): Proportion of data to use for testing. Defaults to 0.2.\n            param_grid (Dict[str, Any], optional): Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.\n\n        Returns:\n            Dict[str, Any]: Performance metrics.\n        \"\"\"\n        # Split the data into training and test sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            data, labels, test_size=split_ratio\n        )\n\n        # Perform grid search if param_grid is provided\n        if param_grid:\n            self.grid_search(X_train, y_train, param_grid)\n\n        # Train the model with the (possibly tuned) parameters\n        self.fit(X_train, y_train)\n\n        # Evaluate the model\n        metrics = self.evaluate(X_test, y_test)\n\n        # Plot the confusion matrix\n        self.plot(X_test, y_test)\n\n        return metrics\n\n    # Implementing the abstract method get_model_params\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return the parameters of the model.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing model parameters.\n        \"\"\"\n        return {\n            \"C\": self.model.C,\n            \"max_iter\": self.model.max_iter,\n            \"solver\": self.model.solver,\n            \"penalty\": self.model.penalty,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.__init__","title":"<code>__init__(random_state=42)</code>","text":"<p>Initialize the Logistic Regression classifier.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int</code> <p>Seed for reproducibility.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def __init__(self, random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the Logistic Regression classifier.\n\n    Args:\n        random_state (int): Seed for reproducibility.\n    \"\"\"\n    super().__init__(model_type=\"logistic_regression\", random_state=random_state)\n    self.model = LogisticRegression(random_state=random_state, solver=\"lbfgs\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.evaluate","title":"<code>evaluate(X_test, y_test)</code>","text":"<p>Evaluate the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data features.</p> required <code>y_test</code> <code>ndarray</code> <p>Test data labels.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).</p> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the model on test data.\n\n    Args:\n        X_test (np.ndarray): Test data features.\n        y_test (np.ndarray): Test data labels.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).\n    \"\"\"\n    y_pred = self.model.predict(X_test)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    return {\n        \"accuracy\": report[\"accuracy\"],\n        \"precision\": report[\"weighted avg\"][\"precision\"],\n        \"recall\": report[\"weighted avg\"][\"recall\"],\n        \"f1_score\": report[\"weighted avg\"][\"f1-score\"],\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.fit","title":"<code>fit(X_train, y_train)</code>","text":"<p>Train the Logistic Regression model.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training data features.</p> required <code>y_train</code> <code>ndarray</code> <p>Training data labels.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def fit(self, X_train: np.ndarray, y_train: np.ndarray) -&gt; None:\n    \"\"\"\n    Train the Logistic Regression model.\n\n    Args:\n        X_train (np.ndarray): Training data features.\n        y_train (np.ndarray): Training data labels.\n    \"\"\"\n    self.model.fit(X_train, y_train)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Return the parameters of the model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing model parameters.</p> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return the parameters of the model.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing model parameters.\n    \"\"\"\n    return {\n        \"C\": self.model.C,\n        \"max_iter\": self.model.max_iter,\n        \"solver\": self.model.solver,\n        \"penalty\": self.model.penalty,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.grid_search","title":"<code>grid_search(X_train, y_train, param_grid)</code>","text":"<p>Perform hyperparameter tuning using grid search.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training data features.</p> required <code>y_train</code> <code>ndarray</code> <p>Training data labels.</p> required <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters to search.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def grid_search(\n    self, X_train: np.ndarray, y_train: np.ndarray, param_grid: Dict[str, Any]\n) -&gt; None:\n    \"\"\"\n    Perform hyperparameter tuning using grid search.\n\n    Args:\n        X_train (np.ndarray): Training data features.\n        y_train (np.ndarray): Training data labels.\n        param_grid (Dict[str, Any]): Dictionary of hyperparameters to search.\n    \"\"\"\n    grid = GridSearchCV(\n        estimator=self.model, param_grid=param_grid, scoring=\"accuracy\", cv=5\n    )\n    grid.fit(X_train, y_train)\n    self.model = grid.best_estimator_\n    print(f\"Best Parameters: {grid.best_params_}\")\n    print(f\"Best Cross-Validated Accuracy: {grid.best_score_}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.plot","title":"<code>plot(X_test, y_test)</code>","text":"<p>Plot the confusion matrix for the test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data features.</p> required <code>y_test</code> <code>ndarray</code> <p>Test data labels.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def plot(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; None:\n    \"\"\"\n    Plot the confusion matrix for the test data.\n\n    Args:\n        X_test (np.ndarray): Test data features.\n        y_test (np.ndarray): Test data labels.\n    \"\"\"\n    y_pred = self.model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=np.unique(y_test),\n        yticklabels=np.unique(y_test),\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix - Logistic Regression\")\n    plt.show()\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.predict","title":"<code>predict(X_test)</code>","text":"<p>Predict the labels for the test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data features.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of predicted labels.</p> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def predict(self, X_test: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict the labels for the test data.\n\n    Args:\n        X_test (np.ndarray): Test data features.\n\n    Returns:\n        np.ndarray: Array of predicted labels.\n    \"\"\"\n    # Use the model to predict the labels for the given test data\n    return self.model.predict(X_test)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/logistic_regression/#scirex.experimental.ml.supervised.classification.logistic_regression.LogisticRegressionClassifier.run","title":"<code>run(data, labels, split_ratio=0.2, param_grid=None)</code>","text":"<p>Execute the full classification pipeline with optional grid search.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input features.</p> required <code>labels</code> <code>ndarray</code> <p>Input labels.</p> required <code>split_ratio</code> <code>float</code> <p>Proportion of data to use for testing. Defaults to 0.2.</p> <code>0.2</code> <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Performance metrics.</p> Source code in <code>scirex/experimental/ml/supervised/classification/logistic_regression.py</code> <pre><code>def run(\n    self,\n    data: np.ndarray,\n    labels: np.ndarray,\n    split_ratio: float = 0.2,\n    param_grid: Dict[str, Any] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute the full classification pipeline with optional grid search.\n\n    Args:\n        data (np.ndarray): Input features.\n        labels (np.ndarray): Input labels.\n        split_ratio (float): Proportion of data to use for testing. Defaults to 0.2.\n        param_grid (Dict[str, Any], optional): Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.\n\n    Returns:\n        Dict[str, Any]: Performance metrics.\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        data, labels, test_size=split_ratio\n    )\n\n    # Perform grid search if param_grid is provided\n    if param_grid:\n        self.grid_search(X_train, y_train, param_grid)\n\n    # Train the model with the (possibly tuned) parameters\n    self.fit(X_train, y_train)\n\n    # Evaluate the model\n    metrics = self.evaluate(X_test, y_test)\n\n    # Plot the confusion matrix\n    self.plot(X_test, y_test)\n\n    return metrics\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/","title":"naive bayes","text":"<p>Module: naive_bayes.py</p> <p>This module implements Naive Bayes classification algorithms, including:   - Gaussian Naive Bayes   - Multinomial Naive Bayes   - Bernoulli Naive Bayes</p> It provides functionality to <ul> <li>Train Naive Bayes classifiers on a dataset</li> <li>Evaluate model performance using classification metrics</li> <li>Visualize results with a confusion matrix</li> <li>Optimize hyperparameters using grid search</li> </ul> <p>Classes:</p> Name Description <code>NaiveBayes</code> <p>Implements Naive Bayes classification using scikit-learn.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.naive_bayes.GaussianNB</li> <li>sklearn.naive_bayes.MultinomialNB</li> <li>sklearn.naive_bayes.BernoulliNB</li> <li>sklearn.metrics (classification metrics)</li> <li>base.py (Classification)</li> </ul> Key Features <ul> <li>Support for Gaussian, Multinomial, and Bernoulli Naive Bayes</li> <li>Grid search for hyperparameter tuning</li> <li>Automatic data preparation and evaluation</li> </ul> Authors <ul> <li>Protyush P. Chowdhury (protyushc@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes","title":"<code>NaiveBayes</code>","text":"<p>               Bases: <code>Classification</code></p> <p>Implements Naive Bayes classification for Gaussian, Multinomial, and Bernoulli distributions.</p> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>class NaiveBayes(Classification):\n    \"\"\"\n    Implements Naive Bayes classification for Gaussian, Multinomial, and Bernoulli distributions.\n    \"\"\"\n\n    def __init__(self, model_type: str = \"gaussian\", random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the NaiveBayes classifier.\n\n        Args:\n            model_type (str): Type of Naive Bayes classifier. Options are:\n                              \"gaussian\", \"multinomial\", \"bernoulli\".\n            random_state (int): Seed for reproducibility where applicable.\n        \"\"\"\n        super().__init__(model_type=model_type, random_state=random_state)\n\n        if model_type == \"gaussian\":\n            self.model = GaussianNB()\n        elif model_type == \"multinomial\":\n            self.model = MultinomialNB()\n        elif model_type == \"bernoulli\":\n            self.model = BernoulliNB()\n        else:\n            raise ValueError(\n                \"Invalid model_type. Choose 'gaussian', 'multinomial', or 'bernoulli'.\"\n            )\n\n    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -&gt; None:\n        \"\"\"\n        Train the Naive Bayes model.\n\n        Args:\n            X_train (np.ndarray): Training data features.\n            y_train (np.ndarray): Training data labels.\n        \"\"\"\n        self.model.fit(X_train, y_train)\n\n    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate the model on test data.\n\n        Args:\n            X_test (np.ndarray): Test data features.\n            y_test (np.ndarray): Test data labels.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).\n        \"\"\"\n        y_pred = self.model.predict(X_test)\n        report = classification_report(y_test, y_pred, output_dict=True)\n        return {\n            \"accuracy\": report[\"accuracy\"],\n            \"precision\": report[\"weighted avg\"][\"precision\"],\n            \"recall\": report[\"weighted avg\"][\"recall\"],\n            \"f1_score\": report[\"weighted avg\"][\"f1-score\"],\n        }\n\n    def plot(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; None:\n        \"\"\"\n        Plot the confusion matrix for the test data.\n\n        Args:\n            X_test (np.ndarray): Test data features.\n            y_test (np.ndarray): Test data labels.\n        \"\"\"\n        y_pred = self.model.predict(X_test)\n        cm = confusion_matrix(y_test, y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(\n            cm,\n            annot=True,\n            fmt=\"d\",\n            cmap=\"Blues\",\n            xticklabels=np.unique(y_test),\n            yticklabels=np.unique(y_test),\n        )\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(f\"Confusion Matrix - {self.model_type.capitalize()} Naive Bayes\")\n        plt.show()\n\n    def grid_search(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        param_grid: Dict[str, Any],\n        cv: int = 5,\n    ) -&gt; None:\n        \"\"\"\n        Perform hyperparameter tuning using grid search.\n\n        Args:\n            X_train (np.ndarray): Training data features.\n            y_train (np.ndarray): Training data labels.\n            param_grid (Dict[str, Any]): Dictionary of hyperparameters to search.\n            cv (int): Number of cross-validation folds. Default is 5.\n        \"\"\"\n        grid = GridSearchCV(\n            estimator=self.model, param_grid=param_grid, scoring=\"accuracy\", cv=cv\n        )\n        grid.fit(X_train, y_train)\n        self.model = grid.best_estimator_\n        print(f\"Best Parameters: {grid.best_params_}\")\n        print(f\"Best Cross-Validated Accuracy: {grid.best_score_}\")\n\n    def run(\n        self,\n        data: np.ndarray,\n        labels: np.ndarray,\n        test_size: float = 0.2,\n        param_grid: Dict[str, Any] = None,\n        cv: int = 5,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Execute the full classification pipeline with optional grid search.\n\n        Args:\n            data (np.ndarray): Input features.\n            labels (np.ndarray): Input labels.\n            test_size (float): Proportion of data to use for testing. Defaults to 0.2.\n            param_grid (Dict[str, Any], optional): Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.\n            cv (int): Number of cross-validation folds for grid search. Default is 5.\n\n        Returns:\n            Dict[str, Any]: Performance metrics.\n        \"\"\"\n        # Split the data into training and test sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            data, labels, test_size=test_size, random_state=self.random_state\n        )\n\n        # Perform grid search if param_grid is provided\n        if param_grid:\n            self.grid_search(X_train, y_train, param_grid, cv=cv)\n\n        # Train the model with the (possibly tuned) parameters\n        self.fit(X_train, y_train)\n\n        # Evaluate the model\n        metrics = self.evaluate(X_test, y_test)\n\n        # Plot the confusion matrix\n        self.plot(X_test, y_test)\n\n        return metrics\n\n    # Implementing the abstract method get_model_params\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return the parameters of the model.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing model parameters.\n        \"\"\"\n        return {\n            \"alpha\": self.model.alpha,\n            \"fit_prior\": self.model.fit_prior,\n            \"class_prior\": self.model.class_prior,\n        }\n\n    # Method to save the model to a file\n    def save_model(self, file_path: str) -&gt; None:\n        \"\"\"\n        Save the trained model to a file.\n\n        Args:\n            file_path (str): Path where the model will be saved.\n        \"\"\"\n        joblib.dump(self.model, file_path)\n        print(f\"Model saved to {file_path}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.__init__","title":"<code>__init__(model_type='gaussian', random_state=42)</code>","text":"<p>Initialize the NaiveBayes classifier.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of Naive Bayes classifier. Options are:               \"gaussian\", \"multinomial\", \"bernoulli\".</p> <code>'gaussian'</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def __init__(self, model_type: str = \"gaussian\", random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the NaiveBayes classifier.\n\n    Args:\n        model_type (str): Type of Naive Bayes classifier. Options are:\n                          \"gaussian\", \"multinomial\", \"bernoulli\".\n        random_state (int): Seed for reproducibility where applicable.\n    \"\"\"\n    super().__init__(model_type=model_type, random_state=random_state)\n\n    if model_type == \"gaussian\":\n        self.model = GaussianNB()\n    elif model_type == \"multinomial\":\n        self.model = MultinomialNB()\n    elif model_type == \"bernoulli\":\n        self.model = BernoulliNB()\n    else:\n        raise ValueError(\n            \"Invalid model_type. Choose 'gaussian', 'multinomial', or 'bernoulli'.\"\n        )\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.evaluate","title":"<code>evaluate(X_test, y_test)</code>","text":"<p>Evaluate the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data features.</p> required <code>y_test</code> <code>ndarray</code> <p>Test data labels.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).</p> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate the model on test data.\n\n    Args:\n        X_test (np.ndarray): Test data features.\n        y_test (np.ndarray): Test data labels.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing evaluation metrics (accuracy, precision, recall, F1-score).\n    \"\"\"\n    y_pred = self.model.predict(X_test)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    return {\n        \"accuracy\": report[\"accuracy\"],\n        \"precision\": report[\"weighted avg\"][\"precision\"],\n        \"recall\": report[\"weighted avg\"][\"recall\"],\n        \"f1_score\": report[\"weighted avg\"][\"f1-score\"],\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.fit","title":"<code>fit(X_train, y_train)</code>","text":"<p>Train the Naive Bayes model.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training data features.</p> required <code>y_train</code> <code>ndarray</code> <p>Training data labels.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def fit(self, X_train: np.ndarray, y_train: np.ndarray) -&gt; None:\n    \"\"\"\n    Train the Naive Bayes model.\n\n    Args:\n        X_train (np.ndarray): Training data features.\n        y_train (np.ndarray): Training data labels.\n    \"\"\"\n    self.model.fit(X_train, y_train)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Return the parameters of the model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing model parameters.</p> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return the parameters of the model.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing model parameters.\n    \"\"\"\n    return {\n        \"alpha\": self.model.alpha,\n        \"fit_prior\": self.model.fit_prior,\n        \"class_prior\": self.model.class_prior,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.grid_search","title":"<code>grid_search(X_train, y_train, param_grid, cv=5)</code>","text":"<p>Perform hyperparameter tuning using grid search.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training data features.</p> required <code>y_train</code> <code>ndarray</code> <p>Training data labels.</p> required <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters to search.</p> required <code>cv</code> <code>int</code> <p>Number of cross-validation folds. Default is 5.</p> <code>5</code> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def grid_search(\n    self,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    param_grid: Dict[str, Any],\n    cv: int = 5,\n) -&gt; None:\n    \"\"\"\n    Perform hyperparameter tuning using grid search.\n\n    Args:\n        X_train (np.ndarray): Training data features.\n        y_train (np.ndarray): Training data labels.\n        param_grid (Dict[str, Any]): Dictionary of hyperparameters to search.\n        cv (int): Number of cross-validation folds. Default is 5.\n    \"\"\"\n    grid = GridSearchCV(\n        estimator=self.model, param_grid=param_grid, scoring=\"accuracy\", cv=cv\n    )\n    grid.fit(X_train, y_train)\n    self.model = grid.best_estimator_\n    print(f\"Best Parameters: {grid.best_params_}\")\n    print(f\"Best Cross-Validated Accuracy: {grid.best_score_}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.plot","title":"<code>plot(X_test, y_test)</code>","text":"<p>Plot the confusion matrix for the test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data features.</p> required <code>y_test</code> <code>ndarray</code> <p>Test data labels.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def plot(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; None:\n    \"\"\"\n    Plot the confusion matrix for the test data.\n\n    Args:\n        X_test (np.ndarray): Test data features.\n        y_test (np.ndarray): Test data labels.\n    \"\"\"\n    y_pred = self.model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=np.unique(y_test),\n        yticklabels=np.unique(y_test),\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(f\"Confusion Matrix - {self.model_type.capitalize()} Naive Bayes\")\n    plt.show()\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.run","title":"<code>run(data, labels, test_size=0.2, param_grid=None, cv=5)</code>","text":"<p>Execute the full classification pipeline with optional grid search.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input features.</p> required <code>labels</code> <code>ndarray</code> <p>Input labels.</p> required <code>test_size</code> <code>float</code> <p>Proportion of data to use for testing. Defaults to 0.2.</p> <code>0.2</code> <code>param_grid</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.</p> <code>None</code> <code>cv</code> <code>int</code> <p>Number of cross-validation folds for grid search. Default is 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Performance metrics.</p> Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def run(\n    self,\n    data: np.ndarray,\n    labels: np.ndarray,\n    test_size: float = 0.2,\n    param_grid: Dict[str, Any] = None,\n    cv: int = 5,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute the full classification pipeline with optional grid search.\n\n    Args:\n        data (np.ndarray): Input features.\n        labels (np.ndarray): Input labels.\n        test_size (float): Proportion of data to use for testing. Defaults to 0.2.\n        param_grid (Dict[str, Any], optional): Dictionary of hyperparameters to search for grid search. If None, grid search will not be performed.\n        cv (int): Number of cross-validation folds for grid search. Default is 5.\n\n    Returns:\n        Dict[str, Any]: Performance metrics.\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        data, labels, test_size=test_size, random_state=self.random_state\n    )\n\n    # Perform grid search if param_grid is provided\n    if param_grid:\n        self.grid_search(X_train, y_train, param_grid, cv=cv)\n\n    # Train the model with the (possibly tuned) parameters\n    self.fit(X_train, y_train)\n\n    # Evaluate the model\n    metrics = self.evaluate(X_test, y_test)\n\n    # Plot the confusion matrix\n    self.plot(X_test, y_test)\n\n    return metrics\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/naive_bayes/#scirex.experimental.ml.supervised.classification.naive_bayes.NaiveBayes.save_model","title":"<code>save_model(file_path)</code>","text":"<p>Save the trained model to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path where the model will be saved.</p> required Source code in <code>scirex/experimental/ml/supervised/classification/naive_bayes.py</code> <pre><code>def save_model(self, file_path: str) -&gt; None:\n    \"\"\"\n    Save the trained model to a file.\n\n    Args:\n        file_path (str): Path where the model will be saved.\n    \"\"\"\n    joblib.dump(self.model, file_path)\n    print(f\"Model saved to {file_path}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/","title":"SVM","text":"<p>Support Vector Machine (SVM) classification implementation for SciREX.</p> <p>This module provides a comprehensive SVM implementation using scikit-learn, supporting multiple kernel types with automatic parameter tuning.</p> Mathematical Background <p>SVM solves the optimization problem: min_{w,b} 1/2||w||\u00b2 + C\u2211max(0, 1 - y\u1d62(w\u00b7x\u1d62 + b))</p> <p>Kernel functions supported: 1. Linear: K(x,y) = x\u00b7y 2. RBF: K(x,y) = exp(-\u03b3||x-y||\u00b2) 3. Polynomial: K(x,y) = (\u03b3x\u00b7y + r)^d 4. Sigmoid: K(x,y) = tanh(\u03b3x\u00b7y + r)</p> <p>The dual formulation solves: max_\u03b1 \u2211\u03b1\u1d62 - 1/2\u2211\u2211\u03b1\u1d62\u03b1\u2c7cy\u1d62y\u2c7cK(x\u1d62,x\u2c7c) subject to: 0 \u2264 \u03b1\u1d62 \u2264 C, \u2211\u03b1\u1d62y\u1d62 = 0</p> Key Features <ul> <li>Multiple kernel functions</li> <li>Automatic parameter optimization</li> <li>Probability estimation support</li> <li>Efficient optimization for large datasets</li> </ul> References <p>[1] Vapnik, V. (1998). Statistical Learning Theory [2] Scholkopf, B., &amp; Smola, A. J. (2002). Learning with Kernels [3] Platt, J. (1999). Probabilistic Outputs for SVMs</p>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier","title":"<code>SVMClassifier</code>","text":"<p>               Bases: <code>Classification</code></p> <p>SVM classifier with automatic parameter tuning.</p> <p>This implementation supports different kernel types and includes automatic parameter optimization using grid search with cross-validation. Each kernel is optimized for its specific characteristics and use cases.</p> <p>Attributes:</p> Name Type Description <code>kernel</code> <p>Type of kernel function</p> <code>cv</code> <p>Number of cross-validation folds</p> <code>best_params</code> <code>Optional[Dict[str, Any]]</code> <p>Best parameters found by grid search</p> Example <p>classifier = SVMClassifier(kernel=\"rbf\", cv=5) X_train = np.array([[1, 2], [2, 3], [3, 4]]) y_train = np.array([0, 0, 1]) classifier.fit(X_train, y_train) print(classifier.best_params)</p> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>class SVMClassifier(Classification):\n    \"\"\"SVM classifier with automatic parameter tuning.\n\n    This implementation supports different kernel types and includes\n    automatic parameter optimization using grid search with cross-validation.\n    Each kernel is optimized for its specific characteristics and use cases.\n\n    Attributes:\n        kernel: Type of kernel function\n        cv: Number of cross-validation folds\n        best_params: Best parameters found by grid search\n\n    Example:\n        &gt;&gt;&gt; classifier = SVMClassifier(kernel=\"rbf\", cv=5)\n        &gt;&gt;&gt; X_train = np.array([[1, 2], [2, 3], [3, 4]])\n        &gt;&gt;&gt; y_train = np.array([0, 0, 1])\n        &gt;&gt;&gt; classifier.fit(X_train, y_train)\n        &gt;&gt;&gt; print(classifier.best_params)\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel: Literal[\"linear\", \"rbf\", \"poly\", \"sigmoid\"] = \"rbf\",\n        cv: int = 5,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize SVM classifier.\n\n        Args:\n            kernel: Kernel function type. Options:\n                   \"linear\": Linear kernel for linearly separable data\n                   \"rbf\": Radial basis function for non-linear patterns\n                   \"poly\": Polynomial kernel for non-linear patterns\n                   \"sigmoid\": Sigmoid kernel for neural network-like behavior\n            cv: Number of cross-validation folds. Defaults to 5.\n            **kwargs: Additional keyword arguments passed to parent class.\n        \"\"\"\n        super().__init__(\"svm\", **kwargs)\n        self._validate_kernel(kernel)\n        self.kernel = kernel\n        self.cv = cv\n        self.best_params: Optional[Dict[str, Any]] = None\n\n    def _get_param_grid(self):\n        \"\"\"Get parameter grid for grid search based on kernel type.\n\n        Returns:\n            Dictionary of parameters to search for the chosen kernel.\n\n        Notes:\n            Parameter grids are optimized for each kernel:\n            - Linear: Only C (regularization)\n            - RBF: C and gamma (kernel coefficient)\n            - Polynomial: C, gamma, degree, and coef0\n            - Sigmoid: C, gamma, and coef0\n        \"\"\"\n        param_grid = {\n            \"C\": [0.1, 1, 10, 100],\n        }\n\n        if self.kernel == \"linear\":\n            return param_grid\n\n        elif self.kernel == \"rbf\":\n            param_grid.update(\n                {\n                    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],\n                }\n            )\n\n        elif self.kernel == \"poly\":\n            param_grid.update(\n                {\n                    \"degree\": [2, 3, 4],\n                    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],\n                    \"coef0\": [0, 1],\n                }\n            )\n\n        elif self.kernel == \"sigmoid\":\n            param_grid.update(\n                {\n                    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],\n                    \"coef0\": [0, 1],\n                }\n            )\n\n        return param_grid\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"Fit SVM model with parameter tuning.\n\n        Performs grid search to find optimal parameters for the chosen\n        kernel type.\n\n        Args:\n            X: Training feature matrix of shape (n_samples, n_features)\n            y: Training labels of shape (n_samples,)\n\n        Notes:\n            - Uses probability estimation for better prediction granularity\n            - Employs parallel processing for faster grid search\n            - May take longer for larger datasets due to quadratic complexity\n        \"\"\"\n        base_model = SVC(\n            kernel=self.kernel, random_state=self.random_state, probability=True\n        )\n\n        param_grid = self._get_param_grid()\n\n        grid_search = GridSearchCV(\n            base_model, param_grid, cv=self.cv, scoring=\"accuracy\", n_jobs=-1\n        )\n\n        print(f\"Training SVM with {self.kernel} kernel...\")\n        print(\"This may take a while for larger datasets.\")\n\n        grid_search.fit(X, y)\n\n        self.best_params = grid_search.best_params_\n        self.model = grid_search.best_estimator_\n\n        print(f\"Best parameters found: {self.best_params}\")\n        print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters of the fitted model.\n\n        Returns:\n            Dictionary containing:\n                - model_type: Type of classifier\n                - kernel: Kernel function used\n                - best_params: Best parameters found by grid search\n                - cv: Number of cross-validation folds used\n        \"\"\"\n        return {\n            \"model_type\": self.model_type,\n            \"kernel\": self.kernel,\n            \"best_params\": self.best_params,\n            \"cv\": self.cv,\n        }\n\n    # Add these methods to svm.py\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict class labels for samples in X.\n\n        Args:\n            X: Test samples of shape (n_samples, n_features)\n\n        Returns:\n            Array of predicted class labels\n\n        Raises:\n            ValueError: If model hasn't been fitted yet\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before prediction\")\n        return self.model.predict(X)\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict class probabilities for samples in X.\n\n        Args:\n            X: Test samples of shape (n_samples, n_features)\n\n        Returns:\n            Array of shape (n_samples, n_classes) with class probabilities\n\n        Raises:\n            ValueError: If model hasn't been fitted yet\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before prediction\")\n        return self.model.predict_proba(X)\n\n    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, float]:\n        \"\"\"Evaluate model performance on test data.\n\n        Args:\n            X_test: Test features of shape (n_samples, n_features)\n            y_test: True labels of shape (n_samples,)\n\n        Returns:\n            Dictionary containing evaluation metrics:\n                - accuracy: Overall classification accuracy\n                - precision: Precision score (micro-averaged)\n                - recall: Recall score (micro-averaged)\n                - f1_score: F1 score (micro-averaged)\n        \"\"\"\n        from sklearn.metrics import (\n            accuracy_score,\n            precision_score,\n            recall_score,\n            f1_score,\n        )\n\n        if self.model is None:\n            raise ValueError(\"Model must be fitted before evaluation\")\n\n        y_pred = self.predict(X_test)\n\n        return {\n            \"accuracy\": accuracy_score(y_test, y_pred),\n            \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n            \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n            \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n        }\n\n    def _validate_kernel(self, kernel: str) -&gt; None:\n        \"\"\"Validate the kernel type.\n\n        Args:\n            kernel: Kernel type to validate\n\n        Raises:\n            ValueError: If kernel is not one of the supported types\n        \"\"\"\n        valid_kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]\n        if kernel not in valid_kernels:\n            raise ValueError(\n                f\"Invalid kernel type '{kernel}'. \"\n                f\"Must be one of: {', '.join(valid_kernels)}\"\n            )\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.__init__","title":"<code>__init__(kernel='rbf', cv=5, **kwargs)</code>","text":"<p>Initialize SVM classifier.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Literal['linear', 'rbf', 'poly', 'sigmoid']</code> <p>Kernel function type. Options:    \"linear\": Linear kernel for linearly separable data    \"rbf\": Radial basis function for non-linear patterns    \"poly\": Polynomial kernel for non-linear patterns    \"sigmoid\": Sigmoid kernel for neural network-like behavior</p> <code>'rbf'</code> <code>cv</code> <code>int</code> <p>Number of cross-validation folds. Defaults to 5.</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to parent class.</p> <code>{}</code> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def __init__(\n    self,\n    kernel: Literal[\"linear\", \"rbf\", \"poly\", \"sigmoid\"] = \"rbf\",\n    cv: int = 5,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize SVM classifier.\n\n    Args:\n        kernel: Kernel function type. Options:\n               \"linear\": Linear kernel for linearly separable data\n               \"rbf\": Radial basis function for non-linear patterns\n               \"poly\": Polynomial kernel for non-linear patterns\n               \"sigmoid\": Sigmoid kernel for neural network-like behavior\n        cv: Number of cross-validation folds. Defaults to 5.\n        **kwargs: Additional keyword arguments passed to parent class.\n    \"\"\"\n    super().__init__(\"svm\", **kwargs)\n    self._validate_kernel(kernel)\n    self.kernel = kernel\n    self.cv = cv\n    self.best_params: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.evaluate","title":"<code>evaluate(X_test, y_test)</code>","text":"<p>Evaluate model performance on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test features of shape (n_samples, n_features)</p> required <code>y_test</code> <code>ndarray</code> <p>True labels of shape (n_samples,)</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing evaluation metrics: - accuracy: Overall classification accuracy - precision: Precision score (micro-averaged) - recall: Recall score (micro-averaged) - f1_score: F1 score (micro-averaged)</p> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model performance on test data.\n\n    Args:\n        X_test: Test features of shape (n_samples, n_features)\n        y_test: True labels of shape (n_samples,)\n\n    Returns:\n        Dictionary containing evaluation metrics:\n            - accuracy: Overall classification accuracy\n            - precision: Precision score (micro-averaged)\n            - recall: Recall score (micro-averaged)\n            - f1_score: F1 score (micro-averaged)\n    \"\"\"\n    from sklearn.metrics import (\n        accuracy_score,\n        precision_score,\n        recall_score,\n        f1_score,\n    )\n\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before evaluation\")\n\n    y_pred = self.predict(X_test)\n\n    return {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit SVM model with parameter tuning.</p> <p>Performs grid search to find optimal parameters for the chosen kernel type.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training feature matrix of shape (n_samples, n_features)</p> required <code>y</code> <code>ndarray</code> <p>Training labels of shape (n_samples,)</p> required Notes <ul> <li>Uses probability estimation for better prediction granularity</li> <li>Employs parallel processing for faster grid search</li> <li>May take longer for larger datasets due to quadratic complexity</li> </ul> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"Fit SVM model with parameter tuning.\n\n    Performs grid search to find optimal parameters for the chosen\n    kernel type.\n\n    Args:\n        X: Training feature matrix of shape (n_samples, n_features)\n        y: Training labels of shape (n_samples,)\n\n    Notes:\n        - Uses probability estimation for better prediction granularity\n        - Employs parallel processing for faster grid search\n        - May take longer for larger datasets due to quadratic complexity\n    \"\"\"\n    base_model = SVC(\n        kernel=self.kernel, random_state=self.random_state, probability=True\n    )\n\n    param_grid = self._get_param_grid()\n\n    grid_search = GridSearchCV(\n        base_model, param_grid, cv=self.cv, scoring=\"accuracy\", n_jobs=-1\n    )\n\n    print(f\"Training SVM with {self.kernel} kernel...\")\n    print(\"This may take a while for larger datasets.\")\n\n    grid_search.fit(X, y)\n\n    self.best_params = grid_search.best_params_\n    self.model = grid_search.best_estimator_\n\n    print(f\"Best parameters found: {self.best_params}\")\n    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get parameters of the fitted model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - model_type: Type of classifier - kernel: Kernel function used - best_params: Best parameters found by grid search - cv: Number of cross-validation folds used</p> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"Get parameters of the fitted model.\n\n    Returns:\n        Dictionary containing:\n            - model_type: Type of classifier\n            - kernel: Kernel function used\n            - best_params: Best parameters found by grid search\n            - cv: Number of cross-validation folds used\n    \"\"\"\n    return {\n        \"model_type\": self.model_type,\n        \"kernel\": self.kernel,\n        \"best_params\": self.best_params,\n        \"cv\": self.cv,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Test samples of shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of predicted class labels</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted yet</p> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict class labels for samples in X.\n\n    Args:\n        X: Test samples of shape (n_samples, n_features)\n\n    Returns:\n        Array of predicted class labels\n\n    Raises:\n        ValueError: If model hasn't been fitted yet\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before prediction\")\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/classification/svm/#scirex.experimental.ml.supervised.classification.svm.SVMClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Test samples of shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_samples, n_classes) with class probabilities</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted yet</p> Source code in <code>scirex/experimental/ml/supervised/classification/svm.py</code> <pre><code>def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict class probabilities for samples in X.\n\n    Args:\n        X: Test samples of shape (n_samples, n_features)\n\n    Returns:\n        Array of shape (n_samples, n_classes) with class probabilities\n\n    Raises:\n        ValueError: If model hasn't been fitted yet\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model must be fitted before prediction\")\n    return self.model.predict_proba(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/","title":"base","text":"<p>Module: base.py</p> <p>This module provides the abstract base class for all regression algorithms in SciREX. It defines shared functionality for:     - Data preparation (loading from CSV and standard scaling)     - Regression performance metric computation (MSE, MAE, R2 score)</p> <p>Classes:</p> Name Description <code>Regression</code> <p>Abstract base class that outlines common behavior for regression algorithms.</p> Dependencies <ul> <li>numpy, pandas, sklearn</li> <li>abc, pathlib, time, typing (for structural and type support)</li> </ul> <p>Key Features:.     - Consistent interface for loading and preparing data     - Standard approach to computing and returning regression metrics     - Enforces subclasses to implement <code>fit</code>, <code>predict</code>, and <code>get_model_params</code></p> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression","title":"<code>Regression</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for regression algorithms in the SciREX library.</p> This class provides <ul> <li>A consistent interface for loading and preparing data</li> <li>A standard approach to computing and returning regression metrics (MSE, MAE, R2)</li> <li>A method for plotting regression results</li> </ul> Subclasses must <ol> <li>Implement the <code>fit(X: np.ndarray, y: np.ndarray) -&gt; None</code> method, which should populate <code>self.model</code>.</li> <li>Implement the <code>get_model_params() -&gt; Dict[str, Any]</code> method, which returns a dict of model parameters for logging/debugging.</li> </ol> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>str</code> <p>The name or identifier of the regression model (e.g., \"linear_regression\", \"random_forest\").</p> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>model</code> <code>Optional</code> <p>The trained regression model.</p> <code>plots_dir</code> <code>Path</code> <p>Directory where regression plots will be saved.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>class Regression(ABC):\n    \"\"\"\n    Abstract base class for regression algorithms in the SciREX library.\n\n    This class provides:\n      - A consistent interface for loading and preparing data\n      - A standard approach to computing and returning regression metrics (MSE, MAE, R2)\n      - A method for plotting regression results\n\n    Subclasses must:\n      1. Implement the `fit(X: np.ndarray, y: np.ndarray) -&gt; None` method, which should populate `self.model`.\n      2. Implement the `get_model_params() -&gt; Dict[str, Any]` method, which returns a dict of model parameters for logging/debugging.\n\n    Attributes:\n        model_type (str): The name or identifier of the regression model (e.g., \"linear_regression\", \"random_forest\").\n        random_state (int): Random seed for reproducibility.\n        model (Optional): The trained regression model.\n        plots_dir (Path): Directory where regression plots will be saved.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the base regression class.\n\n        Args:\n            model_type (str): A string identifier for the regression algorithm\n                              (e.g. \"linear_regression\", \"random_forest\", etc.).\n            random_state (int, optional): Seed for reproducibility where applicable.\n                                          Defaults to 42.\n        \"\"\"\n        self.model_type = model_type\n        self.random_state = random_state\n\n        # initialize the model to None\n        self.model = None\n\n        # create a directory to save the plots\n        self.plots_dir = Path.cwd() / \"Plots\"\n        self.plots_dir.mkdir(parents=True, exist_ok=True)\n\n    def prepare_data(self, path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Load and preprocess data from a CSV file.\n\n        1.This method reads the dataset from the specified path, drops any rows with missing values,\n        2.Scales the features using StandardScaler.\n\n\n        Args:\n           path (str): The path to the dataset.\n\n        Returns:\n           Tuple[np.ndarray, np.ndarray]: A tuple of prepared features (X) and target values (y).\n        \"\"\"\n        df = pd.read_csv(Path(path))\n        df.dropna(inplace=True)\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n        if numeric_columns.empty:\n            raise ValueError(\"No numeric columns found in the dataset.\")\n\n        # Split into features (X) and target (y)\n        X = df[numeric_columns[:-1]].values  # All numeric columns except the last one\n        y = df[numeric_columns[-1]].values  # Last numeric column as the target\n\n        # Scale the features\n        X = StandardScaler().fit_transform(X)\n        return X, y\n\n    @abstractmethod\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the regression model to the training data.\n\n        Args:\n            X (np.ndarray): The input features for training the model.\n            y (np.ndarray): The target values for training the model.\n\n        Subclasses must implement this method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Generate predictions using the trained regression model.\n\n        Args:\n            X (np.ndarray): The input features for generating predictions.\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the model parameters as a dictionary.\n\n        Returns:\n            Dict[str, Any]: A dictionary of model parameters.\n        \"\"\"\n        pass\n\n    def evaluation_metrics(\n        self, y_true: np.ndarray, y_pred: np.ndarray\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute and return regression evaluation metrics.\n\n        Args:\n            y_true (np.ndarray): The true target values.\n            y_pred (np.ndarray): The predicted target values.\n\n        Returns:\n            Dict[str, float]: A dictionary of regression evaluation metrics.\n        \"\"\"\n        mse = mean_squared_error(y_true, y_pred)\n        mae = mean_absolute_error(y_true, y_pred)\n        r2 = r2_score(y_true, y_pred)\n\n        return {\n            \"mse\": mse,\n            \"mae\": mae,\n            \"r2\": r2,\n        }\n\n    def plot_regression_results(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n        \"\"\"\n        Plot the regression results.\n\n        Args:\n          y_true (np.ndarray): The true target values.\n          y_pred (np.ndarray): The predicted target values.\n\n        Returns:\n        Figure: The matplotlib figure object\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(8, 6))\n        ax.scatter(y_true, y_pred, color=\"blue\", label=\"Predictions\")\n        ax.plot(\n            [y_true.min(), y_true.max()],\n            [y_true.min(), y_true.max()],\n            ls=\"--\",\n            color=\"red\",\n            label=\"Perfect Predictions\",\n        )\n        ax.set_xlabel(\"True Values\")\n        ax.set_ylabel(\"Predicted Values\")\n        ax.set_title(f\"{self.model_type} Regression Results\")\n        ax.legend()\n        plt.tight_layout()\n\n        # Save the plot\n        plot_path = self.plots_dir / f\"regression_results_{self.model_type}.png\"\n        fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n        plt.close(fig)\n\n        return fig\n\n    def run(\n        self,\n        data: Optional[np.array] = None,\n        path: Optional[str] = None,\n        test_size: float = 0.2,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run the complete regression pipeline: data loading/preprocessing, fitting the model, and computing regression metrics on the test set.\n\n        Args:\n        data (optional,[np.array]): preprocessed adta array of shape (n_samples, n_features).\n        path(Optional[str]): The path to the dataset.\n        test_size(float): The proportion of the dataset for test set is to set as default 0.2.\n\n        Returns:\n        Dict[str,Any]: A dictionary with the following keys:\n            - \"params\" (Dict[str, Any]): Model parameters from 'self.get_model_params()\n            - \"MSE\" (float): Mean Squared Error of the regression model.\n            -  \"MAE\" (float): Mean Absolute Error of the regression model.\n            - \"R2\" (float): R =-Squared Score of the regression model.\n        \"\"\"\n        if data is None and path is None:\n            raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n        # Load and prepare the data\n        # If data is not provided, load the data from the specified path\n        # If data is provided, use it directly\n        X, y = data if data is not None else self.prepare_data(path)\n\n        # spliting the dataset into training set and testing set\n        x_train, x_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=self.random_state\n        )\n\n        # Fit the model on the training data\n        self.fit(x_train, y_train)\n\n        # check model and make predictions\n        if self.model is None:\n            raise ValueError(\n                f\"{self.model_type} model is not trained. Ensure the 'fit' method is called before predictions.\"\n            )\n        y_pred = self.predict(x_test)\n\n        # compute regression metrics\n        metrics = self.evaluation_metrics(y_test, y_pred)\n\n        # plot the regression results\n        fig = self.plot_regression_results(y_test, y_pred)\n\n        # return the model parameters and evaluation metrics\n        return {\n            \"params\": self.get_model_params(),\n            \"MSE\": metrics[\"mse\"],\n            \"MAE\": metrics[\"mae\"],\n            \"R2\": metrics[\"r2\"],\n            \"plot\": fig,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.__init__","title":"<code>__init__(model_type, random_state=42)</code>","text":"<p>Initialize the base regression class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>A string identifier for the regression algorithm               (e.g. \"linear_regression\", \"random_forest\", etc.).</p> required <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.                           Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"\n    Initialize the base regression class.\n\n    Args:\n        model_type (str): A string identifier for the regression algorithm\n                          (e.g. \"linear_regression\", \"random_forest\", etc.).\n        random_state (int, optional): Seed for reproducibility where applicable.\n                                      Defaults to 42.\n    \"\"\"\n    self.model_type = model_type\n    self.random_state = random_state\n\n    # initialize the model to None\n    self.model = None\n\n    # create a directory to save the plots\n    self.plots_dir = Path.cwd() / \"Plots\"\n    self.plots_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.evaluation_metrics","title":"<code>evaluation_metrics(y_true, y_pred)</code>","text":"<p>Compute and return regression evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The true target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of regression evaluation metrics.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>def evaluation_metrics(\n    self, y_true: np.ndarray, y_pred: np.ndarray\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute and return regression evaluation metrics.\n\n    Args:\n        y_true (np.ndarray): The true target values.\n        y_pred (np.ndarray): The predicted target values.\n\n    Returns:\n        Dict[str, float]: A dictionary of regression evaluation metrics.\n    \"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n\n    return {\n        \"mse\": mse,\n        \"mae\": mae,\n        \"r2\": r2,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.fit","title":"<code>fit(X, y)</code>  <code>abstractmethod</code>","text":"<p>Fit the regression model to the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input features for training the model.</p> required <code>y</code> <code>ndarray</code> <p>The target values for training the model.</p> required <p>Subclasses must implement this method.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>@abstractmethod\ndef fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the regression model to the training data.\n\n    Args:\n        X (np.ndarray): The input features for training the model.\n        y (np.ndarray): The target values for training the model.\n\n    Subclasses must implement this method.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.get_model_params","title":"<code>get_model_params()</code>  <code>abstractmethod</code>","text":"<p>Get the model parameters as a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of model parameters.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>@abstractmethod\ndef get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the model parameters as a dictionary.\n\n    Returns:\n        Dict[str, Any]: A dictionary of model parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.plot_regression_results","title":"<code>plot_regression_results(y_true, y_pred)</code>","text":"<p>Plot the regression results.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The true target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values.</p> required <p>Returns: Figure: The matplotlib figure object</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>def plot_regression_results(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; Figure:\n    \"\"\"\n    Plot the regression results.\n\n    Args:\n      y_true (np.ndarray): The true target values.\n      y_pred (np.ndarray): The predicted target values.\n\n    Returns:\n    Figure: The matplotlib figure object\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.scatter(y_true, y_pred, color=\"blue\", label=\"Predictions\")\n    ax.plot(\n        [y_true.min(), y_true.max()],\n        [y_true.min(), y_true.max()],\n        ls=\"--\",\n        color=\"red\",\n        label=\"Perfect Predictions\",\n    )\n    ax.set_xlabel(\"True Values\")\n    ax.set_ylabel(\"Predicted Values\")\n    ax.set_title(f\"{self.model_type} Regression Results\")\n    ax.legend()\n    plt.tight_layout()\n\n    # Save the plot\n    plot_path = self.plots_dir / f\"regression_results_{self.model_type}.png\"\n    fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    return fig\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.predict","title":"<code>predict(X)</code>  <code>abstractmethod</code>","text":"<p>Generate predictions using the trained regression model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input features for generating predictions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted target values.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>@abstractmethod\ndef predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate predictions using the trained regression model.\n\n    Args:\n        X (np.ndarray): The input features for generating predictions.\n\n    Returns:\n        np.ndarray: The predicted target values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.prepare_data","title":"<code>prepare_data(path)</code>","text":"<p>Load and preprocess data from a CSV file.</p> <p>1.This method reads the dataset from the specified path, drops any rows with missing values, 2.Scales the features using StandardScaler.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: A tuple of prepared features (X) and target values (y).</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>def prepare_data(self, path: str) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load and preprocess data from a CSV file.\n\n    1.This method reads the dataset from the specified path, drops any rows with missing values,\n    2.Scales the features using StandardScaler.\n\n\n    Args:\n       path (str): The path to the dataset.\n\n    Returns:\n       Tuple[np.ndarray, np.ndarray]: A tuple of prepared features (X) and target values (y).\n    \"\"\"\n    df = pd.read_csv(Path(path))\n    df.dropna(inplace=True)\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    if numeric_columns.empty:\n        raise ValueError(\"No numeric columns found in the dataset.\")\n\n    # Split into features (X) and target (y)\n    X = df[numeric_columns[:-1]].values  # All numeric columns except the last one\n    y = df[numeric_columns[-1]].values  # Last numeric column as the target\n\n    # Scale the features\n    X = StandardScaler().fit_transform(X)\n    return X, y\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/base/#scirex.experimental.ml.supervised.regression.base.Regression.run","title":"<code>run(data=None, path=None, test_size=0.2)</code>","text":"<p>Run the complete regression pipeline: data loading/preprocessing, fitting the model, and computing regression metrics on the test set.</p> <p>Args: data (optional,[np.array]): preprocessed adta array of shape (n_samples, n_features). path(Optional[str]): The path to the dataset. test_size(float): The proportion of the dataset for test set is to set as default 0.2.</p> <p>Dict[str,Any]: A dictionary with the following keys:     - \"params\" (Dict[str, Any]): Model parameters from 'self.get_model_params()     - \"MSE\" (float): Mean Squared Error of the regression model.     -  \"MAE\" (float): Mean Absolute Error of the regression model.     - \"R2\" (float): R =-Squared Score of the regression model.</p> Source code in <code>scirex/experimental/ml/supervised/regression/base.py</code> <pre><code>def run(\n    self,\n    data: Optional[np.array] = None,\n    path: Optional[str] = None,\n    test_size: float = 0.2,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run the complete regression pipeline: data loading/preprocessing, fitting the model, and computing regression metrics on the test set.\n\n    Args:\n    data (optional,[np.array]): preprocessed adta array of shape (n_samples, n_features).\n    path(Optional[str]): The path to the dataset.\n    test_size(float): The proportion of the dataset for test set is to set as default 0.2.\n\n    Returns:\n    Dict[str,Any]: A dictionary with the following keys:\n        - \"params\" (Dict[str, Any]): Model parameters from 'self.get_model_params()\n        - \"MSE\" (float): Mean Squared Error of the regression model.\n        -  \"MAE\" (float): Mean Absolute Error of the regression model.\n        - \"R2\" (float): R =-Squared Score of the regression model.\n    \"\"\"\n    if data is None and path is None:\n        raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n    # Load and prepare the data\n    # If data is not provided, load the data from the specified path\n    # If data is provided, use it directly\n    X, y = data if data is not None else self.prepare_data(path)\n\n    # spliting the dataset into training set and testing set\n    x_train, x_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=self.random_state\n    )\n\n    # Fit the model on the training data\n    self.fit(x_train, y_train)\n\n    # check model and make predictions\n    if self.model is None:\n        raise ValueError(\n            f\"{self.model_type} model is not trained. Ensure the 'fit' method is called before predictions.\"\n        )\n    y_pred = self.predict(x_test)\n\n    # compute regression metrics\n    metrics = self.evaluation_metrics(y_test, y_pred)\n\n    # plot the regression results\n    fig = self.plot_regression_results(y_test, y_pred)\n\n    # return the model parameters and evaluation metrics\n    return {\n        \"params\": self.get_model_params(),\n        \"MSE\": metrics[\"mse\"],\n        \"MAE\": metrics[\"mae\"],\n        \"R2\": metrics[\"r2\"],\n        \"plot\": fig,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/","title":"Lasso regression","text":"<p>Module: lasso_regression.py</p> <p>This module implements the LassoRegressionModel class for Lasso regression tasks, extending the base Regression class from the SciREX library.</p> <p>The implementation uses scikit-learn's Lasso regression model for:     - Fitting the Lasso regression algorithm     - Generating predictions for input features     - Evaluating performance using standard regression metrics</p> <p>Classes:</p> Name Description <code>LassoRegressionModel</code> <p>Implements a Lasso Regression model.</p> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.base</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/#scirex.experimental.ml.supervised.regression.lasso_regression.LassoRegressionModel","title":"<code>LassoRegressionModel</code>","text":"<p>               Bases: <code>Regression</code></p> <p>Class: LassoRegressionModel</p> <p>This class implements a Lasso Regression model for regression tasks. The model is based on scikit-learn's Lasso regression implementation.</p> The class provides methods for <ul> <li>Training the Lasso regression model</li> <li>Generating predictions for input features</li> <li>Evaluating performance using standard regression metrics</li> </ul> <p>Attributes:</p> Name Type Description <code>-</code> <code>model</code> <p>Lasso regression model</p> Source code in <code>scirex/experimental/ml/supervised/regression/lasso_regression.py</code> <pre><code>class LassoRegressionModel(Regression):\n    \"\"\"\n    Class: LassoRegressionModel\n\n    This class implements a Lasso Regression model for regression tasks.\n    The model is based on scikit-learn's Lasso regression implementation.\n\n    The class provides methods for:\n        - Training the Lasso regression model\n        - Generating predictions for input features\n        - Evaluating performance using standard regression metrics\n\n    Attributes:\n        - model: Lasso regression model\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the LassoRegressionModel class.\n\n        Args:\n            alpha (float, optional): Regularization strength. Defaults to 1.0.\n            random_state (int, optional): Seed for reproducibility. Defaults to 42.\n        \"\"\"\n        super().__init__(model_type=\"lasso_regression\", random_state=random_state)\n        self.model = Lasso(alpha=alpha, random_state=random_state)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the Lasso regression model to the data.\n\n        Args:\n            X (np.ndarray): Input features for training (n_samples, n_features).\n            y (np.ndarray): Target values for training (n_samples).\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Generate predictions using the trained Lasso regression model.\n\n        Args:\n            X (np.ndarray): Input features for prediction (n_samples, n_features).\n\n        Returns:\n            np.ndarray: Predicted target values (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the model parameters coefficients and intercept.\n\n        Returns:\n            Dict[str, Any]: Model parameters, including coefficients and intercept.\n        \"\"\"\n        return {\n            \"coefficients\": self.model.coef_,\n            \"intercept\": self.model.intercept_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/#scirex.experimental.ml.supervised.regression.lasso_regression.LassoRegressionModel.__init__","title":"<code>__init__(alpha=1.0, random_state=42)</code>","text":"<p>Initialize the LassoRegressionModel class.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Regularization strength. Defaults to 1.0.</p> <code>1.0</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/lasso_regression.py</code> <pre><code>def __init__(self, alpha: float = 1.0, random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the LassoRegressionModel class.\n\n    Args:\n        alpha (float, optional): Regularization strength. Defaults to 1.0.\n        random_state (int, optional): Seed for reproducibility. Defaults to 42.\n    \"\"\"\n    super().__init__(model_type=\"lasso_regression\", random_state=random_state)\n    self.model = Lasso(alpha=alpha, random_state=random_state)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/#scirex.experimental.ml.supervised.regression.lasso_regression.LassoRegressionModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the Lasso regression model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features for training (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values for training (n_samples).</p> required Source code in <code>scirex/experimental/ml/supervised/regression/lasso_regression.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the Lasso regression model to the data.\n\n    Args:\n        X (np.ndarray): Input features for training (n_samples, n_features).\n        y (np.ndarray): Target values for training (n_samples).\n    \"\"\"\n    self.model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/#scirex.experimental.ml.supervised.regression.lasso_regression.LassoRegressionModel.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the model parameters coefficients and intercept.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Model parameters, including coefficients and intercept.</p> Source code in <code>scirex/experimental/ml/supervised/regression/lasso_regression.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the model parameters coefficients and intercept.\n\n    Returns:\n        Dict[str, Any]: Model parameters, including coefficients and intercept.\n    \"\"\"\n    return {\n        \"coefficients\": self.model.coef_,\n        \"intercept\": self.model.intercept_,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/lasso_regression/#scirex.experimental.ml.supervised.regression.lasso_regression.LassoRegressionModel.predict","title":"<code>predict(X)</code>","text":"<p>Generate predictions using the trained Lasso regression model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features for prediction (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted target values (n_samples,).</p> Source code in <code>scirex/experimental/ml/supervised/regression/lasso_regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Generate predictions using the trained Lasso regression model.\n\n    Args:\n        X (np.ndarray): Input features for prediction (n_samples, n_features).\n\n    Returns:\n        np.ndarray: Predicted target values (n_samples,).\n    \"\"\"\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/","title":"Linear regression","text":"<p>Module: linear_regression.py</p> <p>This module implements the LinearRegressionModel class for regression tasks, extending the base Regression class from the SciREX library.</p> <p>The implementation uses scikit-learn's LinearRegression model for:     - Training the linear regression algorithm     - Generating predictions for input features     - Evaluating performance using standard regression metrics</p> Key Features <ul> <li>Support for fitting a linear regression model</li> <li>Access to model parameters (coefficients, intercept)</li> <li>Seamless integration with the SciREX regression pipeline</li> </ul> <p>Classes:</p> Name Description <code>LinearRegressionModel</code> <p>Implements a Linear Regression model.</p> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.base</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/#scirex.experimental.ml.supervised.regression.linear_regression.LinearRegressionModel","title":"<code>LinearRegressionModel</code>","text":"<p>               Bases: <code>Regression</code></p> <p>Linear Regression model implementation using scikit-learn.</p> Source code in <code>scirex/experimental/ml/supervised/regression/linear_regression.py</code> <pre><code>class LinearRegressionModel(Regression):\n    \"\"\"\n    Linear Regression model implementation using scikit-learn.\n    \"\"\"\n\n    def __init__(self, random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the LinearRegressionModel class.\n\n        Args:\n            random_state (int, optional): Seed for reproducibility where applicable.\n                                          Defaults to 42.\n        \"\"\"\n        super().__init__(model_type=\"linear_regression\", random_state=random_state)\n        self.model = LinearRegression()\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the linear regression model to the data.\n\n        Args:\n            X (np.ndarray): The input training features (n_samples, n_features).\n            y (np.ndarray): The target training values (n_samples).\n\n        Returns:\n            None\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict the target values for the input data.\n\n        Args:\n            X (np.ndarray): The input data (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the linear regression model.\n\n        Returns:\n            Dict[str, Any]: The parameters of the linear regression model.\n        \"\"\"\n        return {\n            \"coefficients\": self.model.coef_,\n            \"intercept\": self.model.intercept_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/#scirex.experimental.ml.supervised.regression.linear_regression.LinearRegressionModel.__init__","title":"<code>__init__(random_state=42)</code>","text":"<p>Initialize the LinearRegressionModel class.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.                           Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/linear_regression.py</code> <pre><code>def __init__(self, random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the LinearRegressionModel class.\n\n    Args:\n        random_state (int, optional): Seed for reproducibility where applicable.\n                                      Defaults to 42.\n    \"\"\"\n    super().__init__(model_type=\"linear_regression\", random_state=random_state)\n    self.model = LinearRegression()\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/#scirex.experimental.ml.supervised.regression.linear_regression.LinearRegressionModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the linear regression model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input training features (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>The target training values (n_samples).</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scirex/experimental/ml/supervised/regression/linear_regression.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the linear regression model to the data.\n\n    Args:\n        X (np.ndarray): The input training features (n_samples, n_features).\n        y (np.ndarray): The target training values (n_samples).\n\n    Returns:\n        None\n    \"\"\"\n    self.model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/#scirex.experimental.ml.supervised.regression.linear_regression.LinearRegressionModel.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the parameters of the linear regression model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The parameters of the linear regression model.</p> Source code in <code>scirex/experimental/ml/supervised/regression/linear_regression.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the linear regression model.\n\n    Returns:\n        Dict[str, Any]: The parameters of the linear regression model.\n    \"\"\"\n    return {\n        \"coefficients\": self.model.coef_,\n        \"intercept\": self.model.intercept_,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/linear_regression/#scirex.experimental.ml.supervised.regression.linear_regression.LinearRegressionModel.predict","title":"<code>predict(X)</code>","text":"<p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted target values.</p> Source code in <code>scirex/experimental/ml/supervised/regression/linear_regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict the target values for the input data.\n\n    Args:\n        X (np.ndarray): The input data (n_samples, n_features).\n\n    Returns:\n        np.ndarray: The predicted target values.\n    \"\"\"\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/","title":"Polynomial regression","text":"<p>Module: polynomial_regression.py</p> <p>This module implements the PolynomialRegressionModel class for regression tasks, extending the base Regression class from the SciREX library.</p> <p>The implementation uses scikit-learn's PolynomialFeatures and LinearRegression model for:     - Polynomial feature generation     - Training the polynomial regression algorithm     - Generating predictions for input features     - Evaluating performance using standard regression metrics</p> Key Features <ul> <li>Polynomial feature transformation for non-linear regression</li> <li>Seamless integration with the SciREX regression pipeline</li> <li>Access to model parameters (coefficients, intercept, degree)</li> </ul> <p>Classes:</p> Name Description <code>PolynomialRegressionModel</code> <p>Implements Polynomial Regression.</p> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.base</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>01/Feb/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/#scirex.experimental.ml.supervised.regression.polynomial_regression.PolynomialRegressionModel","title":"<code>PolynomialRegressionModel</code>","text":"<p>               Bases: <code>Regression</code></p> <p>Polynomial Regression model implementation using scikit-learn's PolynomialFeatures and LinearRegression.</p> <p>This model is designed to capture non-linear relationships in data by transforming the features into polynomial features and applying linear regression on those transformed features.</p> Source code in <code>scirex/experimental/ml/supervised/regression/polynomial_regression.py</code> <pre><code>class PolynomialRegressionModel(Regression):\n    \"\"\"\n    Polynomial Regression model implementation using scikit-learn's PolynomialFeatures\n    and LinearRegression.\n\n    This model is designed to capture non-linear relationships in data by transforming\n    the features into polynomial features and applying linear regression on those transformed features.\n    \"\"\"\n\n    def __init__(self, degree: int = 2, random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the PolynomialRegressionModel class.\n\n        Args:\n            degree (int, optional): The degree of the polynomial features. Default is 2.\n            random_state (int, optional): Seed for reproducibility where applicable. Default is 42.\n        \"\"\"\n        super().__init__(model_type=\"polynomial_regression\", random_state=random_state)\n        self.degree = degree\n        self.poly_features = PolynomialFeatures(degree=self.degree)\n        self.model = LinearRegression()\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the polynomial regression model to the data.\n\n        This method transforms the input features into polynomial features, then fits a\n        linear regression model to the transformed features.\n\n        Args:\n            X (np.ndarray): The input training features (n_samples, n_features).\n            y (np.ndarray): The target training values (n_samples).\n\n        Returns:\n            None\n        \"\"\"\n        X_poly = self.poly_features.fit_transform(X)\n        self.model.fit(X_poly, y)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict the target values for the input data.\n\n        This method transforms the input features into polynomial features before making predictions.\n\n        Args:\n            X (np.ndarray): The input data (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        X_poly = self.poly_features.transform(X)\n        return self.model.predict(X_poly)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the polynomial regression model.\n\n        Returns:\n            Dict[str, Any]: The parameters of the polynomial regression model, including:\n                             - Coefficients\n                             - Intercept\n                             - Degree of the polynomial.\n        \"\"\"\n        return {\n            \"coefficients\": self.model.coef_,\n            \"intercept\": self.model.intercept_,\n            \"degree\": self.degree,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/#scirex.experimental.ml.supervised.regression.polynomial_regression.PolynomialRegressionModel.__init__","title":"<code>__init__(degree=2, random_state=42)</code>","text":"<p>Initialize the PolynomialRegressionModel class.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The degree of the polynomial features. Default is 2.</p> <code>2</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable. Default is 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/polynomial_regression.py</code> <pre><code>def __init__(self, degree: int = 2, random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the PolynomialRegressionModel class.\n\n    Args:\n        degree (int, optional): The degree of the polynomial features. Default is 2.\n        random_state (int, optional): Seed for reproducibility where applicable. Default is 42.\n    \"\"\"\n    super().__init__(model_type=\"polynomial_regression\", random_state=random_state)\n    self.degree = degree\n    self.poly_features = PolynomialFeatures(degree=self.degree)\n    self.model = LinearRegression()\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/#scirex.experimental.ml.supervised.regression.polynomial_regression.PolynomialRegressionModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the polynomial regression model to the data.</p> <p>This method transforms the input features into polynomial features, then fits a linear regression model to the transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input training features (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>The target training values (n_samples).</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scirex/experimental/ml/supervised/regression/polynomial_regression.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the polynomial regression model to the data.\n\n    This method transforms the input features into polynomial features, then fits a\n    linear regression model to the transformed features.\n\n    Args:\n        X (np.ndarray): The input training features (n_samples, n_features).\n        y (np.ndarray): The target training values (n_samples).\n\n    Returns:\n        None\n    \"\"\"\n    X_poly = self.poly_features.fit_transform(X)\n    self.model.fit(X_poly, y)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/#scirex.experimental.ml.supervised.regression.polynomial_regression.PolynomialRegressionModel.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the parameters of the polynomial regression model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The parameters of the polynomial regression model, including:              - Coefficients              - Intercept              - Degree of the polynomial.</p> Source code in <code>scirex/experimental/ml/supervised/regression/polynomial_regression.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the polynomial regression model.\n\n    Returns:\n        Dict[str, Any]: The parameters of the polynomial regression model, including:\n                         - Coefficients\n                         - Intercept\n                         - Degree of the polynomial.\n    \"\"\"\n    return {\n        \"coefficients\": self.model.coef_,\n        \"intercept\": self.model.intercept_,\n        \"degree\": self.degree,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/polynomial_regression/#scirex.experimental.ml.supervised.regression.polynomial_regression.PolynomialRegressionModel.predict","title":"<code>predict(X)</code>","text":"<p>Predict the target values for the input data.</p> <p>This method transforms the input features into polynomial features before making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted target values.</p> Source code in <code>scirex/experimental/ml/supervised/regression/polynomial_regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict the target values for the input data.\n\n    This method transforms the input features into polynomial features before making predictions.\n\n    Args:\n        X (np.ndarray): The input data (n_samples, n_features).\n\n    Returns:\n        np.ndarray: The predicted target values.\n    \"\"\"\n    X_poly = self.poly_features.transform(X)\n    return self.model.predict(X_poly)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/","title":"Ridge regression","text":"<p>Module: ridge_regression.py</p> <p>This module implements the RidgeRegressionModel class for ridge regression tasks, extending the base Regression class from the SciREX library.</p> <p>The implementation uses scikit-learn's Ridge regression model for:     - Fitting the ridge regression algorithm     - Generating predictions for input features     - Evaluating performance using standard regression metrics</p> Key Features <ul> <li>Support for fitting a ridge regression model</li> <li>Access to model parameters (coefficients, intercept)</li> <li>Seamless integration with the SciREX regression pipeline</li> </ul> <p>Classes:</p> Name Description <code>RidgeRegressionModel</code> <p>Implements a Ridge Regression model.</p> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.base</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>16/Jan/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/#scirex.experimental.ml.supervised.regression.ridge_regression.RidgeRegressionModel","title":"<code>RidgeRegressionModel</code>","text":"<p>               Bases: <code>Regression</code></p> <p>Ridge Regression model implementation using scikit-learn.</p> <p>This model performs Ridge Regression, which is a linear model that uses L2 regularization. Ridge regression is useful when there is multicollinearity among input features or when there are more predictors than observations.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Ridge</code> <p>A scikit-learn Ridge model.</p> Source code in <code>scirex/experimental/ml/supervised/regression/ridge_regression.py</code> <pre><code>class RidgeRegressionModel(Regression):\n    \"\"\"\n    Ridge Regression model implementation using scikit-learn.\n\n    This model performs Ridge Regression, which is a linear model that uses L2 regularization.\n    Ridge regression is useful when there is multicollinearity among input features or when\n    there are more predictors than observations.\n\n    Attributes:\n        model (Ridge): A scikit-learn Ridge model.\n    \"\"\"\n\n    def __init__(self, alpha: float = 1.0, random_state: int = 42) -&gt; None:\n        \"\"\"\n        Initialize the RidgeRegressionModel class.\n\n        Args:\n            alpha (float, optional): Regularization strength; must be a positive float.\n                                      Defaults to 1.0.\n            random_state (int, optional): Seed for reproducibility where applicable.\n                                          Defaults to 42.\n        \"\"\"\n        super().__init__(model_type=\"ridge_regression\", random_state=random_state)\n        self.model = Ridge(alpha=alpha, random_state=random_state)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the Ridge regression model to the input data.\n\n        Args:\n            X (np.ndarray): The input training features (n_samples, n_features).\n            y (np.ndarray): The target training values (n_samples).\n\n        Returns:\n            None\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict the target values for the input data.\n\n        Args:\n            X (np.ndarray): The input features (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The predicted target values (n_samples,).\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the Ridge regression model.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the model's parameters:\n                - \"coefficients\": The coefficients (weights) of the linear model.\n                - \"intercept\": The intercept term of the linear model.\n        \"\"\"\n        return {\n            \"coefficients\": self.model.coef_,\n            \"intercept\": self.model.intercept_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/#scirex.experimental.ml.supervised.regression.ridge_regression.RidgeRegressionModel.__init__","title":"<code>__init__(alpha=1.0, random_state=42)</code>","text":"<p>Initialize the RidgeRegressionModel class.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Regularization strength; must be a positive float.                       Defaults to 1.0.</p> <code>1.0</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.                           Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/ridge_regression.py</code> <pre><code>def __init__(self, alpha: float = 1.0, random_state: int = 42) -&gt; None:\n    \"\"\"\n    Initialize the RidgeRegressionModel class.\n\n    Args:\n        alpha (float, optional): Regularization strength; must be a positive float.\n                                  Defaults to 1.0.\n        random_state (int, optional): Seed for reproducibility where applicable.\n                                      Defaults to 42.\n    \"\"\"\n    super().__init__(model_type=\"ridge_regression\", random_state=random_state)\n    self.model = Ridge(alpha=alpha, random_state=random_state)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/#scirex.experimental.ml.supervised.regression.ridge_regression.RidgeRegressionModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the Ridge regression model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input training features (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>The target training values (n_samples).</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scirex/experimental/ml/supervised/regression/ridge_regression.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the Ridge regression model to the input data.\n\n    Args:\n        X (np.ndarray): The input training features (n_samples, n_features).\n        y (np.ndarray): The target training values (n_samples).\n\n    Returns:\n        None\n    \"\"\"\n    self.model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/#scirex.experimental.ml.supervised.regression.ridge_regression.RidgeRegressionModel.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the parameters of the Ridge regression model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the model's parameters: - \"coefficients\": The coefficients (weights) of the linear model. - \"intercept\": The intercept term of the linear model.</p> Source code in <code>scirex/experimental/ml/supervised/regression/ridge_regression.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the Ridge regression model.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the model's parameters:\n            - \"coefficients\": The coefficients (weights) of the linear model.\n            - \"intercept\": The intercept term of the linear model.\n    \"\"\"\n    return {\n        \"coefficients\": self.model.coef_,\n        \"intercept\": self.model.intercept_,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/ridge_regression/#scirex.experimental.ml.supervised.regression.ridge_regression.RidgeRegressionModel.predict","title":"<code>predict(X)</code>","text":"<p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input features (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted target values (n_samples,).</p> Source code in <code>scirex/experimental/ml/supervised/regression/ridge_regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict the target values for the input data.\n\n    Args:\n        X (np.ndarray): The input features (n_samples, n_features).\n\n    Returns:\n        np.ndarray: The predicted target values (n_samples,).\n    \"\"\"\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/svr/","title":"support vector regression","text":"<p>Module: svr.py</p> <p>This module implements the SVRModel class for regression tasks, extending the base Regression class from the SciREX library.</p> <p>The implementation uses scikit-learn's SVR (Support Vector Regression) model for:     - Training the SVR algorithm     - Generating predictions for input features     - Evaluating performance using standard regression metrics</p> Key Features <ul> <li>Support for fitting an SVR model</li> <li>Access to model parameters (kernel, C, epsilon)</li> <li>Seamless integration with the SciREX regression pipeline</li> </ul> <p>Classes:</p> Name Description <code>SVRModel</code> <p>Implements a Support Vector Regression model.</p> Dependencies <ul> <li>numpy</li> <li>scikit-learn</li> <li>scirex.experimental.ml.supervised.regression.base</li> </ul> Authors <ul> <li>Paranidharan (paranidharan@iisc.ac.in)</li> </ul> Version Info <ul> <li>01/Feb/2025: Initial version</li> </ul>"},{"location":"api/experimental/ml/supervised/regression/svr/#scirex.experimental.ml.supervised.regression.svr.SVRModel","title":"<code>SVRModel</code>","text":"<p>               Bases: <code>Regression</code></p> <p>Support Vector Regression (SVR) model implementation using scikit-learn.</p> Source code in <code>scirex/experimental/ml/supervised/regression/svr.py</code> <pre><code>class SVRModel(Regression):\n    \"\"\"\n    Support Vector Regression (SVR) model implementation using scikit-learn.\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel: str = \"rbf\",\n        C: float = 1.0,\n        epsilon: float = 0.1,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the SVRModel class.\n\n        Args:\n            kernel (str, optional): The kernel type used in the model (default is 'rbf').\n            C (float, optional): Regularization parameter (default is 1.0).\n            epsilon (float, optional): Epsilon parameter for the margin (default is 0.1).\n            random_state (int, optional): Seed for reproducibility (default is 42).\n        \"\"\"\n        super().__init__(model_type=\"svr\", random_state=random_state)\n        self.model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the SVR model to the data.\n\n        Args:\n            X (np.ndarray): The input training features (n_samples, n_features).\n            y (np.ndarray): The target training values (n_samples).\n\n        Returns:\n            None\n        \"\"\"\n        self.model.fit(X, y)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict the target values for the input data.\n\n        Args:\n            X (np.ndarray): The input data (n_samples, n_features).\n\n        Returns:\n            np.ndarray: The predicted target values.\n        \"\"\"\n        return self.model.predict(X)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the parameters of the SVR model.\n\n        Returns:\n            Dict[str, Any]: The parameters of the SVR model.\n        \"\"\"\n        return {\n            \"kernel\": self.model.kernel,\n            \"C\": self.model.C,\n            \"epsilon\": self.model.epsilon,\n        }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/svr/#scirex.experimental.ml.supervised.regression.svr.SVRModel.__init__","title":"<code>__init__(kernel='rbf', C=1.0, epsilon=0.1, random_state=42)</code>","text":"<p>Initialize the SVRModel class.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>str</code> <p>The kernel type used in the model (default is 'rbf').</p> <code>'rbf'</code> <code>C</code> <code>float</code> <p>Regularization parameter (default is 1.0).</p> <code>1.0</code> <code>epsilon</code> <code>float</code> <p>Epsilon parameter for the margin (default is 0.1).</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility (default is 42).</p> <code>42</code> Source code in <code>scirex/experimental/ml/supervised/regression/svr.py</code> <pre><code>def __init__(\n    self,\n    kernel: str = \"rbf\",\n    C: float = 1.0,\n    epsilon: float = 0.1,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"\n    Initialize the SVRModel class.\n\n    Args:\n        kernel (str, optional): The kernel type used in the model (default is 'rbf').\n        C (float, optional): Regularization parameter (default is 1.0).\n        epsilon (float, optional): Epsilon parameter for the margin (default is 0.1).\n        random_state (int, optional): Seed for reproducibility (default is 42).\n    \"\"\"\n    super().__init__(model_type=\"svr\", random_state=random_state)\n    self.model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/svr/#scirex.experimental.ml.supervised.regression.svr.SVRModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the SVR model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input training features (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>The target training values (n_samples).</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scirex/experimental/ml/supervised/regression/svr.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the SVR model to the data.\n\n    Args:\n        X (np.ndarray): The input training features (n_samples, n_features).\n        y (np.ndarray): The target training values (n_samples).\n\n    Returns:\n        None\n    \"\"\"\n    self.model.fit(X, y)\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/svr/#scirex.experimental.ml.supervised.regression.svr.SVRModel.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the parameters of the SVR model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The parameters of the SVR model.</p> Source code in <code>scirex/experimental/ml/supervised/regression/svr.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the parameters of the SVR model.\n\n    Returns:\n        Dict[str, Any]: The parameters of the SVR model.\n    \"\"\"\n    return {\n        \"kernel\": self.model.kernel,\n        \"C\": self.model.C,\n        \"epsilon\": self.model.epsilon,\n    }\n</code></pre>"},{"location":"api/experimental/ml/supervised/regression/svr/#scirex.experimental.ml.supervised.regression.svr.SVRModel.predict","title":"<code>predict(X)</code>","text":"<p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted target values.</p> Source code in <code>scirex/experimental/ml/supervised/regression/svr.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict the target values for the input data.\n\n    Args:\n        X (np.ndarray): The input data (n_samples, n_features).\n\n    Returns:\n        np.ndarray: The predicted target values.\n    \"\"\"\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/agglomerative/","title":"Agglomerative","text":"<p>Module: agglomerative.py</p> <p>This module provides an Agglomerative Clustering implementation using scikit-learn. It optionally allows a user-defined cluster count or uses silhouette scores (2..max_k) to auto-select the optimal cluster count.</p> <p>Classes:</p> Name Description <code>Agglomerative</code> <p>Implements an agglomerative clustering approach with optional            user-specified n_clusters or silhouette-based auto selection.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.cluster.AgglomerativeClustering</li> <li>sklearn.metrics.silhouette_score</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>Automatic scanning of possible cluster counts (2..max_k) if n_clusters is not provided</li> <li>Silhouette-based selection of the best cluster count</li> <li>Provides get_model_params() for retrieving final clustering info</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial release</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/agglomerative/#scirex.experimental.ml.unsupervised.clustering.agglomerative.Agglomerative","title":"<code>Agglomerative</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>Agglomerative Clustering with optional user-defined 'n_clusters' or automatic silhouette-based selection.</p> <p>Attributes:</p> Name Type Description <code>n_clusters</code> <code>Optional[int]</code> <p>User-specified cluster count. If not provided, the algorithm auto-selects.</p> <code>max_k</code> <code>int</code> <p>Maximum number of clusters for auto-selection if n_clusters is None.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster labels for each data point after fitting.</p> <code>n_clusters_</code> <code>Optional[int]</code> <p>The actual number of clusters used in the final model.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/agglomerative.py</code> <pre><code>class Agglomerative(Clustering):\n    \"\"\"\n    Agglomerative Clustering with optional user-defined 'n_clusters' or\n    automatic silhouette-based selection.\n\n    Attributes:\n        n_clusters (Optional[int]):\n            User-specified cluster count. If not provided, the algorithm auto-selects.\n        max_k (int):\n            Maximum number of clusters for auto-selection if n_clusters is None.\n        labels (Optional[np.ndarray]):\n            Cluster labels for each data point after fitting.\n        n_clusters_ (Optional[int]):\n            The actual number of clusters used in the final model.\n    \"\"\"\n\n    def __init__(self, n_clusters: Optional[int] = None, max_k: int = 10) -&gt; None:\n        \"\"\"\n        Initialize the Agglomerative clustering class.\n\n        Args:\n            n_clusters (Optional[int], optional):\n                If provided, the class will use this cluster count directly.\n                Otherwise, it scans 2..max_k using silhouette. Defaults to None.\n            max_k (int, optional):\n                Maximum number of clusters to try (auto selection) if n_clusters is None.\n                Defaults to 10.\n        \"\"\"\n        super().__init__(\"agglomerative\")\n        self.n_clusters = n_clusters\n        self.max_k = max_k\n\n        self.labels: Optional[np.ndarray] = None\n        self.n_clusters_: Optional[int] = None\n        self.model: Optional[AgglomerativeClustering] = None\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the Agglomerative Clustering model to the data.\n\n        - If n_clusters is provided, skip auto selection and use that value.\n        - Else, compute silhouette scores for k in [2..max_k],\n          pick the best k, and finalize the clustering.\n\n        Args:\n            X (np.ndarray): Input data array of shape (n_samples, n_features).\n        \"\"\"\n        X = X.astype(np.float32, copy=False)\n        n_samples = X.shape[0]\n\n        # 1) If user specified a cluster count\n        if self.n_clusters is not None:\n            optimal_k = self.n_clusters\n            print(\n                f\"Fitting AgglomerativeClustering with user-defined n_clusters={optimal_k}.\\n\"\n            )\n        else:\n            # 2) Auto selection using silhouette\n            k_values = range(2, self.max_k + 1)\n            silhouette_scores = []\n            sample_size = min(1000, n_samples)\n\n            for k in k_values:\n                model = AgglomerativeClustering(n_clusters=k, linkage=\"average\")\n                labels = model.fit_predict(X)\n\n                if len(np.unique(labels)) &lt;= 1:\n                    silhouette_scores.append(-1)\n                    continue\n\n                if n_samples &gt; sample_size:\n                    indices = np.random.choice(n_samples, sample_size, replace=False)\n                    score = silhouette_score(X[indices], labels[indices])\n                else:\n                    score = silhouette_score(X, labels)\n\n                silhouette_scores.append(score)\n\n            # pick best silhouette\n            idx_best = np.argmax(silhouette_scores)\n            optimal_k = k_values[idx_best]\n            print(f\"Optimal k (silhouette) = {optimal_k}\")\n\n        # Final fit\n        self.model = AgglomerativeClustering(n_clusters=optimal_k, linkage=\"average\")\n        self.labels = self.model.fit_predict(X)\n        self.n_clusters_ = len(np.unique(self.labels))\n\n        print(f\"AgglomerativeClustering fitted with {optimal_k} clusters.\\n\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve key parameters and results from the fitted AgglomerativeClustering model.\n\n        Returns:\n            Dict[str, Any]:\n                - n_clusters (int): The final number of clusters used\n        \"\"\"\n        return {\"n_clusters\": self.n_clusters_}\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/agglomerative/#scirex.experimental.ml.unsupervised.clustering.agglomerative.Agglomerative.__init__","title":"<code>__init__(n_clusters=None, max_k=10)</code>","text":"<p>Initialize the Agglomerative clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>Optional[int]</code> <p>If provided, the class will use this cluster count directly. Otherwise, it scans 2..max_k using silhouette. Defaults to None.</p> <code>None</code> <code>max_k</code> <code>int</code> <p>Maximum number of clusters to try (auto selection) if n_clusters is None. Defaults to 10.</p> <code>10</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/agglomerative.py</code> <pre><code>def __init__(self, n_clusters: Optional[int] = None, max_k: int = 10) -&gt; None:\n    \"\"\"\n    Initialize the Agglomerative clustering class.\n\n    Args:\n        n_clusters (Optional[int], optional):\n            If provided, the class will use this cluster count directly.\n            Otherwise, it scans 2..max_k using silhouette. Defaults to None.\n        max_k (int, optional):\n            Maximum number of clusters to try (auto selection) if n_clusters is None.\n            Defaults to 10.\n    \"\"\"\n    super().__init__(\"agglomerative\")\n    self.n_clusters = n_clusters\n    self.max_k = max_k\n\n    self.labels: Optional[np.ndarray] = None\n    self.n_clusters_: Optional[int] = None\n    self.model: Optional[AgglomerativeClustering] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/agglomerative/#scirex.experimental.ml.unsupervised.clustering.agglomerative.Agglomerative.fit","title":"<code>fit(X)</code>","text":"<p>Fit the Agglomerative Clustering model to the data.</p> <ul> <li>If n_clusters is provided, skip auto selection and use that value.</li> <li>Else, compute silhouette scores for k in [2..max_k],   pick the best k, and finalize the clustering.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data array of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/agglomerative.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the Agglomerative Clustering model to the data.\n\n    - If n_clusters is provided, skip auto selection and use that value.\n    - Else, compute silhouette scores for k in [2..max_k],\n      pick the best k, and finalize the clustering.\n\n    Args:\n        X (np.ndarray): Input data array of shape (n_samples, n_features).\n    \"\"\"\n    X = X.astype(np.float32, copy=False)\n    n_samples = X.shape[0]\n\n    # 1) If user specified a cluster count\n    if self.n_clusters is not None:\n        optimal_k = self.n_clusters\n        print(\n            f\"Fitting AgglomerativeClustering with user-defined n_clusters={optimal_k}.\\n\"\n        )\n    else:\n        # 2) Auto selection using silhouette\n        k_values = range(2, self.max_k + 1)\n        silhouette_scores = []\n        sample_size = min(1000, n_samples)\n\n        for k in k_values:\n            model = AgglomerativeClustering(n_clusters=k, linkage=\"average\")\n            labels = model.fit_predict(X)\n\n            if len(np.unique(labels)) &lt;= 1:\n                silhouette_scores.append(-1)\n                continue\n\n            if n_samples &gt; sample_size:\n                indices = np.random.choice(n_samples, sample_size, replace=False)\n                score = silhouette_score(X[indices], labels[indices])\n            else:\n                score = silhouette_score(X, labels)\n\n            silhouette_scores.append(score)\n\n        # pick best silhouette\n        idx_best = np.argmax(silhouette_scores)\n        optimal_k = k_values[idx_best]\n        print(f\"Optimal k (silhouette) = {optimal_k}\")\n\n    # Final fit\n    self.model = AgglomerativeClustering(n_clusters=optimal_k, linkage=\"average\")\n    self.labels = self.model.fit_predict(X)\n    self.n_clusters_ = len(np.unique(self.labels))\n\n    print(f\"AgglomerativeClustering fitted with {optimal_k} clusters.\\n\")\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/agglomerative/#scirex.experimental.ml.unsupervised.clustering.agglomerative.Agglomerative.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Retrieve key parameters and results from the fitted AgglomerativeClustering model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - n_clusters (int): The final number of clusters used</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/agglomerative.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve key parameters and results from the fitted AgglomerativeClustering model.\n\n    Returns:\n        Dict[str, Any]:\n            - n_clusters (int): The final number of clusters used\n    \"\"\"\n    return {\"n_clusters\": self.n_clusters_}\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/","title":"Base","text":"<p>Module: base.py</p> <p>This module provides the abstract base class for all clustering implementations in SciREX. It defines shared functionality for:     - Data preparation (loading from CSV and standard scaling)     - Clustering metric computation (silhouette, calinski-harabasz, davies-bouldin)     - 2D plotting using PCA for visualization</p> <p>Classes:</p> Name Description <code>Clustering</code> <p>Abstract base class that outlines common behavior for clustering algorithms.</p> Dependencies <ul> <li>numpy, pandas, matplotlib, sklearn</li> <li>abc, pathlib, time, typing (for structural and type support)</li> </ul> Key Features <ul> <li>Consistent interface for loading and preparing data</li> <li>Standard approach to computing and returning clustering metrics</li> <li>PCA-based 2D plotting routine for visualizing clusters in two dimensions</li> <li>Enforces subclasses to implement <code>fit</code> and <code>get_model_params</code></li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering","title":"<code>Clustering</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for clustering algorithms in the SciREX library.</p> This class provides <ul> <li>A consistent interface for loading and preparing data</li> <li>A standard approach to computing and returning clustering metrics</li> <li>A PCA-based 2D plotting routine for visualizing clusters</li> </ul> Subclasses must <ol> <li>Implement the <code>fit(X: np.ndarray) -&gt; None</code> method, which should populate <code>self.labels</code>.</li> <li>Implement the <code>get_model_params() -&gt; Dict[str, Any]</code> method, which returns a dict    of model parameters for logging/debugging.</li> </ol> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>str</code> <p>The name or identifier of the clustering model (e.g., \"kmeans\", \"dbscan\").</p> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Array of cluster labels assigned to each sample after fitting.</p> <code>plots_dir</code> <code>Path</code> <p>Directory where cluster plots will be saved.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>class Clustering(ABC):\n    \"\"\"\n    Abstract base class for clustering algorithms in the SciREX library.\n\n    This class provides:\n      - A consistent interface for loading and preparing data\n      - A standard approach to computing and returning clustering metrics\n      - A PCA-based 2D plotting routine for visualizing clusters\n\n    Subclasses must:\n      1. Implement the `fit(X: np.ndarray) -&gt; None` method, which should populate `self.labels`.\n      2. Implement the `get_model_params() -&gt; Dict[str, Any]` method, which returns a dict\n         of model parameters for logging/debugging.\n\n    Attributes:\n        model_type (str): The name or identifier of the clustering model (e.g., \"kmeans\", \"dbscan\").\n        random_state (int): Random seed for reproducibility.\n        labels (Optional[np.ndarray]): Array of cluster labels assigned to each sample after fitting.\n        plots_dir (Path): Directory where cluster plots will be saved.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the base clustering class.\n\n        Args:\n            model_type (str): A string identifier for the clustering algorithm\n                              (e.g. \"kmeans\", \"dbscan\", etc.).\n            random_state (int, optional): Seed for reproducibility where applicable.\n                                          Defaults to 42.\n        \"\"\"\n        self.model_type = model_type\n        self.random_state = random_state\n\n        # Directory for saving plots\n        self.plots_dir = Path.cwd() / \"plots\"\n        self.plots_dir.mkdir(parents=True, exist_ok=True)\n\n        # Subclasses must set self.labels after fitting\n        self.labels: Optional[np.ndarray] = None\n\n    def prepare_data(self, path: str) -&gt; np.ndarray:\n        \"\"\"\n        Load and preprocess data from a CSV file, returning a scaled NumPy array.\n\n        This method:\n          1. Reads the CSV file into a pandas DataFrame.\n          2. Drops rows containing NaN values.\n          3. Selects only numeric columns from the DataFrame.\n          4. Scales these features using scikit-learn's StandardScaler.\n          5. Returns the scaled values as a NumPy array.\n\n        Args:\n            path (str): Filepath to the CSV data file.\n\n        Returns:\n            np.ndarray: A 2D array of shape (n_samples, n_features) containing\n                        standardized numeric data.\n\n        Raises:\n            ValueError: If no numeric columns are found in the data.\n        \"\"\"\n        df = pd.read_csv(Path(path))\n        df = df.dropna()\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n        if numeric_columns.empty:\n            raise ValueError(\"No numeric columns found in the data.\")\n        features = df[numeric_columns].values\n        return StandardScaler().fit_transform(features)\n\n    @abstractmethod\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the clustering model on a preprocessed dataset, assigning labels to `self.labels`.\n\n        Args:\n            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing\n                            the data to be clustered.\n\n        Subclasses must implement this method. After fitting the model,\n        `self.labels` should be set to an array of cluster labels of shape (n_samples,).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return model parameters for logging or debugging.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing key model parameters and\n                            potentially any learned attributes (e.g. number of clusters).\n        \"\"\"\n        pass\n\n    def plots(self, X: np.ndarray, labels: np.ndarray) -&gt; Tuple[Figure, Path]:\n        \"\"\"\n        Create a 2D scatter plot of clusters using PCA for dimensionality reduction.\n\n        Steps:\n          1. If X has &gt;=2 features, run PCA to reduce it to 2 components.\n          2. If X has only 1 feature, it is zero-padded to form a 2D embedding for plotting.\n          3. Each unique cluster label is plotted with a distinct color.\n          4. The figure is saved in `self.plots_dir` as `cluster_plot_{self.model_type}.png`.\n\n        Args:\n            X (np.ndarray): Data array of shape (n_samples, n_features).\n            labels (np.ndarray): Cluster labels for each sample.\n\n        Returns:\n            Tuple[Figure, Path]:\n              - The matplotlib Figure object.\n              - The path where the figure was saved (plot_path).\n\n        Notes:\n            Subclasses typically do not override this method. Instead, they rely on the\n            base implementation for consistent plotting.\n        \"\"\"\n        n_features = X.shape[1]\n        if n_features &gt;= 2:\n            pca = PCA(n_components=2, random_state=self.random_state)\n            X_pca = pca.fit_transform(X)\n        else:\n            # If there's only 1 feature, we simulate a second dimension with zeros\n            print(\"Dataset has only 1 feature. Zero-padding to 2D for visualization.\")\n            pca = PCA(n_components=1, random_state=self.random_state)\n            X_1d = pca.fit_transform(X)\n            X_pca = np.hstack([X_1d, np.zeros((X_1d.shape[0], 1))])\n\n        unique_labels = np.unique(labels)\n\n        fig = plt.figure(figsize=(8, 6), dpi=100)\n        ax = fig.add_subplot(111)\n\n        colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n        for label, color in zip(unique_labels, colors):\n            mask = labels == label\n            ax.scatter(\n                X_pca[mask, 0],\n                X_pca[mask, 1],\n                color=(\"k\" if label == -1 else color),\n                label=f\"Cluster {label}\",\n                alpha=0.7,\n                s=60,\n            )\n\n        ax.set_xlabel(\"PCA Component 1\", fontsize=10)\n        ax.set_ylabel(\"PCA Component 2\", fontsize=10)\n        ax.legend(fontsize=8)\n        ax.set_title(\n            f\"{self.model_type.upper()} Clustering Results (2D PCA)\",\n            fontsize=12,\n            pad=15,\n        )\n        plt.tight_layout()\n\n        # Save the figure\n        plot_path = self.plots_dir / f\"cluster_plot_{self.model_type}.png\"\n        fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n        plt.close(fig)\n        return fig, plot_path\n\n    def run(\n        self, data: Optional[np.ndarray] = None, path: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run the complete clustering pipeline: data loading/preprocessing,\n        fitting the model, and computing standard clustering metrics.\n\n        Args:\n            data (Optional[np.ndarray]): Preprocessed data array of shape (n_samples, n_features).\n            path (Optional[str]): Path to a CSV file from which to read data.\n                                  If `data` is not provided, this must be specified.\n\n        Returns:\n            Dict[str, Any]: A dictionary with the following keys:\n                - \"params\" (Dict[str, Any]): Model parameters from `self.get_model_params()`\n                - \"silhouette_score\" (float)\n                - \"calinski_harabasz_score\" (float)\n                - \"davies_bouldin_score\" (float)\n\n        Raises:\n            ValueError: If neither `data` nor `path` is provided, or if `self.labels`\n                        remains None after fitting (indicating a subclass didn't set it).\n        \"\"\"\n        if data is None and path is None:\n            raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n        # Load/prepare data if needed\n        X = data if data is not None else self.prepare_data(path)\n\n        # Fit the model\n        self.fit(X)\n\n        # Check labels\n        if self.labels is None:\n            raise ValueError(\"Model has not assigned labels. Did you implement fit()?\")\n\n        # Compute clustering metrics\n        silhouette = silhouette_score(X, self.labels)\n        calinski_harabasz = calinski_harabasz_score(X, self.labels)\n        davies_bouldin = davies_bouldin_score(X, self.labels)\n\n        # Return results\n        return {\n            \"params\": self.get_model_params(),\n            \"silhouette_score\": silhouette,\n            \"calinski_harabasz_score\": calinski_harabasz,\n            \"davies_bouldin_score\": davies_bouldin,\n        }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.__init__","title":"<code>__init__(model_type, random_state=42)</code>","text":"<p>Initialize the base clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>A string identifier for the clustering algorithm               (e.g. \"kmeans\", \"dbscan\", etc.).</p> required <code>random_state</code> <code>int</code> <p>Seed for reproducibility where applicable.                           Defaults to 42.</p> <code>42</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"\n    Initialize the base clustering class.\n\n    Args:\n        model_type (str): A string identifier for the clustering algorithm\n                          (e.g. \"kmeans\", \"dbscan\", etc.).\n        random_state (int, optional): Seed for reproducibility where applicable.\n                                      Defaults to 42.\n    \"\"\"\n    self.model_type = model_type\n    self.random_state = random_state\n\n    # Directory for saving plots\n    self.plots_dir = Path.cwd() / \"plots\"\n    self.plots_dir.mkdir(parents=True, exist_ok=True)\n\n    # Subclasses must set self.labels after fitting\n    self.labels: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.fit","title":"<code>fit(X)</code>  <code>abstractmethod</code>","text":"<p>Fit the clustering model on a preprocessed dataset, assigning labels to <code>self.labels</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array of shape (n_samples, n_features) containing             the data to be clustered.</p> required <p>Subclasses must implement this method. After fitting the model, <code>self.labels</code> should be set to an array of cluster labels of shape (n_samples,).</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>@abstractmethod\ndef fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the clustering model on a preprocessed dataset, assigning labels to `self.labels`.\n\n    Args:\n        X (np.ndarray): A 2D array of shape (n_samples, n_features) containing\n                        the data to be clustered.\n\n    Subclasses must implement this method. After fitting the model,\n    `self.labels` should be set to an array of cluster labels of shape (n_samples,).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.get_model_params","title":"<code>get_model_params()</code>  <code>abstractmethod</code>","text":"<p>Return model parameters for logging or debugging.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing key model parameters and             potentially any learned attributes (e.g. number of clusters).</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>@abstractmethod\ndef get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return model parameters for logging or debugging.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing key model parameters and\n                        potentially any learned attributes (e.g. number of clusters).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.plots","title":"<code>plots(X, labels)</code>","text":"<p>Create a 2D scatter plot of clusters using PCA for dimensionality reduction.</p> Steps <ol> <li>If X has &gt;=2 features, run PCA to reduce it to 2 components.</li> <li>If X has only 1 feature, it is zero-padded to form a 2D embedding for plotting.</li> <li>Each unique cluster label is plotted with a distinct color.</li> <li>The figure is saved in <code>self.plots_dir</code> as <code>cluster_plot_{self.model_type}.png</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data array of shape (n_samples, n_features).</p> required <code>labels</code> <code>ndarray</code> <p>Cluster labels for each sample.</p> required <p>Returns:</p> Type Description <code>Tuple[Figure, Path]</code> <p>Tuple[Figure, Path]: - The matplotlib Figure object. - The path where the figure was saved (plot_path).</p> Notes <p>Subclasses typically do not override this method. Instead, they rely on the base implementation for consistent plotting.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>def plots(self, X: np.ndarray, labels: np.ndarray) -&gt; Tuple[Figure, Path]:\n    \"\"\"\n    Create a 2D scatter plot of clusters using PCA for dimensionality reduction.\n\n    Steps:\n      1. If X has &gt;=2 features, run PCA to reduce it to 2 components.\n      2. If X has only 1 feature, it is zero-padded to form a 2D embedding for plotting.\n      3. Each unique cluster label is plotted with a distinct color.\n      4. The figure is saved in `self.plots_dir` as `cluster_plot_{self.model_type}.png`.\n\n    Args:\n        X (np.ndarray): Data array of shape (n_samples, n_features).\n        labels (np.ndarray): Cluster labels for each sample.\n\n    Returns:\n        Tuple[Figure, Path]:\n          - The matplotlib Figure object.\n          - The path where the figure was saved (plot_path).\n\n    Notes:\n        Subclasses typically do not override this method. Instead, they rely on the\n        base implementation for consistent plotting.\n    \"\"\"\n    n_features = X.shape[1]\n    if n_features &gt;= 2:\n        pca = PCA(n_components=2, random_state=self.random_state)\n        X_pca = pca.fit_transform(X)\n    else:\n        # If there's only 1 feature, we simulate a second dimension with zeros\n        print(\"Dataset has only 1 feature. Zero-padding to 2D for visualization.\")\n        pca = PCA(n_components=1, random_state=self.random_state)\n        X_1d = pca.fit_transform(X)\n        X_pca = np.hstack([X_1d, np.zeros((X_1d.shape[0], 1))])\n\n    unique_labels = np.unique(labels)\n\n    fig = plt.figure(figsize=(8, 6), dpi=100)\n    ax = fig.add_subplot(111)\n\n    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n    for label, color in zip(unique_labels, colors):\n        mask = labels == label\n        ax.scatter(\n            X_pca[mask, 0],\n            X_pca[mask, 1],\n            color=(\"k\" if label == -1 else color),\n            label=f\"Cluster {label}\",\n            alpha=0.7,\n            s=60,\n        )\n\n    ax.set_xlabel(\"PCA Component 1\", fontsize=10)\n    ax.set_ylabel(\"PCA Component 2\", fontsize=10)\n    ax.legend(fontsize=8)\n    ax.set_title(\n        f\"{self.model_type.upper()} Clustering Results (2D PCA)\",\n        fontsize=12,\n        pad=15,\n    )\n    plt.tight_layout()\n\n    # Save the figure\n    plot_path = self.plots_dir / f\"cluster_plot_{self.model_type}.png\"\n    fig.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n    plt.close(fig)\n    return fig, plot_path\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.prepare_data","title":"<code>prepare_data(path)</code>","text":"<p>Load and preprocess data from a CSV file, returning a scaled NumPy array.</p> This method <ol> <li>Reads the CSV file into a pandas DataFrame.</li> <li>Drops rows containing NaN values.</li> <li>Selects only numeric columns from the DataFrame.</li> <li>Scales these features using scikit-learn's StandardScaler.</li> <li>Returns the scaled values as a NumPy array.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Filepath to the CSV data file.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (n_samples, n_features) containing         standardized numeric data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no numeric columns are found in the data.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>def prepare_data(self, path: str) -&gt; np.ndarray:\n    \"\"\"\n    Load and preprocess data from a CSV file, returning a scaled NumPy array.\n\n    This method:\n      1. Reads the CSV file into a pandas DataFrame.\n      2. Drops rows containing NaN values.\n      3. Selects only numeric columns from the DataFrame.\n      4. Scales these features using scikit-learn's StandardScaler.\n      5. Returns the scaled values as a NumPy array.\n\n    Args:\n        path (str): Filepath to the CSV data file.\n\n    Returns:\n        np.ndarray: A 2D array of shape (n_samples, n_features) containing\n                    standardized numeric data.\n\n    Raises:\n        ValueError: If no numeric columns are found in the data.\n    \"\"\"\n    df = pd.read_csv(Path(path))\n    df = df.dropna()\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    if numeric_columns.empty:\n        raise ValueError(\"No numeric columns found in the data.\")\n    features = df[numeric_columns].values\n    return StandardScaler().fit_transform(features)\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/base/#scirex.experimental.ml.unsupervised.clustering.base.Clustering.run","title":"<code>run(data=None, path=None)</code>","text":"<p>Run the complete clustering pipeline: data loading/preprocessing, fitting the model, and computing standard clustering metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[ndarray]</code> <p>Preprocessed data array of shape (n_samples, n_features).</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Path to a CSV file from which to read data.                   If <code>data</code> is not provided, this must be specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary with the following keys: - \"params\" (Dict[str, Any]): Model parameters from <code>self.get_model_params()</code> - \"silhouette_score\" (float) - \"calinski_harabasz_score\" (float) - \"davies_bouldin_score\" (float)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>data</code> nor <code>path</code> is provided, or if <code>self.labels</code>         remains None after fitting (indicating a subclass didn't set it).</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/base.py</code> <pre><code>def run(\n    self, data: Optional[np.ndarray] = None, path: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run the complete clustering pipeline: data loading/preprocessing,\n    fitting the model, and computing standard clustering metrics.\n\n    Args:\n        data (Optional[np.ndarray]): Preprocessed data array of shape (n_samples, n_features).\n        path (Optional[str]): Path to a CSV file from which to read data.\n                              If `data` is not provided, this must be specified.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the following keys:\n            - \"params\" (Dict[str, Any]): Model parameters from `self.get_model_params()`\n            - \"silhouette_score\" (float)\n            - \"calinski_harabasz_score\" (float)\n            - \"davies_bouldin_score\" (float)\n\n    Raises:\n        ValueError: If neither `data` nor `path` is provided, or if `self.labels`\n                    remains None after fitting (indicating a subclass didn't set it).\n    \"\"\"\n    if data is None and path is None:\n        raise ValueError(\"Either 'data' or 'path' must be provided.\")\n\n    # Load/prepare data if needed\n    X = data if data is not None else self.prepare_data(path)\n\n    # Fit the model\n    self.fit(X)\n\n    # Check labels\n    if self.labels is None:\n        raise ValueError(\"Model has not assigned labels. Did you implement fit()?\")\n\n    # Compute clustering metrics\n    silhouette = silhouette_score(X, self.labels)\n    calinski_harabasz = calinski_harabasz_score(X, self.labels)\n    davies_bouldin = davies_bouldin_score(X, self.labels)\n\n    # Return results\n    return {\n        \"params\": self.get_model_params(),\n        \"silhouette_score\": silhouette,\n        \"calinski_harabasz_score\": calinski_harabasz,\n        \"davies_bouldin_score\": davies_bouldin,\n    }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/dbscan/","title":"Dbscan","text":"<p>Module: dbscan.py</p> <p>This module provides a DBSCAN (Density-Based Spatial Clustering of Applications with Noise) implementation.</p> <p>It includes an optional automated heuristic for estimating <code>eps</code> and <code>min_samples</code> by analyzing neighborhood distances. The user can override these defaults before fitting.</p> <p>Classes:</p> Name Description <code>Dbscan</code> <p>Implements DBSCAN with a simple heuristic for <code>eps</code> and <code>min_samples</code>.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.cluster.DBSCAN</li> <li>sklearn.neighbors.NearestNeighbors</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>Automatic estimation of <code>eps</code> via median k-distances</li> <li>Automatic estimation of <code>min_samples</code> via log2(n) heuristic</li> <li>Counting of discovered clusters and noise points</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/dbscan/#scirex.experimental.ml.unsupervised.clustering.dbscan.Dbscan","title":"<code>Dbscan</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>DBSCAN clustering algorithm with optional automatic estimation of <code>eps</code> and <code>min_samples</code>.</p> <p>Attributes:</p> Name Type Description <code>eps</code> <code>Optional[float]</code> <p>If provided, use this neighborhood distance for DBSCAN.</p> <code>min_samples</code> <code>Optional[int]</code> <p>If provided, use this minimum samples count for a point to be considered a core point.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster labels for each data point after fitting.</p> <code>n_clusters_</code> <code>Optional[int]</code> <p>The number of clusters found (excluding noise).</p> <code>n_noise_</code> <code>Optional[int]</code> <p>The number of noise points labeled as -1.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/dbscan.py</code> <pre><code>class Dbscan(Clustering):\n    \"\"\"\n    DBSCAN clustering algorithm with optional automatic estimation of `eps` and `min_samples`.\n\n    Attributes:\n        eps (Optional[float]):\n            If provided, use this neighborhood distance for DBSCAN.\n        min_samples (Optional[int]):\n            If provided, use this minimum samples count for a point to be considered a core point.\n        labels (Optional[np.ndarray]):\n            Cluster labels for each data point after fitting.\n        n_clusters_ (Optional[int]):\n            The number of clusters found (excluding noise).\n        n_noise_ (Optional[int]):\n            The number of noise points labeled as -1.\n    \"\"\"\n\n    def __init__(\n        self, eps: Optional[float] = None, min_samples: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Dbscan clustering class.\n\n        Args:\n            eps (Optional[float], optional):\n                User-defined neighborhood distance. If None, auto-estimation is used.\n            min_samples (Optional[int], optional):\n                User-defined min_samples. If None, auto-estimation is used.\n        \"\"\"\n        super().__init__(\"dbscan\")\n        self.eps = eps\n        self.min_samples = min_samples\n\n        # Final results after fitting\n        self.labels: Optional[np.ndarray] = None\n        self.n_clusters_: Optional[int] = None\n        self.n_noise_: Optional[int] = None\n        self.model: Optional[DBSCAN] = None\n\n    def _estimate_params(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Estimate eps and min_samples if they are not already set by the user,\n        using a heuristic approach:\n          - min_samples = max(5, floor(log2(n)) + 1)\n          - eps = median distance to the 'min_samples-th' nearest neighbor\n        \"\"\"\n        n_samples = X.shape[0]\n        rng = np.random.default_rng(self.random_state)\n\n        # Estimate min_samples if not set by user\n        if self.min_samples is None:\n            auto_min_samples = max(5, int(np.log2(n_samples)) + 1)\n            self.min_samples = auto_min_samples\n        else:\n            auto_min_samples = self.min_samples\n\n        # Estimate eps if not set by user\n        if self.eps is None:\n            # Subsample for distance estimation\n            sample_size = min(1000, n_samples)\n            indices = rng.choice(n_samples, sample_size, replace=False)\n            X_sample = X[indices]\n\n            # Compute the distance to the (min_samples)-th neighbor\n            nbrs = NearestNeighbors(n_neighbors=auto_min_samples)\n            nbrs.fit(X_sample)\n            distances, _ = nbrs.kneighbors(X_sample)\n            k_distances = distances[:, -1]\n\n            auto_eps = float(np.median(k_distances))\n            self.eps = auto_eps\n\n        print(\"Auto-estimated parameters:\")\n        print(f\"eps = {self.eps:.4f}, min_samples = {self.min_samples}\")\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the DBSCAN model to the data.\n\n        If eps or min_samples are None, a heuristic is used to estimate them.\n        The resulting DBSCAN model is stored in self.model, along with labels,\n        cluster count, and noise count.\n\n        Args:\n            X (np.ndarray): Input data of shape (n_samples, n_features).\n        \"\"\"\n        X = X.astype(np.float32, copy=False)\n\n        # If user didn't provide eps or min_samples, estimate them\n        if self.eps is None or self.min_samples is None:\n            self._estimate_params(X)\n        else:\n            print(\n                f\"Using user-defined eps = {self.eps}, min_samples = {self.min_samples}\"\n            )\n\n        # fit DBSCAN\n        self.model = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n        self.labels = self.model.fit_predict(X)\n\n        # Count clusters (excluding noise)\n        self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n        self.n_noise_ = np.count_nonzero(self.labels == -1)\n\n        print(f\"\\nDBSCAN fitted with eps={self.eps}, min_samples={self.min_samples}\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve key parameters and results from the fitted DBSCAN model.\n\n        Returns:\n            Dict[str, Any]:\n                - model_type (str): \"dbscan\"\n                - eps (float): The final eps used\n                - min_samples (int): The final min_samples used\n                - n_clusters (int): Number of clusters found (excluding noise)\n                - n_noise (int): Number of noise points (-1 label)\n        \"\"\"\n        return {\n            \"eps\": self.eps,\n            \"min_samples\": self.min_samples,\n            \"n_clusters\": self.n_clusters_,\n            \"n_noise\": self.n_noise_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/dbscan/#scirex.experimental.ml.unsupervised.clustering.dbscan.Dbscan.__init__","title":"<code>__init__(eps=None, min_samples=None)</code>","text":"<p>Initialize the Dbscan clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>Optional[float]</code> <p>User-defined neighborhood distance. If None, auto-estimation is used.</p> <code>None</code> <code>min_samples</code> <code>Optional[int]</code> <p>User-defined min_samples. If None, auto-estimation is used.</p> <code>None</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/dbscan.py</code> <pre><code>def __init__(\n    self, eps: Optional[float] = None, min_samples: Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Initialize the Dbscan clustering class.\n\n    Args:\n        eps (Optional[float], optional):\n            User-defined neighborhood distance. If None, auto-estimation is used.\n        min_samples (Optional[int], optional):\n            User-defined min_samples. If None, auto-estimation is used.\n    \"\"\"\n    super().__init__(\"dbscan\")\n    self.eps = eps\n    self.min_samples = min_samples\n\n    # Final results after fitting\n    self.labels: Optional[np.ndarray] = None\n    self.n_clusters_: Optional[int] = None\n    self.n_noise_: Optional[int] = None\n    self.model: Optional[DBSCAN] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/dbscan/#scirex.experimental.ml.unsupervised.clustering.dbscan.Dbscan.fit","title":"<code>fit(X)</code>","text":"<p>Fit the DBSCAN model to the data.</p> <p>If eps or min_samples are None, a heuristic is used to estimate them. The resulting DBSCAN model is stored in self.model, along with labels, cluster count, and noise count.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/dbscan.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the DBSCAN model to the data.\n\n    If eps or min_samples are None, a heuristic is used to estimate them.\n    The resulting DBSCAN model is stored in self.model, along with labels,\n    cluster count, and noise count.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n    \"\"\"\n    X = X.astype(np.float32, copy=False)\n\n    # If user didn't provide eps or min_samples, estimate them\n    if self.eps is None or self.min_samples is None:\n        self._estimate_params(X)\n    else:\n        print(\n            f\"Using user-defined eps = {self.eps}, min_samples = {self.min_samples}\"\n        )\n\n    # fit DBSCAN\n    self.model = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n    self.labels = self.model.fit_predict(X)\n\n    # Count clusters (excluding noise)\n    self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n    self.n_noise_ = np.count_nonzero(self.labels == -1)\n\n    print(f\"\\nDBSCAN fitted with eps={self.eps}, min_samples={self.min_samples}\")\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/dbscan/#scirex.experimental.ml.unsupervised.clustering.dbscan.Dbscan.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Retrieve key parameters and results from the fitted DBSCAN model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - model_type (str): \"dbscan\" - eps (float): The final eps used - min_samples (int): The final min_samples used - n_clusters (int): Number of clusters found (excluding noise) - n_noise (int): Number of noise points (-1 label)</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/dbscan.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve key parameters and results from the fitted DBSCAN model.\n\n    Returns:\n        Dict[str, Any]:\n            - model_type (str): \"dbscan\"\n            - eps (float): The final eps used\n            - min_samples (int): The final min_samples used\n            - n_clusters (int): Number of clusters found (excluding noise)\n            - n_noise (int): Number of noise points (-1 label)\n    \"\"\"\n    return {\n        \"eps\": self.eps,\n        \"min_samples\": self.min_samples,\n        \"n_clusters\": self.n_clusters_,\n        \"n_noise\": self.n_noise_,\n    }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/gmm/","title":"GMM","text":"<p>Module: gmm.py</p> <p>This module provides a Gaussian Mixture Model (GMM) clustering implementation using scikit-learn's GaussianMixture.</p> <p>It optionally allows a user-defined number of components or automatically scans [2..max_k] for the best silhouette score.</p> <p>Classes:</p> Name Description <code>Gmm</code> <p>Gaussian Mixture Model clustering with optional user-specified n_components  or silhouette-based auto selection.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.mixture.GaussianMixture</li> <li>sklearn.metrics.silhouette_score</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>Automatic scanning of [2..max_k] for best silhouette score if n_components is None</li> <li>Final model is stored, along with predicted cluster labels</li> <li>Ties into the base <code>Clustering</code> for plotting/metrics</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial release</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/gmm/#scirex.experimental.ml.unsupervised.clustering.gmm.Gmm","title":"<code>Gmm</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>Gaussian Mixture Model clustering with optional user-defined 'n_components' or automatic silhouette-based selection.</p> <p>Attributes:</p> Name Type Description <code>n_components</code> <code>Optional[int]</code> <p>The actual number of components used in the final fitted model. If provided, the class will skip auto-selection and directly use this many mixture components.</p> <code>max_k</code> <code>int</code> <p>Maximum number of components to consider for auto selection if n_components is None.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster/component labels for each data point after fitting.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/gmm.py</code> <pre><code>class Gmm(Clustering):\n    \"\"\"\n    Gaussian Mixture Model clustering with optional user-defined 'n_components'\n    or automatic silhouette-based selection.\n\n    Attributes:\n        n_components (Optional[int]):\n            The actual number of components used in the final fitted model.\n            If provided, the class will skip auto-selection\n            and directly use this many mixture components.\n        max_k (int):\n            Maximum number of components to consider for auto selection if n_components is None.\n        labels (Optional[np.ndarray]):\n            Cluster/component labels for each data point after fitting.\n    \"\"\"\n\n    def __init__(self, n_components: Optional[int] = None, max_k: int = 10) -&gt; None:\n        \"\"\"\n        Initialize the Gmm clustering class.\n\n        Args:\n            n_components (Optional[int], optional):\n                If provided, the model will directly use this many Gaussian components.\n                Otherwise, it scans [2..max_k] for the best silhouette score. Defaults to None.\n            max_k (int, optional):\n                Maximum components to try for auto selection if n_components is None. Defaults to 10.\n        \"\"\"\n        super().__init__(\"gmm\")\n        self.n_components = n_components\n        self.max_k = max_k\n\n        # Populated after fitting\n        self.labels: Optional[np.ndarray] = None\n        self.n_components_: Optional[int] = None\n        self.model: Optional[GaussianMixture] = None\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the GMM model to the data.\n\n        If user-defined n_components is set, skip auto selection.\n        Otherwise, compute silhouette scores across [2..max_k]\n        and pick the best.\n\n        Args:\n            X (np.ndarray): Scaled feature matrix of shape (n_samples, n_features).\n        \"\"\"\n        X = X.astype(np.float32, copy=False)\n        n_samples, n_features = X.shape\n\n        if self.n_components is not None:\n            # Use user-specified\n            self.n_components_ = self.n_components\n            print(f\"Fitting GMM with user-defined n_components={self.n_components_}.\\n\")\n        else:\n            # Automatic silhouette-based selection\n            k_values = range(2, self.max_k + 1)\n            silhouettes = []\n\n            # Subsampling for silhouette\n            rng = np.random.default_rng(self.random_state)\n            sample_size = min(1000, n_samples)\n\n            for k in k_values:\n                gmm = GaussianMixture(n_components=k, random_state=self.random_state)\n                gmm.fit(X)\n                labels_candidate = gmm.predict(X)\n\n                # Must have at least 2 distinct clusters for silhouette\n                if len(np.unique(labels_candidate)) &gt; 1:\n                    if n_samples &gt; sample_size:\n                        indices = rng.choice(n_samples, sample_size, replace=False)\n                        X_sample = X[indices]\n                        labels_sample = labels_candidate[indices]\n                    else:\n                        X_sample = X\n                        labels_sample = labels_candidate\n                    score = silhouette_score(X_sample, labels_sample)\n                else:\n                    score = -1  # invalid silhouette\n\n                silhouettes.append(score)\n\n            best_k = k_values[np.argmax(silhouettes)]\n            self.n_components_ = best_k\n            print(f\"Optimal k (silhouette) = {best_k}\\n\")\n\n        self.model = GaussianMixture(\n            n_components=self.n_components_, random_state=self.random_state\n        )\n        self.labels = self.model.fit_predict(X)\n\n        print(f\"GMM fitted with n_components={self.n_components_}.\\n\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get parameters/results of the fitted GMM model.\n\n        Returns:\n            Dict[str, Any]:\n                - n_components (int): The final number of components used\n                - max_k (int): The maximum considered if auto\n        \"\"\"\n        return {\"n_components\": self.n_components_, \"max_k\": self.max_k}\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/gmm/#scirex.experimental.ml.unsupervised.clustering.gmm.Gmm.__init__","title":"<code>__init__(n_components=None, max_k=10)</code>","text":"<p>Initialize the Gmm clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>Optional[int]</code> <p>If provided, the model will directly use this many Gaussian components. Otherwise, it scans [2..max_k] for the best silhouette score. Defaults to None.</p> <code>None</code> <code>max_k</code> <code>int</code> <p>Maximum components to try for auto selection if n_components is None. Defaults to 10.</p> <code>10</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/gmm.py</code> <pre><code>def __init__(self, n_components: Optional[int] = None, max_k: int = 10) -&gt; None:\n    \"\"\"\n    Initialize the Gmm clustering class.\n\n    Args:\n        n_components (Optional[int], optional):\n            If provided, the model will directly use this many Gaussian components.\n            Otherwise, it scans [2..max_k] for the best silhouette score. Defaults to None.\n        max_k (int, optional):\n            Maximum components to try for auto selection if n_components is None. Defaults to 10.\n    \"\"\"\n    super().__init__(\"gmm\")\n    self.n_components = n_components\n    self.max_k = max_k\n\n    # Populated after fitting\n    self.labels: Optional[np.ndarray] = None\n    self.n_components_: Optional[int] = None\n    self.model: Optional[GaussianMixture] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/gmm/#scirex.experimental.ml.unsupervised.clustering.gmm.Gmm.fit","title":"<code>fit(X)</code>","text":"<p>Fit the GMM model to the data.</p> <p>If user-defined n_components is set, skip auto selection. Otherwise, compute silhouette scores across [2..max_k] and pick the best.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Scaled feature matrix of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/gmm.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the GMM model to the data.\n\n    If user-defined n_components is set, skip auto selection.\n    Otherwise, compute silhouette scores across [2..max_k]\n    and pick the best.\n\n    Args:\n        X (np.ndarray): Scaled feature matrix of shape (n_samples, n_features).\n    \"\"\"\n    X = X.astype(np.float32, copy=False)\n    n_samples, n_features = X.shape\n\n    if self.n_components is not None:\n        # Use user-specified\n        self.n_components_ = self.n_components\n        print(f\"Fitting GMM with user-defined n_components={self.n_components_}.\\n\")\n    else:\n        # Automatic silhouette-based selection\n        k_values = range(2, self.max_k + 1)\n        silhouettes = []\n\n        # Subsampling for silhouette\n        rng = np.random.default_rng(self.random_state)\n        sample_size = min(1000, n_samples)\n\n        for k in k_values:\n            gmm = GaussianMixture(n_components=k, random_state=self.random_state)\n            gmm.fit(X)\n            labels_candidate = gmm.predict(X)\n\n            # Must have at least 2 distinct clusters for silhouette\n            if len(np.unique(labels_candidate)) &gt; 1:\n                if n_samples &gt; sample_size:\n                    indices = rng.choice(n_samples, sample_size, replace=False)\n                    X_sample = X[indices]\n                    labels_sample = labels_candidate[indices]\n                else:\n                    X_sample = X\n                    labels_sample = labels_candidate\n                score = silhouette_score(X_sample, labels_sample)\n            else:\n                score = -1  # invalid silhouette\n\n            silhouettes.append(score)\n\n        best_k = k_values[np.argmax(silhouettes)]\n        self.n_components_ = best_k\n        print(f\"Optimal k (silhouette) = {best_k}\\n\")\n\n    self.model = GaussianMixture(\n        n_components=self.n_components_, random_state=self.random_state\n    )\n    self.labels = self.model.fit_predict(X)\n\n    print(f\"GMM fitted with n_components={self.n_components_}.\\n\")\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/gmm/#scirex.experimental.ml.unsupervised.clustering.gmm.Gmm.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get parameters/results of the fitted GMM model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - n_components (int): The final number of components used - max_k (int): The maximum considered if auto</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/gmm.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get parameters/results of the fitted GMM model.\n\n    Returns:\n        Dict[str, Any]:\n            - n_components (int): The final number of components used\n            - max_k (int): The maximum considered if auto\n    \"\"\"\n    return {\"n_components\": self.n_components_, \"max_k\": self.max_k}\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/hdbscan/","title":"Hdbscan","text":"<p>Module: hdbscan.py</p> <p>This module provides an HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) implementation. It optionally allows user-defined <code>min_cluster_size</code> and <code>min_samples</code>, or applies heuristics to determine them.</p> <p>Classes:</p> Name Description <code>Hdbscan</code> <p>Implements HDBSCAN with an optional user override or heuristic-based approach.</p> Dependencies <ul> <li>numpy</li> <li>hdbscan (pip install hdbscan)</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>If user-defined 'min_cluster_size' or 'min_samples' is given, skip auto-heuristic</li> <li>Otherwise, compute simple heuristics</li> <li>Inherits from base <code>Clustering</code> for consistency with other clustering modules</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial release</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/hdbscan/#scirex.experimental.ml.unsupervised.clustering.hdbscan.Hdbscan","title":"<code>Hdbscan</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>HDBSCAN clustering with optional user-defined 'min_cluster_size' and 'min_samples', or a heuristic-based approach if they are not provided.</p> <p>Attributes:</p> Name Type Description <code>min_cluster_size</code> <code>Optional[int]</code> <p>User-specified or auto-calculated minimum cluster size.</p> <code>min_samples</code> <code>Optional[int]</code> <p>User-specified or auto-calculated minimum samples for a point to be core.</p> <code>cluster_selection_method</code> <code>str</code> <p>Method for extracting clusters from the condensed tree. Defaults to 'eom'.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster labels for each data point after fitting (some may be -1 for noise).</p> <code>n_clusters_</code> <code>Optional[int]</code> <p>Number of clusters discovered (excluding noise).</p> <code>n_noise_</code> <code>Optional[int]</code> <p>Number of data points labeled as noise (-1).</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/hdbscan.py</code> <pre><code>class Hdbscan(Clustering):\n    \"\"\"\n    HDBSCAN clustering with optional user-defined 'min_cluster_size' and 'min_samples',\n    or a heuristic-based approach if they are not provided.\n\n    Attributes:\n        min_cluster_size (Optional[int]):\n            User-specified or auto-calculated minimum cluster size.\n        min_samples (Optional[int]):\n            User-specified or auto-calculated minimum samples for a point to be core.\n        cluster_selection_method (str):\n            Method for extracting clusters from the condensed tree. Defaults to 'eom'.\n        labels (Optional[np.ndarray]):\n            Cluster labels for each data point after fitting (some may be -1 for noise).\n        n_clusters_ (Optional[int]):\n            Number of clusters discovered (excluding noise).\n        n_noise_ (Optional[int]):\n            Number of data points labeled as noise (-1).\n    \"\"\"\n\n    def __init__(\n        self,\n        min_cluster_size: Optional[int] = None,\n        min_samples: Optional[int] = None,\n        cluster_selection_method: str = \"eom\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the HDBSCAN clustering model.\n\n        Args:\n            min_cluster_size (Optional[int], optional):\n                If provided, HDBSCAN will use this min cluster size directly.\n            min_samples (Optional[int], optional):\n                If provided, HDBSCAN will use this min_samples directly.\n            cluster_selection_method (str, optional):\n                The method to extract clusters from condensed tree:\n                'eom' (Excess of Mass) or 'leaf'. Defaults to 'eom'.\n        \"\"\"\n        super().__init__(\"hdbscan\")\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n        self.cluster_selection_method = cluster_selection_method\n\n        # Attributes set after fitting\n        self.labels: Optional[np.ndarray] = None\n        self.n_clusters_: Optional[int] = None\n        self.n_noise_: Optional[int] = None\n        self.model: Optional[HDBSCAN] = None\n\n    def _estimate_params(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Estimate min_cluster_size and min_samples using simple heuristics:\n          - min_samples = max(1, floor(log(n)))\n          - min_cluster_size = max(5, floor(2% of n))\n        \"\"\"\n        n_samples = X.shape[0]\n        # Heuristic for min_samples\n        auto_min_samples = max(1, int(np.log(n_samples)))\n        # Heuristic for min_cluster_size\n        auto_min_cluster_size = max(5, int(0.02 * n_samples))\n\n        # If not user-defined, assign\n        if self.min_cluster_size is None:\n            self.min_cluster_size = auto_min_cluster_size\n        if self.min_samples is None:\n            self.min_samples = auto_min_samples\n\n        print(\"Auto-estimated parameters for HDBSCAN:\")\n        print(\n            f\"min_cluster_size = {self.min_cluster_size}, \"\n            f\"min_samples = {self.min_samples}\"\n        )\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit HDBSCAN to the data.\n\n        - If min_cluster_size/min_samples are None, estimate them heuristically.\n        - Then create and fit an HDBSCAN model, storing labels, cluster count, and noise count.\n\n        Args:\n            X (np.ndarray): Input data array of shape (n_samples, n_features).\n        \"\"\"\n        X = X.astype(np.float32, copy=False)\n\n        # If user did not provide min_cluster_size or min_samples, estimate them\n        if self.min_cluster_size is None or self.min_samples is None:\n            self._estimate_params(X)\n        else:\n            print(\n                f\"Using user-defined parameters: \"\n                f\"min_cluster_size={self.min_cluster_size}, \"\n                f\"min_samples={self.min_samples}\"\n            )\n\n        # Create and fit the model\n        self.model = HDBSCAN(\n            min_cluster_size=self.min_cluster_size,\n            min_samples=self.min_samples,\n            cluster_selection_method=self.cluster_selection_method,\n        )\n        self.labels = self.model.fit_predict(X)\n\n        # Count clusters (excluding noise)\n        self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n        self.n_noise_ = np.count_nonzero(self.labels == -1)\n\n        print(\n            f\"\\nHDBSCAN fitted with min_cluster_size={self.min_cluster_size}, \"\n            f\"min_samples={self.min_samples}\"\n        )\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve key parameters and results from the fitted HDBSCAN model.\n\n        Returns:\n            Dict[str, Any]:\n                - min_cluster_size (int): Final min_cluster_size used\n                - min_samples (int): Final min_samples used\n                - n_clusters (int): Number of clusters discovered\n                - n_noise (int): Number of noise points\n        \"\"\"\n        return {\n            \"min_cluster_size\": self.min_cluster_size,\n            \"min_samples\": self.min_samples,\n            \"n_clusters\": self.n_clusters_,\n            \"n_noise\": self.n_noise_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/hdbscan/#scirex.experimental.ml.unsupervised.clustering.hdbscan.Hdbscan.__init__","title":"<code>__init__(min_cluster_size=None, min_samples=None, cluster_selection_method='eom')</code>","text":"<p>Initialize the HDBSCAN clustering model.</p> <p>Parameters:</p> Name Type Description Default <code>min_cluster_size</code> <code>Optional[int]</code> <p>If provided, HDBSCAN will use this min cluster size directly.</p> <code>None</code> <code>min_samples</code> <code>Optional[int]</code> <p>If provided, HDBSCAN will use this min_samples directly.</p> <code>None</code> <code>cluster_selection_method</code> <code>str</code> <p>The method to extract clusters from condensed tree: 'eom' (Excess of Mass) or 'leaf'. Defaults to 'eom'.</p> <code>'eom'</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/hdbscan.py</code> <pre><code>def __init__(\n    self,\n    min_cluster_size: Optional[int] = None,\n    min_samples: Optional[int] = None,\n    cluster_selection_method: str = \"eom\",\n) -&gt; None:\n    \"\"\"\n    Initialize the HDBSCAN clustering model.\n\n    Args:\n        min_cluster_size (Optional[int], optional):\n            If provided, HDBSCAN will use this min cluster size directly.\n        min_samples (Optional[int], optional):\n            If provided, HDBSCAN will use this min_samples directly.\n        cluster_selection_method (str, optional):\n            The method to extract clusters from condensed tree:\n            'eom' (Excess of Mass) or 'leaf'. Defaults to 'eom'.\n    \"\"\"\n    super().__init__(\"hdbscan\")\n    self.min_cluster_size = min_cluster_size\n    self.min_samples = min_samples\n    self.cluster_selection_method = cluster_selection_method\n\n    # Attributes set after fitting\n    self.labels: Optional[np.ndarray] = None\n    self.n_clusters_: Optional[int] = None\n    self.n_noise_: Optional[int] = None\n    self.model: Optional[HDBSCAN] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/hdbscan/#scirex.experimental.ml.unsupervised.clustering.hdbscan.Hdbscan.fit","title":"<code>fit(X)</code>","text":"<p>Fit HDBSCAN to the data.</p> <ul> <li>If min_cluster_size/min_samples are None, estimate them heuristically.</li> <li>Then create and fit an HDBSCAN model, storing labels, cluster count, and noise count.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data array of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/hdbscan.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit HDBSCAN to the data.\n\n    - If min_cluster_size/min_samples are None, estimate them heuristically.\n    - Then create and fit an HDBSCAN model, storing labels, cluster count, and noise count.\n\n    Args:\n        X (np.ndarray): Input data array of shape (n_samples, n_features).\n    \"\"\"\n    X = X.astype(np.float32, copy=False)\n\n    # If user did not provide min_cluster_size or min_samples, estimate them\n    if self.min_cluster_size is None or self.min_samples is None:\n        self._estimate_params(X)\n    else:\n        print(\n            f\"Using user-defined parameters: \"\n            f\"min_cluster_size={self.min_cluster_size}, \"\n            f\"min_samples={self.min_samples}\"\n        )\n\n    # Create and fit the model\n    self.model = HDBSCAN(\n        min_cluster_size=self.min_cluster_size,\n        min_samples=self.min_samples,\n        cluster_selection_method=self.cluster_selection_method,\n    )\n    self.labels = self.model.fit_predict(X)\n\n    # Count clusters (excluding noise)\n    self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n    self.n_noise_ = np.count_nonzero(self.labels == -1)\n\n    print(\n        f\"\\nHDBSCAN fitted with min_cluster_size={self.min_cluster_size}, \"\n        f\"min_samples={self.min_samples}\"\n    )\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/hdbscan/#scirex.experimental.ml.unsupervised.clustering.hdbscan.Hdbscan.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Retrieve key parameters and results from the fitted HDBSCAN model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - min_cluster_size (int): Final min_cluster_size used - min_samples (int): Final min_samples used - n_clusters (int): Number of clusters discovered - n_noise (int): Number of noise points</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/hdbscan.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve key parameters and results from the fitted HDBSCAN model.\n\n    Returns:\n        Dict[str, Any]:\n            - min_cluster_size (int): Final min_cluster_size used\n            - min_samples (int): Final min_samples used\n            - n_clusters (int): Number of clusters discovered\n            - n_noise (int): Number of noise points\n    \"\"\"\n    return {\n        \"min_cluster_size\": self.min_cluster_size,\n        \"min_samples\": self.min_samples,\n        \"n_clusters\": self.n_clusters_,\n        \"n_noise\": self.n_noise_,\n    }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/kmeans/","title":"Kmeans","text":"<p>Module: kmeans.py</p> <p>This module provides a K-means clustering implementation using scikit-learn's MiniBatchKMeans. The Kmeans class inherits from a generic Clustering base class and offers:   - Option for the user to input a custom number of clusters   - Option for Automatic selection of the optimal number of clusters via silhouette or elbow methods</p> <p>Classes:</p> Name Description <code>Kmeans</code> <p>K-Means clustering with automatic parameter selection and optional user override.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.cluster.MiniBatchKMeans</li> <li>sklearn.metrics.silhouette_score</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>Scans [2..max_k] to find the best cluster count using silhouette or elbow</li> <li>Final cluster count stored in <code>optimal_k</code>, with a fitted model and labels</li> <li>Inherits from the base <code>Clustering</code> for consistent plotting and metric computation</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial version</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/kmeans/#scirex.experimental.ml.unsupervised.clustering.kmeans.Kmeans","title":"<code>Kmeans</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>K-Means clustering with optional user-defined 'n_clusters' or automatic selection.</p> <p>Attributes:</p> Name Type Description <code>n_clusters</code> <code>Optional[int]</code> <p>If provided, the class will skip automatic selection and use this number of clusters.</p> <code>max_k</code> <code>int</code> <p>Maximum number of clusters to consider for automatic selection if n_clusters is None.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster labels for each data point after fitting.</p> <code>n_clusters_</code> <code>Optional[int]</code> <p>The actual number of clusters used by the final fitted model.</p> <code>inertia_</code> <code>Optional[float]</code> <p>The final inertia (sum of squared distances to the closest centroid).</p> <code>cluster_centers_</code> <code>Optional[ndarray]</code> <p>Coordinates of cluster centers in the final fitted model.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/kmeans.py</code> <pre><code>class Kmeans(Clustering):\n    \"\"\"\n    K-Means clustering with optional user-defined 'n_clusters' or automatic selection.\n\n    Attributes:\n        n_clusters (Optional[int]):\n            If provided, the class will skip automatic selection and use this number of clusters.\n        max_k (int):\n            Maximum number of clusters to consider for automatic selection if n_clusters is None.\n        labels (Optional[np.ndarray]):\n            Cluster labels for each data point after fitting.\n        n_clusters_ (Optional[int]):\n            The actual number of clusters used by the final fitted model.\n        inertia_ (Optional[float]):\n            The final inertia (sum of squared distances to the closest centroid).\n        cluster_centers_ (Optional[np.ndarray]):\n            Coordinates of cluster centers in the final fitted model.\n    \"\"\"\n\n    def __init__(\n        self, n_clusters: Optional[int] = None, max_k: Optional[int] = 10\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Kmeans clustering class.\n\n        Args:\n            n_clusters (Optional[int], optional):\n                User-defined number of clusters when provided, the algorithm will ignore automatic selection and directly use 'n_clusters'.\n                Defaults to None.\n            max_k (Optional[int], optional):\n                Maximum number of clusters to try for automatic selection if n_clusters is None.\n                Defaults to 10.\n        \"\"\"\n        super().__init__(\"kmeans\")\n        self.n_clusters = n_clusters\n        self.max_k = max_k\n\n        # Attributes populated after fitting\n        self.labels: Optional[np.ndarray] = None\n        self.n_clusters_: Optional[int] = None\n        self.inertia_: Optional[float] = None\n        self.cluster_centers_: Optional[np.ndarray] = None\n\n    def _calculate_elbow_scores(self, X: np.ndarray, k_values: range) -&gt; np.ndarray:\n        \"\"\"\n        Calculate inertia scores for each candidate k, then compute a second-derivative-like ratio\n        for the elbow method.\n\n        Args:\n            X (np.ndarray):\n                Input data of shape (n_samples, n_features).\n            k_values (range):\n                Range of k values to evaluate (e.g., range(2, max_k+1)).\n\n        Returns:\n            np.ndarray:\n                An array of \"elbow scores\" computed from the second derivative of inertia\n                normalized by the first derivative. Higher values indicate a stronger elbow.\n                If there are insufficient points to compute second derivatives, returns an empty array.\n        \"\"\"\n        inertias = []\n        for k in k_values:\n            kmeans = MiniBatchKMeans(\n                n_clusters=k, random_state=self.random_state, batch_size=1000\n            )\n            kmeans.fit(X)\n            inertias.append(kmeans.inertia_)\n\n        inertias = np.array(inertias)\n        diffs = np.diff(inertias)\n        if len(diffs) &lt; 2:\n            return np.zeros(0)\n\n        # Second derivative\n        diffs_r = np.diff(diffs)\n\n        # Avoid division by zero or extremely small values\n        valid_mask = np.abs(diffs[1:]) &gt; 1e-10\n        elbow_score = np.zeros_like(diffs_r)\n        elbow_score[valid_mask] = diffs_r[valid_mask] / np.abs(diffs[1:][valid_mask])\n\n        return elbow_score\n\n    def _calculate_silhouette_scores(\n        self, X: np.ndarray, k_values: range\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Calculate silhouette scores for each candidate k in k_values.\n\n        Args:\n            X (np.ndarray):\n                Input data of shape (n_samples, n_features).\n            k_values (range):\n                Range of k values to evaluate (e.g., range(2, max_k+1)).\n\n        Returns:\n            np.ndarray:\n                Silhouette scores for each candidate k. A higher silhouette score indicates\n                better separation between clusters.\n        \"\"\"\n        silhouette_scores = []\n        sample_size = min(1000, X.shape[0])\n        rng = np.random.default_rng(self.random_state)\n\n        for k in k_values:\n            kmeans = MiniBatchKMeans(\n                n_clusters=k, random_state=self.random_state, batch_size=1000\n            )\n            labels = kmeans.fit_predict(X)\n\n            # Silhouette is undefined for a single cluster\n            if len(np.unique(labels)) &lt;= 1:\n                silhouette_scores.append(-1)\n                continue\n\n            # Subsample to speed computations if dataset is very large\n            if X.shape[0] &gt; sample_size:\n                indices = rng.choice(X.shape[0], sample_size, replace=False)\n                X_sample = X[indices]\n                labels_sample = labels[indices]\n            else:\n                X_sample = X\n                labels_sample = labels\n\n            score = silhouette_score(X_sample, labels_sample)\n            silhouette_scores.append(score)\n\n        return np.array(silhouette_scores)\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the K-Means model to the data. If 'n_clusters' is set by the user, it uses that directly.\n        Otherwise, it performs automatic selection of the optimal number of clusters using both\n        the silhouette and elbow methods.\n\n        Args:\n            X (np.ndarray):\n                Scaled feature matrix of shape (n_samples, n_features).\n        \"\"\"\n        if self.n_clusters is not None:\n            # User-specified number of clusters\n            self.optimal_k = self.n_clusters\n        else:\n            # Automatic selection\n            k_values = range(2, self.max_k + 1)\n            silhouette_scores = self._calculate_silhouette_scores(X, k_values)\n            elbow_scores = self._calculate_elbow_scores(X, k_values)\n\n            # Fallback if we can't compute elbow for some reason\n            if len(k_values) &lt;= 2 or len(elbow_scores) == 0:\n                self.optimal_k = 2\n            else:\n                optimal_k_silhouette = k_values[np.argmax(silhouette_scores)]\n                optimal_k_elbow = k_values[:-2][np.argmax(elbow_scores)]\n\n                print(f\"Optimal k (silhouette) = {optimal_k_silhouette}\")\n                print(f\"Optimal k (elbow)      = {optimal_k_elbow}\")\n\n                self.optimal_k = optimal_k_elbow\n\n        # Fit final model with chosen k\n        self.model = MiniBatchKMeans(\n            n_clusters=self.optimal_k, random_state=self.random_state, batch_size=1000\n        )\n        self.labels = self.model.fit_predict(X)\n        self.n_clusters_ = len(np.unique(self.labels))\n\n        self.inertia_ = self.model.inertia_\n        self.cluster_centers_ = self.model.cluster_centers_\n\n        print(f\"\\nKMeans fitted with {self.optimal_k} clusters.\")\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve key parameters and results from the fitted K-Means model.\n\n        Returns:\n            Dict[str, Any]:\n                - max_k (int): Maximum clusters originally specified for auto-selection\n                - n_clusters (int): The actual number of clusters used in the final model\n                - inertia_ (float): Final within-cluster sum of squares (inertia)\n                - cluster_centers_ (Optional[List[List[float]]]): Cluster centers as a list of lists\n        \"\"\"\n        return {\n            \"max_k\": self.max_k,\n            \"n_clusters\": self.n_clusters_,\n            \"inertia_\": self.inertia_,\n            \"cluster_centers_\": (\n                self.cluster_centers_.tolist()\n                if self.cluster_centers_ is not None\n                else None\n            ),\n        }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/kmeans/#scirex.experimental.ml.unsupervised.clustering.kmeans.Kmeans.__init__","title":"<code>__init__(n_clusters=None, max_k=10)</code>","text":"<p>Initialize the Kmeans clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>Optional[int]</code> <p>User-defined number of clusters when provided, the algorithm will ignore automatic selection and directly use 'n_clusters'. Defaults to None.</p> <code>None</code> <code>max_k</code> <code>Optional[int]</code> <p>Maximum number of clusters to try for automatic selection if n_clusters is None. Defaults to 10.</p> <code>10</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/kmeans.py</code> <pre><code>def __init__(\n    self, n_clusters: Optional[int] = None, max_k: Optional[int] = 10\n) -&gt; None:\n    \"\"\"\n    Initialize the Kmeans clustering class.\n\n    Args:\n        n_clusters (Optional[int], optional):\n            User-defined number of clusters when provided, the algorithm will ignore automatic selection and directly use 'n_clusters'.\n            Defaults to None.\n        max_k (Optional[int], optional):\n            Maximum number of clusters to try for automatic selection if n_clusters is None.\n            Defaults to 10.\n    \"\"\"\n    super().__init__(\"kmeans\")\n    self.n_clusters = n_clusters\n    self.max_k = max_k\n\n    # Attributes populated after fitting\n    self.labels: Optional[np.ndarray] = None\n    self.n_clusters_: Optional[int] = None\n    self.inertia_: Optional[float] = None\n    self.cluster_centers_: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/kmeans/#scirex.experimental.ml.unsupervised.clustering.kmeans.Kmeans.fit","title":"<code>fit(X)</code>","text":"<p>Fit the K-Means model to the data. If 'n_clusters' is set by the user, it uses that directly. Otherwise, it performs automatic selection of the optimal number of clusters using both the silhouette and elbow methods.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Scaled feature matrix of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/kmeans.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the K-Means model to the data. If 'n_clusters' is set by the user, it uses that directly.\n    Otherwise, it performs automatic selection of the optimal number of clusters using both\n    the silhouette and elbow methods.\n\n    Args:\n        X (np.ndarray):\n            Scaled feature matrix of shape (n_samples, n_features).\n    \"\"\"\n    if self.n_clusters is not None:\n        # User-specified number of clusters\n        self.optimal_k = self.n_clusters\n    else:\n        # Automatic selection\n        k_values = range(2, self.max_k + 1)\n        silhouette_scores = self._calculate_silhouette_scores(X, k_values)\n        elbow_scores = self._calculate_elbow_scores(X, k_values)\n\n        # Fallback if we can't compute elbow for some reason\n        if len(k_values) &lt;= 2 or len(elbow_scores) == 0:\n            self.optimal_k = 2\n        else:\n            optimal_k_silhouette = k_values[np.argmax(silhouette_scores)]\n            optimal_k_elbow = k_values[:-2][np.argmax(elbow_scores)]\n\n            print(f\"Optimal k (silhouette) = {optimal_k_silhouette}\")\n            print(f\"Optimal k (elbow)      = {optimal_k_elbow}\")\n\n            self.optimal_k = optimal_k_elbow\n\n    # Fit final model with chosen k\n    self.model = MiniBatchKMeans(\n        n_clusters=self.optimal_k, random_state=self.random_state, batch_size=1000\n    )\n    self.labels = self.model.fit_predict(X)\n    self.n_clusters_ = len(np.unique(self.labels))\n\n    self.inertia_ = self.model.inertia_\n    self.cluster_centers_ = self.model.cluster_centers_\n\n    print(f\"\\nKMeans fitted with {self.optimal_k} clusters.\")\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/kmeans/#scirex.experimental.ml.unsupervised.clustering.kmeans.Kmeans.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Retrieve key parameters and results from the fitted K-Means model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - max_k (int): Maximum clusters originally specified for auto-selection - n_clusters (int): The actual number of clusters used in the final model - inertia_ (float): Final within-cluster sum of squares (inertia) - cluster_centers_ (Optional[List[List[float]]]): Cluster centers as a list of lists</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/kmeans.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve key parameters and results from the fitted K-Means model.\n\n    Returns:\n        Dict[str, Any]:\n            - max_k (int): Maximum clusters originally specified for auto-selection\n            - n_clusters (int): The actual number of clusters used in the final model\n            - inertia_ (float): Final within-cluster sum of squares (inertia)\n            - cluster_centers_ (Optional[List[List[float]]]): Cluster centers as a list of lists\n    \"\"\"\n    return {\n        \"max_k\": self.max_k,\n        \"n_clusters\": self.n_clusters_,\n        \"inertia_\": self.inertia_,\n        \"cluster_centers_\": (\n            self.cluster_centers_.tolist()\n            if self.cluster_centers_ is not None\n            else None\n        ),\n    }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/optics/","title":"Optics","text":"<p>Module: optics.py</p> <p>This module provides an OPTICS (Ordering Points To Identify the Clustering Structure) implementation.</p> <p>It allows optional user-defined 'min_samples' and 'min_cluster_size', or applies a heuristic approach if they're not provided.</p> <p>Classes:</p> Name Description <code>Optics</code> <p>Implements OPTICS with optional user override or heuristic-based approach.</p> Dependencies <ul> <li>numpy</li> <li>sklearn.cluster.OPTICS</li> <li>base.py (Clustering)</li> </ul> Key Features <ul> <li>If user-defined 'min_samples' or 'min_cluster_size' is set, skip auto-heuristic</li> <li>Otherwise, compute a simple heuristic</li> </ul> Authors <ul> <li>Debajyoti Sahoo (debajyotis@iisc.ac.in)</li> </ul> Version Info <ul> <li>28/Dec/2024: Initial release</li> </ul>"},{"location":"api/experimental/ml/unsupervised/clustering/optics/#scirex.experimental.ml.unsupervised.clustering.optics.Optics","title":"<code>Optics</code>","text":"<p>               Bases: <code>Clustering</code></p> <p>OPTICS clustering with optional user-defined 'min_samples' and 'min_cluster_size', or a heuristic-based approach if they are not provided.</p> <p>Attributes:</p> Name Type Description <code>min_samples</code> <code>Optional[int]</code> <p>If provided, used directly by OPTICS; otherwise estimated.</p> <code>min_cluster_size</code> <code>Optional[int]</code> <p>If provided, used directly by OPTICS; otherwise estimated.</p> <code>xi</code> <code>float</code> <p>Determines the minimum steepness on the reachability plot for cluster extraction.</p> <code>labels</code> <code>Optional[ndarray]</code> <p>Cluster labels for each data point after fitting (-1 for noise).</p> <code>n_clusters_</code> <code>Optional[int]</code> <p>Number of clusters discovered (excluding noise).</p> <code>n_noise_</code> <code>Optional[int]</code> <p>Number of data points labeled as noise.</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/optics.py</code> <pre><code>class Optics(Clustering):\n    \"\"\"\n    OPTICS clustering with optional user-defined 'min_samples' and 'min_cluster_size',\n    or a heuristic-based approach if they are not provided.\n\n    Attributes:\n        min_samples (Optional[int]):\n            If provided, used directly by OPTICS; otherwise estimated.\n        min_cluster_size (Optional[int]):\n            If provided, used directly by OPTICS; otherwise estimated.\n        xi (float):\n            Determines the minimum steepness on the reachability plot for cluster extraction.\n        labels (Optional[np.ndarray]):\n            Cluster labels for each data point after fitting (-1 for noise).\n        n_clusters_ (Optional[int]):\n            Number of clusters discovered (excluding noise).\n        n_noise_ (Optional[int]):\n            Number of data points labeled as noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        min_samples: Optional[int] = None,\n        min_cluster_size: Optional[int] = None,\n        xi: float = 0.05,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the OPTICS clustering model.\n\n        Args:\n            min_samples (Optional[int], optional):\n                If provided, the algorithm uses this min_samples. Otherwise, a heuristic is used.\n            min_cluster_size (Optional[int], optional):\n                If provided, the algorithm uses this min_cluster_size. Otherwise, a heuristic is used.\n            xi (float, optional):\n                Determines the minimum steepness on the reachability plot for cluster extraction.\n                Defaults to 0.05.\n        \"\"\"\n        super().__init__(\"optics\")\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.xi = xi\n\n        # Attributes set after fitting\n        self.labels: Optional[np.ndarray] = None\n        self.n_clusters_: Optional[int] = None\n        self.n_noise_: Optional[int] = None\n\n    def _estimate_params(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Estimate 'min_samples' and 'min_cluster_size' if they are not already provided\n        by the user. We apply a simple heuristic approach:\n          - min_samples = max(5, floor(log2(n)) + 1)\n          - min_cluster_size = max(min_samples, min(50, floor(0.05 * n)))\n        \"\"\"\n        n_samples = X.shape[0]\n\n        if self.min_samples is None:\n            auto_min_samples = max(5, int(np.log2(n_samples)) + 1)\n            self.min_samples = auto_min_samples\n\n        if self.min_cluster_size is None:\n            # Use min_cluster_size &gt;= min_samples, up to 5% of data or 50\n            auto_min_cluster_size = max(\n                self.min_samples, min(50, int(0.05 * n_samples))\n            )\n            self.min_cluster_size = auto_min_cluster_size\n\n        print(\"Auto-estimated parameters for OPTICS:\")\n        print(\n            f\"min_samples = {self.min_samples}, min_cluster_size = {self.min_cluster_size}\"\n        )\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Fit the OPTICS model to the data.\n        - If min_samples/min_cluster_size are not set, estimate them heuristically.\n        - Then create and fit an OPTICS model, storing labels and cluster info.\n\n        Args:\n            X (np.ndarray): Input data array of shape (n_samples, n_features).\n        \"\"\"\n        X = X.astype(np.float32, copy=False)\n        n_samples = X.shape[0]\n\n        # If user did not provide min_samples or min_cluster_size, estimate them\n        if self.min_samples is None or self.min_cluster_size is None:\n            self._estimate_params(X)\n        else:\n            print(\n                f\"Using user-defined parameters: min_samples={self.min_samples}, \"\n                f\"min_cluster_size={self.min_cluster_size}\"\n            )\n\n        # fit the model\n        self.model = SKLearnOPTICS(\n            min_samples=self.min_samples,\n            min_cluster_size=self.min_cluster_size,\n            xi=self.xi,\n            metric=\"euclidean\",\n            n_jobs=-1,\n        )\n        self.model.fit(X)\n\n        self.labels = self.model.labels_\n        self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n        self.n_noise_ = np.count_nonzero(self.labels == -1)\n\n    def get_model_params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve key parameters and results from the fitted OPTICS model.\n\n        Returns:\n            Dict[str, Any]:\n                - min_samples (int): The final min_samples used\n                - min_cluster_size (int): The final min_cluster_size used\n                - n_clusters (int): Number of clusters discovered (excluding noise)\n                - n_noise (int): Number of data points labeled as noise\n        \"\"\"\n        return {\n            \"min_samples\": self.min_samples,\n            \"min_cluster_size\": self.min_cluster_size,\n            \"n_clusters\": self.n_clusters_,\n            \"n_noise\": self.n_noise_,\n        }\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/optics/#scirex.experimental.ml.unsupervised.clustering.optics.Optics.__init__","title":"<code>__init__(min_samples=None, min_cluster_size=None, xi=0.05)</code>","text":"<p>Initialize the OPTICS clustering model.</p> <p>Parameters:</p> Name Type Description Default <code>min_samples</code> <code>Optional[int]</code> <p>If provided, the algorithm uses this min_samples. Otherwise, a heuristic is used.</p> <code>None</code> <code>min_cluster_size</code> <code>Optional[int]</code> <p>If provided, the algorithm uses this min_cluster_size. Otherwise, a heuristic is used.</p> <code>None</code> <code>xi</code> <code>float</code> <p>Determines the minimum steepness on the reachability plot for cluster extraction. Defaults to 0.05.</p> <code>0.05</code> Source code in <code>scirex/experimental/ml/unsupervised/clustering/optics.py</code> <pre><code>def __init__(\n    self,\n    min_samples: Optional[int] = None,\n    min_cluster_size: Optional[int] = None,\n    xi: float = 0.05,\n) -&gt; None:\n    \"\"\"\n    Initialize the OPTICS clustering model.\n\n    Args:\n        min_samples (Optional[int], optional):\n            If provided, the algorithm uses this min_samples. Otherwise, a heuristic is used.\n        min_cluster_size (Optional[int], optional):\n            If provided, the algorithm uses this min_cluster_size. Otherwise, a heuristic is used.\n        xi (float, optional):\n            Determines the minimum steepness on the reachability plot for cluster extraction.\n            Defaults to 0.05.\n    \"\"\"\n    super().__init__(\"optics\")\n    self.min_samples = min_samples\n    self.min_cluster_size = min_cluster_size\n    self.xi = xi\n\n    # Attributes set after fitting\n    self.labels: Optional[np.ndarray] = None\n    self.n_clusters_: Optional[int] = None\n    self.n_noise_: Optional[int] = None\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/optics/#scirex.experimental.ml.unsupervised.clustering.optics.Optics.fit","title":"<code>fit(X)</code>","text":"<p>Fit the OPTICS model to the data. - If min_samples/min_cluster_size are not set, estimate them heuristically. - Then create and fit an OPTICS model, storing labels and cluster info.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data array of shape (n_samples, n_features).</p> required Source code in <code>scirex/experimental/ml/unsupervised/clustering/optics.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Fit the OPTICS model to the data.\n    - If min_samples/min_cluster_size are not set, estimate them heuristically.\n    - Then create and fit an OPTICS model, storing labels and cluster info.\n\n    Args:\n        X (np.ndarray): Input data array of shape (n_samples, n_features).\n    \"\"\"\n    X = X.astype(np.float32, copy=False)\n    n_samples = X.shape[0]\n\n    # If user did not provide min_samples or min_cluster_size, estimate them\n    if self.min_samples is None or self.min_cluster_size is None:\n        self._estimate_params(X)\n    else:\n        print(\n            f\"Using user-defined parameters: min_samples={self.min_samples}, \"\n            f\"min_cluster_size={self.min_cluster_size}\"\n        )\n\n    # fit the model\n    self.model = SKLearnOPTICS(\n        min_samples=self.min_samples,\n        min_cluster_size=self.min_cluster_size,\n        xi=self.xi,\n        metric=\"euclidean\",\n        n_jobs=-1,\n    )\n    self.model.fit(X)\n\n    self.labels = self.model.labels_\n    self.n_clusters_ = len(set(self.labels)) - (1 if -1 in self.labels else 0)\n    self.n_noise_ = np.count_nonzero(self.labels == -1)\n</code></pre>"},{"location":"api/experimental/ml/unsupervised/clustering/optics/#scirex.experimental.ml.unsupervised.clustering.optics.Optics.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Retrieve key parameters and results from the fitted OPTICS model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: - min_samples (int): The final min_samples used - min_cluster_size (int): The final min_cluster_size used - n_clusters (int): Number of clusters discovered (excluding noise) - n_noise (int): Number of data points labeled as noise</p> Source code in <code>scirex/experimental/ml/unsupervised/clustering/optics.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve key parameters and results from the fitted OPTICS model.\n\n    Returns:\n        Dict[str, Any]:\n            - min_samples (int): The final min_samples used\n            - min_cluster_size (int): The final min_cluster_size used\n            - n_clusters (int): Number of clusters discovered (excluding noise)\n            - n_noise (int): Number of data points labeled as noise\n    \"\"\"\n    return {\n        \"min_samples\": self.min_samples,\n        \"min_cluster_size\": self.min_cluster_size,\n        \"n_clusters\": self.n_clusters_,\n        \"n_noise\": self.n_noise_,\n    }\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/","title":"pruning","text":"<p>Example Script: pruning.py</p> <pre><code>This script implements model compression using TensorFlow Model Optimization Toolkit's pruning capabilities\non neural network architectures. It demonstrates a reusable implementation for applying and evaluating\nmodel pruning with polynomial decay schedule.\n\nThis example includes:\n    - Implementation of a reusable ModelPruning class for model compression\n    - Building and training baseline CNN models\n    - Applying progressive pruning from 50% to 80% sparsity\n    - Training and evaluation workflows for both baseline and pruned models\n\n\n Authors:\n - Nithyashree R (nithyashreer@iisc.ac.in)\n\nVersion Info:\n    - 06/01/2024: Initial version\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning","title":"<code>ModelPruning</code>","text":"<p>A reusable class for performing pruning on any model and dataset.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The base model architecture that will undergo pruning.</p> <code>pruned_model</code> <code>Model</code> <p>The model after pruning.</p> <code>baseline_model_accuracy</code> <code>float</code> <p>Accuracy of the baseline model evaluated on test data.</p> <code>pruned_model_accuracy</code> <code>float</code> <p>Accuracy of the pruned model evaluated on test data.</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>class ModelPruning:\n    \"\"\"\n    A reusable class for performing pruning on any model and dataset.\n\n    Attributes:\n        model (tf.keras.Model): The base model architecture that will undergo pruning.\n        pruned_model (tf.keras.Model): The model after pruning.\n        baseline_model_accuracy (float): Accuracy of the baseline model evaluated on test data.\n        pruned_model_accuracy (float): Accuracy of the pruned model evaluated on test data.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_shape=(28, 28),\n        num_classes=10,\n        epochs=10,\n        batch_size=35,  # Changed default batch_size to get ~1688 steps\n        validation_split=0.1,\n    ):\n        \"\"\"\n        Initializes the pruning process for a model.\n\n        :param input_shape: Shape of the input data.\n        :type input_shape: tuple\n        :param num_classes: Number of output classes.\n        :type num_classes: int\n        :param epochs: Number of epochs to train the pruned model. Default is 10.\n        :type epochs: int\n        :param batch_size: Size of the training batch. Default is 35.\n        :type batch_size: int\n        :param validation_split: Fraction of training data to be used for validation. Default is 0.1.\n        :type validation_split: float\n        \"\"\"\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.validation_split = validation_split\n        self.model = self._build_model()\n        self.pruned_model = None\n        self.baseline_model_accuracy = None\n        self.pruned_model_accuracy = None\n\n    def _build_model(self):\n        \"\"\"\n        Builds the base model architecture.\n\n        :return: A compiled Keras model.\n        :rtype: tf.keras.Model\n        \"\"\"\n        model = keras.Sequential(\n            [\n                keras.layers.InputLayer(input_shape=self.input_shape),\n                keras.layers.Reshape(target_shape=(28, 28, 1)),\n                keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=\"relu\"),\n                keras.layers.MaxPooling2D(pool_size=(2, 2)),\n                keras.layers.Flatten(),\n                keras.layers.Dense(self.num_classes),\n            ]\n        )\n        model.compile(\n            optimizer=\"adam\",\n            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[\"accuracy\"],\n        )\n        return model\n\n    def train_baseline_model(self, train_images, train_labels):\n        \"\"\"\n        Trains the baseline model without pruning.\n\n        :param train_images: Training data features.\n        :param train_labels: Training data labels.\n        \"\"\"\n        self.model.fit(\n            train_images,\n            train_labels,\n            batch_size=self.batch_size,  # Added batch_size parameter\n            epochs=self.epochs,\n            validation_split=self.validation_split,\n        )\n\n    def evaluate_baseline(self, test_images, test_labels):\n        \"\"\"\n        Evaluates the baseline model.\n\n        :param test_images: Test data features.\n        :param test_labels: Test data labels.\n        :return: Accuracy of the baseline model.\n        :rtype: float\n        \"\"\"\n        _, self.baseline_model_accuracy = self.model.evaluate(\n            test_images, test_labels, verbose=0\n        )\n        return self.baseline_model_accuracy\n\n    def save_baseline_model(self):\n        \"\"\"\n        Saves the baseline model to a temporary file using the .keras format.\n\n        :return: Path to the saved model file.\n        :rtype: str\n        \"\"\"\n        keras_file = tempfile.mktemp(\".keras\")\n        self.model.save(keras_file, save_format=\"keras\")\n        return keras_file\n\n    def apply_pruning(self):\n        \"\"\"\n        Applies pruning to the base model.\n\n        :return: A pruned model.\n        :rtype: tf.keras.Model\n        \"\"\"\n        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n        batch_size = self.batch_size\n        epochs = self.epochs\n        validation_split = self.validation_split\n\n        # Ensure the model is built before accessing input_shape\n        self.model.build((None, *self.input_shape))\n        num_images = 60000 * (\n            1 - validation_split\n        )  # Fixed number of training images for MNIST\n\n        end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n\n        pruning_params = {\n            \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n                initial_sparsity=0.50,\n                final_sparsity=0.80,\n                begin_step=0,\n                end_step=end_step,\n            )\n        }\n\n        self.pruned_model = prune_low_magnitude(self.model, **pruning_params)\n        self.pruned_model.compile(\n            optimizer=\"adam\",\n            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[\"accuracy\"],\n        )\n\n        return self.pruned_model\n\n    def train_pruned_model(self, train_images, train_labels):\n        \"\"\"\n        Trains the pruned model.\n\n        :param train_images: Training data features.\n        :param train_labels: Training data labels.\n        \"\"\"\n        logdir = tempfile.mkdtemp()\n\n        callbacks = [\n            tfmot.sparsity.keras.UpdatePruningStep(),\n            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n        ]\n\n        self.pruned_model.fit(\n            train_images,\n            train_labels,\n            batch_size=self.batch_size,\n            epochs=self.epochs,\n            validation_split=self.validation_split,\n            callbacks=callbacks,\n        )\n\n    def evaluate_pruned_model(self, test_images, test_labels):\n        \"\"\"\n        Evaluates the pruned model.\n\n        :param test_images: Test data features.\n        :param test_labels: Test data labels.\n        :return: Accuracy of the pruned model.\n        :rtype: float\n        \"\"\"\n        _, self.pruned_model_accuracy = self.pruned_model.evaluate(\n            test_images, test_labels, verbose=0\n        )\n        return self.pruned_model_accuracy\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.__init__","title":"<code>__init__(input_shape=(28, 28), num_classes=10, epochs=10, batch_size=35, validation_split=0.1)</code>","text":"<p>Initializes the pruning process for a model.</p> <p>:param input_shape: Shape of the input data. :type input_shape: tuple :param num_classes: Number of output classes. :type num_classes: int :param epochs: Number of epochs to train the pruned model. Default is 10. :type epochs: int :param batch_size: Size of the training batch. Default is 35. :type batch_size: int :param validation_split: Fraction of training data to be used for validation. Default is 0.1. :type validation_split: float</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def __init__(\n    self,\n    input_shape=(28, 28),\n    num_classes=10,\n    epochs=10,\n    batch_size=35,  # Changed default batch_size to get ~1688 steps\n    validation_split=0.1,\n):\n    \"\"\"\n    Initializes the pruning process for a model.\n\n    :param input_shape: Shape of the input data.\n    :type input_shape: tuple\n    :param num_classes: Number of output classes.\n    :type num_classes: int\n    :param epochs: Number of epochs to train the pruned model. Default is 10.\n    :type epochs: int\n    :param batch_size: Size of the training batch. Default is 35.\n    :type batch_size: int\n    :param validation_split: Fraction of training data to be used for validation. Default is 0.1.\n    :type validation_split: float\n    \"\"\"\n    self.input_shape = input_shape\n    self.num_classes = num_classes\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.validation_split = validation_split\n    self.model = self._build_model()\n    self.pruned_model = None\n    self.baseline_model_accuracy = None\n    self.pruned_model_accuracy = None\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.apply_pruning","title":"<code>apply_pruning()</code>","text":"<p>Applies pruning to the base model.</p> <p>:return: A pruned model. :rtype: tf.keras.Model</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def apply_pruning(self):\n    \"\"\"\n    Applies pruning to the base model.\n\n    :return: A pruned model.\n    :rtype: tf.keras.Model\n    \"\"\"\n    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n    batch_size = self.batch_size\n    epochs = self.epochs\n    validation_split = self.validation_split\n\n    # Ensure the model is built before accessing input_shape\n    self.model.build((None, *self.input_shape))\n    num_images = 60000 * (\n        1 - validation_split\n    )  # Fixed number of training images for MNIST\n\n    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n\n    pruning_params = {\n        \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n            initial_sparsity=0.50,\n            final_sparsity=0.80,\n            begin_step=0,\n            end_step=end_step,\n        )\n    }\n\n    self.pruned_model = prune_low_magnitude(self.model, **pruning_params)\n    self.pruned_model.compile(\n        optimizer=\"adam\",\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"],\n    )\n\n    return self.pruned_model\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.evaluate_baseline","title":"<code>evaluate_baseline(test_images, test_labels)</code>","text":"<p>Evaluates the baseline model.</p> <p>:param test_images: Test data features. :param test_labels: Test data labels. :return: Accuracy of the baseline model. :rtype: float</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def evaluate_baseline(self, test_images, test_labels):\n    \"\"\"\n    Evaluates the baseline model.\n\n    :param test_images: Test data features.\n    :param test_labels: Test data labels.\n    :return: Accuracy of the baseline model.\n    :rtype: float\n    \"\"\"\n    _, self.baseline_model_accuracy = self.model.evaluate(\n        test_images, test_labels, verbose=0\n    )\n    return self.baseline_model_accuracy\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.evaluate_pruned_model","title":"<code>evaluate_pruned_model(test_images, test_labels)</code>","text":"<p>Evaluates the pruned model.</p> <p>:param test_images: Test data features. :param test_labels: Test data labels. :return: Accuracy of the pruned model. :rtype: float</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def evaluate_pruned_model(self, test_images, test_labels):\n    \"\"\"\n    Evaluates the pruned model.\n\n    :param test_images: Test data features.\n    :param test_labels: Test data labels.\n    :return: Accuracy of the pruned model.\n    :rtype: float\n    \"\"\"\n    _, self.pruned_model_accuracy = self.pruned_model.evaluate(\n        test_images, test_labels, verbose=0\n    )\n    return self.pruned_model_accuracy\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.save_baseline_model","title":"<code>save_baseline_model()</code>","text":"<p>Saves the baseline model to a temporary file using the .keras format.</p> <p>:return: Path to the saved model file. :rtype: str</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def save_baseline_model(self):\n    \"\"\"\n    Saves the baseline model to a temporary file using the .keras format.\n\n    :return: Path to the saved model file.\n    :rtype: str\n    \"\"\"\n    keras_file = tempfile.mktemp(\".keras\")\n    self.model.save(keras_file, save_format=\"keras\")\n    return keras_file\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.train_baseline_model","title":"<code>train_baseline_model(train_images, train_labels)</code>","text":"<p>Trains the baseline model without pruning.</p> <p>:param train_images: Training data features. :param train_labels: Training data labels.</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def train_baseline_model(self, train_images, train_labels):\n    \"\"\"\n    Trains the baseline model without pruning.\n\n    :param train_images: Training data features.\n    :param train_labels: Training data labels.\n    \"\"\"\n    self.model.fit(\n        train_images,\n        train_labels,\n        batch_size=self.batch_size,  # Added batch_size parameter\n        epochs=self.epochs,\n        validation_split=self.validation_split,\n    )\n</code></pre>"},{"location":"api/experimental/model_compression/pruning/#scirex.experimental.model_compression.pruning.ModelPruning.train_pruned_model","title":"<code>train_pruned_model(train_images, train_labels)</code>","text":"<p>Trains the pruned model.</p> <p>:param train_images: Training data features. :param train_labels: Training data labels.</p> Source code in <code>scirex/experimental/model_compression/pruning.py</code> <pre><code>def train_pruned_model(self, train_images, train_labels):\n    \"\"\"\n    Trains the pruned model.\n\n    :param train_images: Training data features.\n    :param train_labels: Training data labels.\n    \"\"\"\n    logdir = tempfile.mkdtemp()\n\n    callbacks = [\n        tfmot.sparsity.keras.UpdatePruningStep(),\n        tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n    ]\n\n    self.pruned_model.fit(\n        train_images,\n        train_labels,\n        batch_size=self.batch_size,\n        epochs=self.epochs,\n        validation_split=self.validation_split,\n        callbacks=callbacks,\n    )\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/","title":"quantization","text":"<p>Example Script: quantization.py</p> <p>This script implements model optimization using TensorFlow Model Optimization Toolkit's quantization capabilities on neural network architectures. It demonstrates an implementation for applying and evaluating both quantization-aware training and post-training quantization.</p> This example includes <ul> <li>Implementation of QuantizationAwareTraining class</li> <li>Building and training baseline CNN models</li> <li>Applying quantization-aware training with TensorFlow</li> <li>Training and evaluation workflows for both models</li> <li>Converting models to TFLite format with optimization</li> <li>Implementing post-training quantization for model compression</li> </ul> Authors <ul> <li>Nithyashree R (nithyashreer@iisc.ac.in)</li> </ul> Version Info <ul> <li>06/01/2024: Initial version</li> </ul>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining","title":"<code>QuantizationAwareTraining</code>","text":"<p>A reusable class for performing quantization-aware training on any dataset.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The base model architecture.</p> <code>q_aware_model</code> <code>Model</code> <p>The quantization-aware trained model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>class QuantizationAwareTraining:\n    \"\"\"\n    A reusable class for performing quantization-aware training on any dataset.\n\n    Attributes:\n        model (tf.keras.Model): The base model architecture.\n        q_aware_model (tf.keras.Model): The quantization-aware trained model.\n    \"\"\"\n\n    def __init__(\n        self, input_shape, num_classes, filters=12, kernel_size=(3, 3), pool_size=(2, 2)\n    ):\n        \"\"\"\n        Initializes the model architecture.\n\n        :param input_shape: Shape of input data.\n        :param num_classes: Number of output classes.\n        :param filters: Number of filters for the Conv2D layer.\n        :param kernel_size: Kernel size for the Conv2D layer.\n        :param pool_size: Pool size for the MaxPooling2D layer.\n        \"\"\"\n        self.model = self._build_model(\n            input_shape, num_classes, filters, kernel_size, pool_size\n        )\n        self.q_aware_model = None\n\n    @staticmethod\n    def _build_model(input_shape, num_classes, filters, kernel_size, pool_size):\n        \"\"\"\n        Builds the base model architecture.\n\n        :param input_shape: Shape of input data.\n        :param num_classes: Number of output classes.\n        :param filters: Number of filters for the Conv2D layer.\n        :param kernel_size: Kernel size for the Conv2D layer.\n        :param pool_size: Pool size for the MaxPooling2D layer.\n        :return: A Keras model.\n        \"\"\"\n        model = tf.keras.Sequential(\n            [\n                tf.keras.layers.InputLayer(input_shape=input_shape),\n                tf.keras.layers.Reshape(target_shape=input_shape + (1,)),\n                tf.keras.layers.Conv2D(\n                    filters=filters, kernel_size=kernel_size, activation=\"relu\"\n                ),\n                tf.keras.layers.MaxPooling2D(pool_size=pool_size),\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(num_classes),\n            ]\n        )\n        return model\n\n    def train(self, train_data, train_labels, epochs=10, validation_split=0.1):\n        \"\"\"\n        Trains the base model.\n\n        :param train_data: Training dataset.\n        :param train_labels: Training labels.\n        :param epochs: Number of training epochs.\n        :param validation_split: Fraction of training data for validation.\n        \"\"\"\n        self.model.compile(\n            optimizer=\"adam\",\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[\"accuracy\"],\n        )\n        self.model.fit(\n            train_data,\n            train_labels,\n            epochs=epochs,\n            validation_split=validation_split,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(\n                    monitor=\"val_loss\", patience=3, restore_best_weights=True\n                )\n            ],\n        )\n\n    def apply_quantization_aware_training(self):\n        \"\"\"\n        Applies quantization-aware training to the base model.\n        \"\"\"\n        quantize_model = tfmot.quantization.keras.quantize_model\n        self.q_aware_model = quantize_model(self.model)\n\n        self.q_aware_model.compile(\n            optimizer=\"adam\",\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[\"accuracy\"],\n        )\n\n    def train_q_aware_model(\n        self, train_data, train_labels, batch_size=500, epochs=10, validation_split=0.1\n    ):\n        \"\"\"\n        Trains the quantization-aware model.\n\n        :param train_data: Training dataset.\n        :param train_labels: Training labels.\n        :param batch_size: Batch size for training.\n        :param epochs: Number of training epochs.\n        :param validation_split: Fraction of training data for validation.\n        \"\"\"\n        if self.q_aware_model is None:\n            raise ValueError(\n                \"Quantization-aware model is not initialized. Call `apply_quantization_aware_training` first.\"\n            )\n\n        self.q_aware_model.fit(\n            train_data,\n            train_labels,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_split=validation_split,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(\n                    monitor=\"val_loss\", patience=3, restore_best_weights=True\n                )\n            ],\n        )\n\n    def evaluate(self, test_data, test_labels):\n        \"\"\"\n        Evaluates both the base model and the quantized model.\n\n        :param test_data: Test dataset.\n        :param test_labels: Test labels.\n        :return: Accuracy of base model and quantized model.\n        \"\"\"\n        baseline_accuracy = self.model.evaluate(test_data, test_labels, verbose=0)[1]\n        q_aware_accuracy = self.q_aware_model.evaluate(\n            test_data, test_labels, verbose=0\n        )[1]\n        return baseline_accuracy, q_aware_accuracy\n\n    def convert_to_tflite(self):\n        \"\"\"\n        Converts the quantization-aware model to TensorFlow Lite format.\n\n        :return: Quantized TFLite model.\n        \"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.q_aware_model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        return converter.convert()\n\n    def post_quantization(self):\n        \"\"\"\n        Applies post-training quantization to the base model.\n\n        :return: Post-quantized TFLite model.\n        \"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        return converter.convert()\n\n    @staticmethod\n    def save_model(model_content, filename):\n        \"\"\"\n        Saves the TFLite model to a file.\n\n        :param model_content: The TFLite model content.\n        :param filename: File name to save the model.\n        \"\"\"\n        with open(filename, \"wb\") as file:\n            file.write(model_content)\n\n    @staticmethod\n    def measure_model_size(filepath):\n        \"\"\"\n        Measures the size of a model file.\n\n        :param filepath: Path to the model file.\n        :return: Size of the model in megabytes.\n        \"\"\"\n        return os.path.getsize(filepath) / float(2**20)\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.__init__","title":"<code>__init__(input_shape, num_classes, filters=12, kernel_size=(3, 3), pool_size=(2, 2))</code>","text":"<p>Initializes the model architecture.</p> <p>:param input_shape: Shape of input data. :param num_classes: Number of output classes. :param filters: Number of filters for the Conv2D layer. :param kernel_size: Kernel size for the Conv2D layer. :param pool_size: Pool size for the MaxPooling2D layer.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def __init__(\n    self, input_shape, num_classes, filters=12, kernel_size=(3, 3), pool_size=(2, 2)\n):\n    \"\"\"\n    Initializes the model architecture.\n\n    :param input_shape: Shape of input data.\n    :param num_classes: Number of output classes.\n    :param filters: Number of filters for the Conv2D layer.\n    :param kernel_size: Kernel size for the Conv2D layer.\n    :param pool_size: Pool size for the MaxPooling2D layer.\n    \"\"\"\n    self.model = self._build_model(\n        input_shape, num_classes, filters, kernel_size, pool_size\n    )\n    self.q_aware_model = None\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.apply_quantization_aware_training","title":"<code>apply_quantization_aware_training()</code>","text":"<p>Applies quantization-aware training to the base model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def apply_quantization_aware_training(self):\n    \"\"\"\n    Applies quantization-aware training to the base model.\n    \"\"\"\n    quantize_model = tfmot.quantization.keras.quantize_model\n    self.q_aware_model = quantize_model(self.model)\n\n    self.q_aware_model.compile(\n        optimizer=\"adam\",\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"],\n    )\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.convert_to_tflite","title":"<code>convert_to_tflite()</code>","text":"<p>Converts the quantization-aware model to TensorFlow Lite format.</p> <p>:return: Quantized TFLite model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def convert_to_tflite(self):\n    \"\"\"\n    Converts the quantization-aware model to TensorFlow Lite format.\n\n    :return: Quantized TFLite model.\n    \"\"\"\n    converter = tf.lite.TFLiteConverter.from_keras_model(self.q_aware_model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    return converter.convert()\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.evaluate","title":"<code>evaluate(test_data, test_labels)</code>","text":"<p>Evaluates both the base model and the quantized model.</p> <p>:param test_data: Test dataset. :param test_labels: Test labels. :return: Accuracy of base model and quantized model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def evaluate(self, test_data, test_labels):\n    \"\"\"\n    Evaluates both the base model and the quantized model.\n\n    :param test_data: Test dataset.\n    :param test_labels: Test labels.\n    :return: Accuracy of base model and quantized model.\n    \"\"\"\n    baseline_accuracy = self.model.evaluate(test_data, test_labels, verbose=0)[1]\n    q_aware_accuracy = self.q_aware_model.evaluate(\n        test_data, test_labels, verbose=0\n    )[1]\n    return baseline_accuracy, q_aware_accuracy\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.measure_model_size","title":"<code>measure_model_size(filepath)</code>  <code>staticmethod</code>","text":"<p>Measures the size of a model file.</p> <p>:param filepath: Path to the model file. :return: Size of the model in megabytes.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>@staticmethod\ndef measure_model_size(filepath):\n    \"\"\"\n    Measures the size of a model file.\n\n    :param filepath: Path to the model file.\n    :return: Size of the model in megabytes.\n    \"\"\"\n    return os.path.getsize(filepath) / float(2**20)\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.post_quantization","title":"<code>post_quantization()</code>","text":"<p>Applies post-training quantization to the base model.</p> <p>:return: Post-quantized TFLite model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def post_quantization(self):\n    \"\"\"\n    Applies post-training quantization to the base model.\n\n    :return: Post-quantized TFLite model.\n    \"\"\"\n    converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    return converter.convert()\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.save_model","title":"<code>save_model(model_content, filename)</code>  <code>staticmethod</code>","text":"<p>Saves the TFLite model to a file.</p> <p>:param model_content: The TFLite model content. :param filename: File name to save the model.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>@staticmethod\ndef save_model(model_content, filename):\n    \"\"\"\n    Saves the TFLite model to a file.\n\n    :param model_content: The TFLite model content.\n    :param filename: File name to save the model.\n    \"\"\"\n    with open(filename, \"wb\") as file:\n        file.write(model_content)\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.train","title":"<code>train(train_data, train_labels, epochs=10, validation_split=0.1)</code>","text":"<p>Trains the base model.</p> <p>:param train_data: Training dataset. :param train_labels: Training labels. :param epochs: Number of training epochs. :param validation_split: Fraction of training data for validation.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def train(self, train_data, train_labels, epochs=10, validation_split=0.1):\n    \"\"\"\n    Trains the base model.\n\n    :param train_data: Training dataset.\n    :param train_labels: Training labels.\n    :param epochs: Number of training epochs.\n    :param validation_split: Fraction of training data for validation.\n    \"\"\"\n    self.model.compile(\n        optimizer=\"adam\",\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"],\n    )\n    self.model.fit(\n        train_data,\n        train_labels,\n        epochs=epochs,\n        validation_split=validation_split,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor=\"val_loss\", patience=3, restore_best_weights=True\n            )\n        ],\n    )\n</code></pre>"},{"location":"api/experimental/model_compression/quantization/#scirex.experimental.model_compression.quantization.QuantizationAwareTraining.train_q_aware_model","title":"<code>train_q_aware_model(train_data, train_labels, batch_size=500, epochs=10, validation_split=0.1)</code>","text":"<p>Trains the quantization-aware model.</p> <p>:param train_data: Training dataset. :param train_labels: Training labels. :param batch_size: Batch size for training. :param epochs: Number of training epochs. :param validation_split: Fraction of training data for validation.</p> Source code in <code>scirex/experimental/model_compression/quantization.py</code> <pre><code>def train_q_aware_model(\n    self, train_data, train_labels, batch_size=500, epochs=10, validation_split=0.1\n):\n    \"\"\"\n    Trains the quantization-aware model.\n\n    :param train_data: Training dataset.\n    :param train_labels: Training labels.\n    :param batch_size: Batch size for training.\n    :param epochs: Number of training epochs.\n    :param validation_split: Fraction of training data for validation.\n    \"\"\"\n    if self.q_aware_model is None:\n        raise ValueError(\n            \"Quantization-aware model is not initialized. Call `apply_quantization_aware_training` first.\"\n        )\n\n    self.q_aware_model.fit(\n        train_data,\n        train_labels,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_split=validation_split,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor=\"val_loss\", patience=3, restore_best_weights=True\n            )\n        ],\n    )\n</code></pre>"},{"location":"api/transformers/","title":"Transformers Module","text":"<p>Module: transformer.py</p> <p>This module implements Transformer architectures using Flax NNX.</p> Key Classes <p>EncoderBlock: Transformer Encoder Block DecoderBlock: Transformer Decoder Block PositionalEmbedding: Standard Positional Embedding EncoderModel: Transformer Encoder-only Model EncoderDecoderModel: Transformer Encoder-Decoder Model</p> Key Features <ul> <li>Built on top of JAX and Flax NNX</li> <li>Supports both Encoder and Encoder-Decoder architectures</li> <li>includes Multi-Head Attention and Position-wise Feedforward Networks</li> <li>Configurable dropout and layer normalization</li> </ul> Authors <ul> <li>Lokesh Mohanty (lokeshm@iisc.ac.in)</li> </ul>"},{"location":"api/transformers/#scirex.transformers.transformer.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder Block.</p> <p>Attributes:</p> Name Type Description <code>d_model</code> <p>The number of expected features in the input.</p> <code>d_hidden</code> <p>The internal dimension of the feedforward network.</p> <code>n_heads</code> <p>The number of heads in the multiheadattention models.</p> <code>self_attn</code> <p>The self-attention module.</p> <code>cross_attn</code> <p>The cross-attention module.</p> <code>ffn</code> <p>The feedforward network module.</p> <code>norm1</code> <p>Layer normalization for the self-attention output.</p> <code>norm2</code> <p>Layer normalization for the cross-attention output.</p> <code>norm3</code> <p>Layer normalization for the feedforward output.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>class DecoderBlock(nn.Module):\n    \"\"\"\n    Transformer Decoder Block.\n\n    Attributes:\n        d_model: The number of expected features in the input.\n        d_hidden: The internal dimension of the feedforward network.\n        n_heads: The number of heads in the multiheadattention models.\n        self_attn: The self-attention module.\n        cross_attn: The cross-attention module.\n        ffn: The feedforward network module.\n        norm1: Layer normalization for the self-attention output.\n        norm2: Layer normalization for the cross-attention output.\n        norm3: Layer normalization for the feedforward output.\n    \"\"\"\n\n    def __init__(self, d_model: int, d_hidden: int, n_heads: int, rngs: nn.Rngs):\n        self.d_model = d_model\n        self.d_hidden = d_hidden\n        self.n_heads = n_heads\n        self.self_attn = nn.MultiHeadAttention(n_heads, d_model, decode=False, rngs=rngs)\n        self.cross_attn = nn.MultiHeadAttention(n_heads, d_model, decode=False, rngs=rngs)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_hidden, rngs=rngs),\n            nn.relu,\n            nn.Linear(d_hidden, d_model, rngs=rngs),\n        )\n        self.norm1 = nn.LayerNorm(d_model, rngs=rngs)\n        self.norm2 = nn.LayerNorm(d_model, rngs=rngs)\n        self.norm3 = nn.LayerNorm(d_model, rngs=rngs)\n\n    def get_causal_attention_mask(self, context_size: int) -&gt; Array:\n        \"\"\"\n        Creates a causal mask for the attention mechanism.\n\n        Args:\n            context_size: The size of the context window.\n\n        Returns:\n            A causal mask of shape (1, 1, context_size, context_size).\n        \"\"\"\n        i = jnp.arange(context_size)[:, None]\n        j = jnp.arange(context_size)\n        mask = (i &gt;= j).astype(jnp.int32)\n        mask = jnp.reshape(mask, (1, 1, context_size, context_size))\n        return mask\n\n    def __call__(self, x: Array, y: Array, mask: Optional[Array] = None) -&gt; Array:\n        \"\"\"\n        Forward pass of the DecoderBlock.\n\n        Args:\n            x: Input array (target sequence) of shape (batch_size, seq_len, d_model).\n            y: Input array (source sequence from encoder) of shape (batch_size, seq_len, d_model).\n            mask: Optional mask array.\n\n        Returns:\n            Output array of shape (batch_size, seq_len, d_model).\n        \"\"\"\n        causal_mask = self.get_causal_attention_mask(x.shape[1])\n        if mask is not None:\n            padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32)\n            padding_mask = jnp.minimum(padding_mask, causal_mask)\n        else:\n            padding_mask = None\n\n        x = self.norm1(x + self.self_attn(x, x, x, mask=causal_mask))\n        x = self.norm2(x + self.cross_attn(x, y, y, mask=padding_mask))\n        x = self.norm3(x + self.ffn(x))\n        return x\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.DecoderBlock.__call__","title":"<code>__call__(x, y, mask=None)</code>","text":"<p>Forward pass of the DecoderBlock.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input array (target sequence) of shape (batch_size, seq_len, d_model).</p> required <code>y</code> <code>Array</code> <p>Input array (source sequence from encoder) of shape (batch_size, seq_len, d_model).</p> required <code>mask</code> <code>Optional[Array]</code> <p>Optional mask array.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Output array of shape (batch_size, seq_len, d_model).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def __call__(self, x: Array, y: Array, mask: Optional[Array] = None) -&gt; Array:\n    \"\"\"\n    Forward pass of the DecoderBlock.\n\n    Args:\n        x: Input array (target sequence) of shape (batch_size, seq_len, d_model).\n        y: Input array (source sequence from encoder) of shape (batch_size, seq_len, d_model).\n        mask: Optional mask array.\n\n    Returns:\n        Output array of shape (batch_size, seq_len, d_model).\n    \"\"\"\n    causal_mask = self.get_causal_attention_mask(x.shape[1])\n    if mask is not None:\n        padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32)\n        padding_mask = jnp.minimum(padding_mask, causal_mask)\n    else:\n        padding_mask = None\n\n    x = self.norm1(x + self.self_attn(x, x, x, mask=causal_mask))\n    x = self.norm2(x + self.cross_attn(x, y, y, mask=padding_mask))\n    x = self.norm3(x + self.ffn(x))\n    return x\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.DecoderBlock.get_causal_attention_mask","title":"<code>get_causal_attention_mask(context_size)</code>","text":"<p>Creates a causal mask for the attention mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The size of the context window.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A causal mask of shape (1, 1, context_size, context_size).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def get_causal_attention_mask(self, context_size: int) -&gt; Array:\n    \"\"\"\n    Creates a causal mask for the attention mechanism.\n\n    Args:\n        context_size: The size of the context window.\n\n    Returns:\n        A causal mask of shape (1, 1, context_size, context_size).\n    \"\"\"\n    i = jnp.arange(context_size)[:, None]\n    j = jnp.arange(context_size)\n    mask = (i &gt;= j).astype(jnp.int32)\n    mask = jnp.reshape(mask, (1, 1, context_size, context_size))\n    return mask\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderBlock","title":"<code>EncoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Encoder Block.</p> <p>Attributes:</p> Name Type Description <code>d_model</code> <p>The number of expected features in the input.</p> <code>d_hidden</code> <p>The internal dimension of the feedforward network.</p> <code>n_heads</code> <p>The number of heads in the multiheadattention models.</p> <code>attn</code> <p>The multi-head attention module.</p> <code>ffn</code> <p>The feedforward network module.</p> <code>norm1</code> <p>Layer normalization for the attention output.</p> <code>norm2</code> <p>Layer normalization for the feedforward output.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>class EncoderBlock(nn.Module):\n    \"\"\"\n    Transformer Encoder Block.\n\n    Attributes:\n        d_model: The number of expected features in the input.\n        d_hidden: The internal dimension of the feedforward network.\n        n_heads: The number of heads in the multiheadattention models.\n        attn: The multi-head attention module.\n        ffn: The feedforward network module.\n        norm1: Layer normalization for the attention output.\n        norm2: Layer normalization for the feedforward output.\n    \"\"\"\n\n    def __init__(self, d_model: int, d_hidden: int, n_heads: int, rngs: nn.Rngs):\n        self.d_model = d_model\n        self.d_hidden = d_hidden\n        self.n_heads = n_heads\n\n        self.attn = nn.MultiHeadAttention(n_heads, d_model, decode=False, rngs=rngs)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_hidden, rngs=rngs),\n            nn.relu,\n            nn.Linear(d_hidden, d_model, rngs=rngs),\n        )\n\n        self.norm1 = nn.LayerNorm(d_model, rngs=rngs)\n        self.norm2 = nn.LayerNorm(d_model, rngs=rngs)\n\n    def __call__(self, x: Array, mask: Optional[Array] = None) -&gt; Array:\n        \"\"\"\n        Forward pass of the EncoderBlock.\n\n        Args:\n            x: Input array of shape (batch_size, seq_len, d_model).\n            mask: Optional mask array.\n\n        Returns:\n            Output array of shape (batch_size, seq_len, d_model).\n        \"\"\"\n        padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32) if mask else None\n        x = self.norm1(x + self.attn(x, x, x, mask=padding_mask, decode=False))\n        x = self.norm2(x + self.ffn(x))\n        return x\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderBlock.__call__","title":"<code>__call__(x, mask=None)</code>","text":"<p>Forward pass of the EncoderBlock.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input array of shape (batch_size, seq_len, d_model).</p> required <code>mask</code> <code>Optional[Array]</code> <p>Optional mask array.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Output array of shape (batch_size, seq_len, d_model).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def __call__(self, x: Array, mask: Optional[Array] = None) -&gt; Array:\n    \"\"\"\n    Forward pass of the EncoderBlock.\n\n    Args:\n        x: Input array of shape (batch_size, seq_len, d_model).\n        mask: Optional mask array.\n\n    Returns:\n        Output array of shape (batch_size, seq_len, d_model).\n    \"\"\"\n    padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32) if mask else None\n    x = self.norm1(x + self.attn(x, x, x, mask=padding_mask, decode=False))\n    x = self.norm2(x + self.ffn(x))\n    return x\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderDecoderModel","title":"<code>EncoderDecoderModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Encoder-Decoder Model.</p> <p>Attributes:</p> Name Type Description <code>emb</code> <p>Embedding function (token + positional).</p> <code>encoder</code> <p>Encoder block.</p> <code>decoder</code> <p>Decoder block.</p> <code>dropout</code> <p>Dropout layer.</p> <code>head</code> <p>Linear head for output projection.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>class EncoderDecoderModel(nn.Module):\n    \"\"\"\n    Transformer Encoder-Decoder Model.\n\n    Attributes:\n        emb: Embedding function (token + positional).\n        encoder: Encoder block.\n        decoder: Decoder block.\n        dropout: Dropout layer.\n        head: Linear head for output projection.\n    \"\"\"\n\n    def __init__(\n        self,\n        context_size: int,\n        vocab_size: int,\n        d_model: int,\n        d_hidden: int,\n        n_heads: int,\n        dropout_rate: float,\n        rngs: nn.Rngs,\n    ):\n        token_emb = nn.Embed(vocab_size, d_model, rngs=rngs)\n        pos_emb = PositionalEmbedding(context_size, d_model, rngs=rngs)\n        self.emb = lambda x: token_emb(x) + pos_emb(x)\n        self.encoder = EncoderBlock(d_model, d_hidden, n_heads, rngs=rngs)\n        self.decoder = DecoderBlock(d_model, d_hidden, n_heads, rngs=rngs)\n        self.dropout = nn.Dropout(dropout_rate, rngs=rngs)\n        self.head = nn.Linear(d_model, vocab_size, rngs=rngs)\n\n    def __call__(self, x: Array, y: Array, mask: Optional[Array] = None, deterministic: bool = False) -&gt; Array:\n        \"\"\"\n        Forward pass of the EncoderDecoderModel.\n\n        Args:\n            x: Source token indices of shape (batch_size, source_seq_len).\n            y: Target token indices of shape (batch_size, target_seq_len).\n            mask: Optional mask array.\n            deterministic: If True, dropout is disabled.\n\n        Returns:\n            Logits of shape (batch_size, target_seq_len, vocab_size).\n        \"\"\"\n        x, y = self.emb(x), self.emb(y)\n        x = self.encoder(x, mask=mask)\n        y = self.decoder(y, x, mask=mask)\n        # per nn.Dropout - disable (deterministic=True) for eval, keep (False) for training\n        y = self.dropout(y, deterministic=deterministic)\n\n        return self.head(y)\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderDecoderModel.__call__","title":"<code>__call__(x, y, mask=None, deterministic=False)</code>","text":"<p>Forward pass of the EncoderDecoderModel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Source token indices of shape (batch_size, source_seq_len).</p> required <code>y</code> <code>Array</code> <p>Target token indices of shape (batch_size, target_seq_len).</p> required <code>mask</code> <code>Optional[Array]</code> <p>Optional mask array.</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>If True, dropout is disabled.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Logits of shape (batch_size, target_seq_len, vocab_size).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def __call__(self, x: Array, y: Array, mask: Optional[Array] = None, deterministic: bool = False) -&gt; Array:\n    \"\"\"\n    Forward pass of the EncoderDecoderModel.\n\n    Args:\n        x: Source token indices of shape (batch_size, source_seq_len).\n        y: Target token indices of shape (batch_size, target_seq_len).\n        mask: Optional mask array.\n        deterministic: If True, dropout is disabled.\n\n    Returns:\n        Logits of shape (batch_size, target_seq_len, vocab_size).\n    \"\"\"\n    x, y = self.emb(x), self.emb(y)\n    x = self.encoder(x, mask=mask)\n    y = self.decoder(y, x, mask=mask)\n    # per nn.Dropout - disable (deterministic=True) for eval, keep (False) for training\n    y = self.dropout(y, deterministic=deterministic)\n\n    return self.head(y)\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderModel","title":"<code>EncoderModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Encoder Model.</p> <p>Attributes:</p> Name Type Description <code>emb</code> <p>Embedding function (token + positional).</p> <code>encoder</code> <p>Encoder block.</p> <code>dropout</code> <p>Dropout layer.</p> <code>head</code> <p>Linear head for output projection.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>class EncoderModel(nn.Module):\n    \"\"\"\n    Transformer Encoder Model.\n\n    Attributes:\n        emb: Embedding function (token + positional).\n        encoder: Encoder block.\n        dropout: Dropout layer.\n        head: Linear head for output projection.\n    \"\"\"\n\n    def __init__(\n        self,\n        context_size: int,\n        vocab_size: int,\n        d_model: int,\n        d_hidden: int,\n        n_heads: int,\n        dropout_rate: float,\n        rngs: nn.Rngs,\n    ):\n        token_emb = nn.Embed(vocab_size, d_model, rngs=rngs)\n        pos_emb = PositionalEmbedding(context_size, d_model, rngs=rngs)\n        self.emb = lambda x: token_emb(x) + pos_emb(x)\n        self.encoder = EncoderBlock(d_model, d_hidden, n_heads, rngs=rngs)\n        self.dropout = nn.Dropout(dropout_rate, rngs=rngs)\n        self.head = nn.Linear(d_model, vocab_size, rngs=rngs)\n\n    def __call__(self, x: Array, mask: Optional[Array] = None, deterministic: bool = False) -&gt; Array:\n        \"\"\"\n        Forward pass of the EncoderModel.\n\n        Args:\n            x: Input token indices of shape (batch_size, seq_len).\n            mask: Optional mask array.\n            deterministic: If True, dropout is disabled.\n\n        Returns:\n            Logits of shape (batch_size, seq_len, vocab_size).\n        \"\"\"\n        x = self.emb(x)\n        x = self.encoder(x, mask=mask)\n        # per nn.Dropout - disable (deterministic=True) for eval, keep (False) for training\n        x = self.dropout(x, deterministic=deterministic)\n\n        return self.head(x)\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.EncoderModel.__call__","title":"<code>__call__(x, mask=None, deterministic=False)</code>","text":"<p>Forward pass of the EncoderModel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input token indices of shape (batch_size, seq_len).</p> required <code>mask</code> <code>Optional[Array]</code> <p>Optional mask array.</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>If True, dropout is disabled.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Logits of shape (batch_size, seq_len, vocab_size).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def __call__(self, x: Array, mask: Optional[Array] = None, deterministic: bool = False) -&gt; Array:\n    \"\"\"\n    Forward pass of the EncoderModel.\n\n    Args:\n        x: Input token indices of shape (batch_size, seq_len).\n        mask: Optional mask array.\n        deterministic: If True, dropout is disabled.\n\n    Returns:\n        Logits of shape (batch_size, seq_len, vocab_size).\n    \"\"\"\n    x = self.emb(x)\n    x = self.encoder(x, mask=mask)\n    # per nn.Dropout - disable (deterministic=True) for eval, keep (False) for training\n    x = self.dropout(x, deterministic=deterministic)\n\n    return self.head(x)\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.PositionalEmbedding","title":"<code>PositionalEmbedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positional Encoding module.</p> <p>Attributes:</p> Name Type Description <code>pos_emb</code> <p>Embedding layer for position indices.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>class PositionalEmbedding(nn.Module):\n    \"\"\"\n    Positional Encoding module.\n\n    Attributes:\n        pos_emb: Embedding layer for position indices.\n    \"\"\"\n\n    def __init__(self, context_size: int, d_model: int, rngs: nn.Rngs):\n        self.pos_emb = nn.Embed(context_size, d_model, rngs=rngs)\n\n    def __call__(self, x: Array) -&gt; Array:\n        \"\"\"\n        Adds positional embeddings to the input.\n\n        Args:\n            x: Input array of shape (batch_size, seq_len, d_model).\n\n        Returns:\n            Positional embeddings of shape (1, seq_len, d_model).\n        \"\"\"\n        pos = jnp.arange(0, x.shape[1])[None, :]\n        return self.pos_emb(pos)\n\n    def compute_mask(self, x: Array, mask: Optional[Array] = None) -&gt; Array | None:\n        \"\"\"\n        Computes the mask for the input.\n\n        Args:\n            x: Input array.\n            mask: Optional mask array.\n\n        Returns:\n            Boolean mask array identifying non-zero elements if mask is provided, else None.\n        \"\"\"\n        return jnp.not_equal(x, 0) if mask else None\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.PositionalEmbedding.__call__","title":"<code>__call__(x)</code>","text":"<p>Adds positional embeddings to the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input array of shape (batch_size, seq_len, d_model).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Positional embeddings of shape (1, seq_len, d_model).</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def __call__(self, x: Array) -&gt; Array:\n    \"\"\"\n    Adds positional embeddings to the input.\n\n    Args:\n        x: Input array of shape (batch_size, seq_len, d_model).\n\n    Returns:\n        Positional embeddings of shape (1, seq_len, d_model).\n    \"\"\"\n    pos = jnp.arange(0, x.shape[1])[None, :]\n    return self.pos_emb(pos)\n</code></pre>"},{"location":"api/transformers/#scirex.transformers.transformer.PositionalEmbedding.compute_mask","title":"<code>compute_mask(x, mask=None)</code>","text":"<p>Computes the mask for the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input array.</p> required <code>mask</code> <code>Optional[Array]</code> <p>Optional mask array.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array | None</code> <p>Boolean mask array identifying non-zero elements if mask is provided, else None.</p> Source code in <code>scirex/transformers/transformer.py</code> <pre><code>def compute_mask(self, x: Array, mask: Optional[Array] = None) -&gt; Array | None:\n    \"\"\"\n    Computes the mask for the input.\n\n    Args:\n        x: Input array.\n        mask: Optional mask array.\n\n    Returns:\n        Boolean mask array identifying non-zero elements if mask is provided, else None.\n    \"\"\"\n    return jnp.not_equal(x, 0) if mask else None\n</code></pre>"}]}